{"cells":[{"cell_type":"markdown","metadata":{"id":"CjRWziAVU2lZ"},"source":["# Unit 3: Code your first Deep Reinforcement Learning Algorithm with PyTorch: Reinforce."]},{"cell_type":"markdown","source":["## Create a virtual display"],"metadata":{"id":"bTpYcVZVMzUI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jV6wjQ7Be7p5"},"outputs":[],"source":["%%capture\n","!apt install python-opengl\n","!apt install ffmpeg\n","!apt install xvfb\n","!pip install pyvirtualdisplay\n","!pip install pyglet==1.5.1"]},{"cell_type":"code","source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"],"metadata":{"id":"Sr-Nuyb1dBm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tjrLfPFIW8XK"},"source":["## Install the dependencies\n"]},{"cell_type":"code","source":["!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n","!pip install git+https://github.com/simoninithomas/gym-games\n","!pip install imageio-ffmpeg\n","!pip install pyyaml==6.0"],"metadata":{"id":"e8ZVi-uydpgL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAHAq6RZW3rn"},"source":["## Import the packages\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8oadoJSWp7C"},"outputs":[],"source":["import numpy as np\n","\n","from collections import deque\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","\n","# Gym\n","import gym\n","import gym_pygame\n","\n","# Hugging Face Hub\n","from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n","import imageio"]},{"cell_type":"markdown","source":["## Check if we have a GPU\n","\n","- Let's check if we have a GPU\n","- If it's the case you should see `device:cuda0`"],"metadata":{"id":"RfxJYdMeeVgv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaJu5FeZxXGY"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5TNYa14aRav"},"outputs":[],"source":["print(device)"]},{"cell_type":"markdown","metadata":{"id":"8KEyKYo2ZSC-"},"source":["# First agent: Playing CartPole-v1 ðŸ¤–"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"POOOk15_K6KA"},"outputs":[],"source":["env_id = \"CartPole-v1\"\n","# Create the env\n","env = gym.make(env_id)\n","\n","# Create the evaluation env\n","eval_env = gym.make(env_id)\n","\n","# Get the state space and action space\n","s_size = env.observation_space.shape[0]\n","a_size = env.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMLFrjiBNLYJ"},"outputs":[],"source":["print(\"_____OBSERVATION SPACE_____ \\n\")\n","print(\"The State Space is: \", s_size)\n","print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lu6t4sRNNWkN"},"outputs":[],"source":["print(\"\\n _____ACTION SPACE_____ \\n\")\n","print(\"The Action Space is: \", a_size)\n","print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"]},{"cell_type":"markdown","metadata":{"id":"49kogtxBODX8"},"source":["So we want:\n","- Two fully connected layers (fc1 and fc2).\n","- Using ReLU as activation function of fc1\n","- Using Softmax to output a probability distribution over actions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2LHcHhVZvPZ"},"outputs":[],"source":["class Policy(nn.Module):\n","    def __init__(self, s_size, a_size, h_size):\n","        super(Policy, self).__init__()\n","        # Create two fully connected layers\n","\n","\n","\n","    def forward(self, x):\n","        # Define the forward pass\n","        # state goes to fc1 then we apply ReLU activation function\n","\n","        # fc1 outputs goes to fc2\n","\n","        # We output the softmax\n","\n","    def act(self, state):\n","        \"\"\"\n","        Given a state, take action\n","        \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        probs = self.forward(state).cpu()\n","        m = Categorical(probs)\n","        action = np.argmax(m)\n","        return action.item(), m.log_prob(action)"]},{"cell_type":"markdown","metadata":{"id":"rOMrdwSYOWSC"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGdhRSVrOV4K"},"outputs":[],"source":["class Policy(nn.Module):\n","    def __init__(self, s_size, a_size, h_size):\n","        super(Policy, self).__init__()\n","        self.fc1 = nn.Linear(s_size, h_size)\n","        self.fc2 = nn.Linear(h_size, a_size)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return F.softmax(x, dim=1)\n","\n","    def act(self, state):\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        probs = self.forward(state).cpu()\n","        m = Categorical(probs)\n","        action = np.argmax(m)\n","        return action.item(), m.log_prob(action)"]},{"cell_type":"markdown","metadata":{"id":"ZTGWL4g2eM5B"},"source":["I make a mistake, can you guess where?\n","\n","- To find out let's make a forward pass:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwnqGBCNePor"},"outputs":[],"source":["debug_policy = Policy(s_size, a_size, 64).to(device)\n","debug_policy.act(env.reset())"]},{"cell_type":"markdown","metadata":{"id":"14UYkoxCPaor"},"source":["- Here we see that the error says `ValueError: The value argument to log_prob must be a Tensor`\n","\n","- It means that `action` in `m.log_prob(action)` must be a Tensor **but it's not.**\n","\n","- Do you know why? Check the act function and try to see why it does not work.\n","\n","Advice ðŸ’¡: Something is wrong in this implementation. Remember that we act function **we want to sample an action from the probability distribution over actions**.\n"]},{"cell_type":"markdown","metadata":{"id":"gfGJNZBUP7Vn"},"source":["### (Real) Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ho_UHf49N9i4"},"outputs":[],"source":["class Policy(nn.Module):\n","    def __init__(self, s_size, a_size, h_size):\n","        super(Policy, self).__init__()\n","        self.fc1 = nn.Linear(s_size, h_size)\n","        self.fc2 = nn.Linear(h_size, a_size)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return F.softmax(x, dim=1)\n","\n","    def act(self, state):\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        probs = self.forward(state).cpu()\n","        m = Categorical(probs)\n","        action = m.sample()\n","        return action.item(), m.log_prob(action)"]},{"cell_type":"markdown","metadata":{"id":"rgJWQFU_eUYw"},"source":["By using CartPole, it was easier to debug since **we know that the bug comes from our integration and not from our simple environment**."]},{"cell_type":"markdown","source":["- Since **we want to sample an action from the probability distribution over actions**, we can't use `action = np.argmax(m)` since it will always output the action that have the highest probability.\n","\n","- We need to replace with `action = m.sample()` that will sample an action from the probability distribution P(.|s)"],"metadata":{"id":"c-20i7Pk0l1T"}},{"cell_type":"markdown","metadata":{"id":"4MXoqetzfIoW"},"source":["### Let's build the Reinforce Training Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOdv8Q9NfLK7"},"outputs":[],"source":["def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n","    # Help us to calculate the score during the training\n","    scores_deque = deque(maxlen=100)\n","    scores = []\n","    # Line 3 of pseudocode\n","    for i_episode in range(1, n_training_episodes+1):\n","        saved_log_probs = []\n","        rewards = []\n","        state = # TODO: reset the environment\n","        # Line 4 of pseudocode\n","        for t in range(max_t):\n","            action, log_prob = # TODO get the action\n","            saved_log_probs.append(log_prob)\n","            state, reward, done, _ = # TODO: take an env step\n","            rewards.append(reward)\n","            if done:\n","                break\n","        scores_deque.append(sum(rewards))\n","        scores.append(sum(rewards))\n","\n","        # Line 6 of pseudocode: calculate the return\n","        returns = deque(maxlen=max_t)\n","        n_steps = len(rewards)\n","        # Compute the discounted returns at each timestep,\n","        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n","\n","        # In O(N) time, where N is the number of time steps\n","        # (this definition of the discounted return G_t follows the definition of this quantity\n","        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n","        # G_t = r_(t+1) + r_(t+2) + ...\n","\n","        # Given this formulation, the returns at each timestep t can be computed\n","        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n","        # G_t = r_(t+1) + gamma*G_(t+1)\n","        # G_(t-1) = r_t + gamma* G_t\n","        # (this follows a dynamic programming approach, with which we memorize solutions in order\n","        # to avoid computing them multiple times)\n","\n","        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n","        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n","\n","\n","        ## Given the above, we calculate the returns at timestep t as:\n","        #               gamma[t] * return[t] + reward[t]\n","        #\n","        ## We compute this starting from the last timestep to the first, in order\n","        ## to employ the formula presented above and avoid redundant computations that would be needed\n","        ## if we were to do it from first to last.\n","\n","        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n","        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n","        ## a normal python list would instead require O(N) to do this.\n","        for t in range(n_steps)[::-1]:\n","            disc_return_t = (returns[0] if len(returns)>0 else 0)\n","            returns.appendleft(    ) # TODO: complete here\n","\n","        ## standardization of the returns is employed to make training more stable\n","        eps = np.finfo(np.float32).eps.item()\n","\n","        ## eps is the smallest representable float, which is\n","        # added to the standard deviation of the returns to avoid numerical instabilities\n","        returns = torch.tensor(returns)\n","        returns = (returns - returns.mean()) / (returns.std() + eps)\n","\n","        # Line 7:\n","        policy_loss = []\n","        for log_prob, disc_return in zip(saved_log_probs, returns):\n","            policy_loss.append(-log_prob * disc_return)\n","        policy_loss = torch.cat(policy_loss).sum()\n","\n","        # Line 8: PyTorch prefers gradient descent\n","        optimizer.zero_grad()\n","        policy_loss.backward()\n","        optimizer.step()\n","\n","        if i_episode % print_every == 0:\n","            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n","\n","    return scores"]},{"cell_type":"markdown","metadata":{"id":"YB0Cxrw1StrP"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCNvyElRStWG"},"outputs":[],"source":["def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n","    # Help us to calculate the score during the training\n","    scores_deque = deque(maxlen=100)\n","    scores = []\n","    # Line 3 of pseudocode\n","    for i_episode in range(1, n_training_episodes+1):\n","        saved_log_probs = []\n","        rewards = []\n","        state = env.reset()\n","        # Line 4 of pseudocode\n","        for t in range(max_t):\n","            action, log_prob = policy.act(state)\n","            saved_log_probs.append(log_prob)\n","            state, reward, done, _ = env.step(action)\n","            rewards.append(reward)\n","            if done:\n","                break\n","        scores_deque.append(sum(rewards))\n","        scores.append(sum(rewards))\n","\n","        # Line 6 of pseudocode: calculate the return\n","        returns = deque(maxlen=max_t)\n","        n_steps = len(rewards)\n","        # Compute the discounted returns at each timestep,\n","        # as\n","        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n","        #\n","        # In O(N) time, where N is the number of time steps\n","        # (this definition of the discounted return G_t follows the definition of this quantity\n","        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n","        # G_t = r_(t+1) + r_(t+2) + ...\n","\n","        # Given this formulation, the returns at each timestep t can be computed\n","        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n","        # G_t = r_(t+1) + gamma*G_(t+1)\n","        # G_(t-1) = r_t + gamma* G_t\n","        # (this follows a dynamic programming approach, with which we memorize solutions in order\n","        # to avoid computing them multiple times)\n","\n","        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n","        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n","\n","\n","        ## Given the above, we calculate the returns at timestep t as:\n","        #               gamma[t] * return[t] + reward[t]\n","        #\n","        ## We compute this starting from the last timestep to the first, in order\n","        ## to employ the formula presented above and avoid redundant computations that would be needed\n","        ## if we were to do it from first to last.\n","\n","        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n","        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n","        ## a normal python list would instead require O(N) to do this.\n","        for t in range(n_steps)[::-1]:\n","            disc_return_t = (returns[0] if len(returns)>0 else 0)\n","            returns.appendleft( gamma*disc_return_t + rewards[t]   )\n","\n","        ## standardization of the returns is employed to make training more stable\n","        eps = np.finfo(np.float32).eps.item()\n","        ## eps is the smallest representable float, which is\n","        # added to the standard deviation of the returns to avoid numerical instabilities\n","        returns = torch.tensor(returns)\n","        returns = (returns - returns.mean()) / (returns.std() + eps)\n","\n","        # Line 7:\n","        policy_loss = []\n","        for log_prob, disc_return in zip(saved_log_probs, returns):\n","            policy_loss.append(-log_prob * disc_return)\n","        policy_loss = torch.cat(policy_loss).sum()\n","\n","        # Line 8: PyTorch prefers gradient descent\n","        optimizer.zero_grad()\n","        policy_loss.backward()\n","        optimizer.step()\n","\n","        if i_episode % print_every == 0:\n","            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n","\n","    return scores"]},{"cell_type":"markdown","metadata":{"id":"RIWhQyJjfpEt"},"source":["##  Train it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"utRe1NgtVBYF"},"outputs":[],"source":["cartpole_hyperparameters = {\n","    \"h_size\": 16,\n","    \"n_training_episodes\": 1000,\n","    \"n_evaluation_episodes\": 10,\n","    \"max_t\": 1000,\n","    \"gamma\": 1.0,\n","    \"lr\": 1e-2,\n","    \"env_id\": env_id,\n","    \"state_space\": s_size,\n","    \"action_space\": a_size,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3lWyVXBVfl6"},"outputs":[],"source":["# Create policy and place it to the device\n","cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n","cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGf-hQCnfouB"},"outputs":[],"source":["scores = reinforce(cartpole_policy,\n","                   cartpole_optimizer,\n","                   cartpole_hyperparameters[\"n_training_episodes\"],\n","                   cartpole_hyperparameters[\"max_t\"],\n","                   cartpole_hyperparameters[\"gamma\"],\n","                   100)"]},{"cell_type":"markdown","metadata":{"id":"Qajj2kXqhB3g"},"source":["## Define evaluation method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FamHmxyhBEU"},"outputs":[],"source":["def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n","  \"\"\"\n","  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n","  :param env: The evaluation environment\n","  :param n_eval_episodes: Number of episode to evaluate the agent\n","  :param policy: The Reinforce agent\n","  \"\"\"\n","  episode_rewards = []\n","  for episode in range(n_eval_episodes):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards_ep = 0\n","\n","    for step in range(max_steps):\n","      action, _ = policy.act(state)\n","      new_state, reward, done, info = env.step(action)\n","      total_rewards_ep += reward\n","\n","      if done:\n","        break\n","      state = new_state\n","    episode_rewards.append(total_rewards_ep)\n","  mean_reward = np.mean(episode_rewards)\n","  std_reward = np.std(episode_rewards)\n","\n","  return mean_reward, std_reward"]},{"cell_type":"markdown","metadata":{"id":"xdH2QCrLTrlT"},"source":["## Evaluate our agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohGSXDyHh0xx"},"outputs":[],"source":["evaluate_agent(eval_env,\n","               cartpole_hyperparameters[\"max_t\"],\n","               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n","               cartpole_policy)"]},{"cell_type":"markdown","source":["## Second agent: PixelCopter\n"],"metadata":{"id":"JNLVmKKVKA6j"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBSc8mlfyin3"},"outputs":[],"source":["env_id = \"Pixelcopter-PLE-v0\"\n","env = gym.make(env_id)\n","eval_env = gym.make(env_id)\n","s_size = env.observation_space.shape[0]\n","a_size = env.action_space.n"]},{"cell_type":"code","source":["print(\"_____OBSERVATION SPACE_____ \\n\")\n","print(\"The State Space is: \", s_size)\n","print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"],"metadata":{"id":"L5u_zAHsKBy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n _____ACTION SPACE_____ \\n\")\n","print(\"The Action Space is: \", a_size)\n","print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"],"metadata":{"id":"D7yJM9YXKNbq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NNWvlyvzalXr"},"source":["The observation space (7) ðŸ‘€:\n","- player y position\n","- player velocity\n","- player distance to floor\n","- player distance to ceiling\n","- next block x distance to player\n","- next blocks top y location\n","- next blocks bottom y location\n","\n","The action space(2) ðŸŽ®:\n","- Up (press accelerator)\n","- Do nothing (don't press accelerator)\n","\n","The reward function ðŸ’°:\n","- For each vertical block it passes through it gains a positive reward of +1. Each time a terminal state reached it receives a negative reward of -1."]},{"cell_type":"markdown","source":["### Define the new Policy\n"],"metadata":{"id":"aV1466QP8crz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1eBkCiX2X_S"},"outputs":[],"source":["class Policy(nn.Module):\n","    def __init__(self, s_size, a_size, h_size):\n","        super(Policy, self).__init__()\n","        # Define the three layers here\n","\n","    def forward(self, x):\n","        # Define the forward process here\n","        return F.softmax(x, dim=1)\n","\n","    def act(self, state):\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        probs = self.forward(state).cpu()\n","        m = Categorical(probs)\n","        action = m.sample()\n","        return action.item(), m.log_prob(action)"]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"47iuAFqV8Ws-"}},{"cell_type":"code","source":["class Policy(nn.Module):\n","    def __init__(self, s_size, a_size, h_size):\n","        super(Policy, self).__init__()\n","        self.fc1 = nn.Linear(s_size, h_size)\n","        self.fc2 = nn.Linear(h_size, h_size*2)\n","        self.fc3 = nn.Linear(h_size*2, a_size)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return F.softmax(x, dim=1)\n","\n","    def act(self, state):\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        probs = self.forward(state).cpu()\n","        m = Categorical(probs)\n","        action = m.sample()\n","        return action.item(), m.log_prob(action)"],"metadata":{"id":"wrNuVcHC8Xu7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SM1QiGCSbBkM"},"source":["### Define the hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0uujOR_ypB6"},"outputs":[],"source":["pixelcopter_hyperparameters = {\n","    \"h_size\": 64,\n","    \"n_training_episodes\": 50000,\n","    \"n_evaluation_episodes\": 10,\n","    \"max_t\": 10000,\n","    \"gamma\": 0.99,\n","    \"lr\": 1e-4,\n","    \"env_id\": env_id,\n","    \"state_space\": s_size,\n","    \"action_space\": a_size,\n","}"]},{"cell_type":"markdown","source":["###  Train it"],"metadata":{"id":"wyvXTJWm9GJG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mM2P_ckysFE"},"outputs":[],"source":["# Create policy and place it to the device\n","# torch.manual_seed(50)\n","pixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\n","pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1HEqP-fy-Rf"},"outputs":[],"source":["scores = reinforce(pixelcopter_policy,\n","                   pixelcopter_optimizer,\n","                   pixelcopter_hyperparameters[\"n_training_episodes\"],\n","                   pixelcopter_hyperparameters[\"max_t\"],\n","                   pixelcopter_hyperparameters[\"gamma\"],\n","                   1000)"]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["rOMrdwSYOWSC","gfGJNZBUP7Vn","YB0Cxrw1StrP","47iuAFqV8Ws-"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}