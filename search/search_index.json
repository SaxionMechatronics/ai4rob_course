{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI for Robotics Course","text":"<p>This course covers AI concepts applied to robotics. It's organized into three modules: foundations, deep learning, and reinforcement learning.</p> <pre><code>graph TD\n    A[AI for Robotics Course] --&gt; B[Foundations]\n    A --&gt; C[Deep Learning]\n    A --&gt; D[Reinforcement Learning]\n    D --&gt; H[Practical Applications]\n    B --&gt; H\n    C --&gt; H</code></pre>"},{"location":"#course-modules","title":"Course Modules","text":"FoundationsDeep LearningReinforcement Learning"},{"location":"#foundations","title":"Foundations","text":"<p>Covers fundamental concepts for AI in robotics.</p> <ul> <li>Core robotics principles</li> <li>Basic AI concepts for robotics</li> </ul> <p>Get Started with Foundations </p>"},{"location":"#deep-learning","title":"Deep Learning","text":"<p>Neural networks and deep learning for robotic applications.</p> <ul> <li>Neural network architectures</li> <li>Deep learning for perception and control</li> <li>Implementation examples</li> </ul> <p>Get Started with Deep Learning </p>"},{"location":"#reinforcement-learning","title":"Reinforcement Learning","text":"<p>How robots learn behaviors through environment interaction.</p> <ul> <li>Markov Decision Processes (MDPs)</li> <li>Value-based and policy-based methods</li> <li>Q-Learning and Policy Gradients</li> <li>Implementation examples</li> </ul> <p>Get Started with Reinforcement Learning </p>"},{"location":"#course-features","title":"Course Features","text":"<p>Each module includes:</p> Feature Description Theory Concept explanations with mathematical foundations Interactive Notebooks Google Colab notebooks for practice Quizzes Multiple choice questions Code Examples Implementations you can run and modify <p>Setup</p> <p>Everything runs in simulation (Google Colab). No hardware or local setup required.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Recommended background:</p> <ul> <li> Basic programming knowledge (conditional, loops, variables)</li> <li> Python familiarity (all examples use Python)</li> <li> Linear algebra and calculus basics (vectors, matrices, derivatives)</li> </ul> Not required <ul> <li>No ROS experience needed</li> <li>No hardware deployment experience needed</li> <li>No special equipment needed</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Suggested order:</p> <ol> <li>Foundations</li> <li>Deep Learning</li> <li>Reinforcement Learning</li> </ol>"},{"location":"conclusion/","title":"Wrap-up and takeaway message!","text":"<p>Congratulations on finishing the course! Now that you have a fundamental understanding of how AI works and can be applied, it\u2019s important to remember that AI is not a universal solution. Careful consideration is needed to decide when it should be used. AI has great potential and has already extended the operational capabilities of many systems. In robotics, for example, traditional techniques often struggled with generalization, especially for products where manual modeling was difficult and unexpected situations occurred. In these cases, a data-driven learning approach can be very effective. Instead of explicitly modeling everything, models can now be learned from data. This makes AI an extension of existing tools, expanding the generalizability of systems. However, using AI everywhere comes with trade-offs. It requires large amounts of data and significant computational resources, which can be a disadvantage. Additionally, AI solutions are often highly specific: an algorithm that works well on a quadrupedal robot may not necessarily work on a humanoid robot. This \u201ccorrespondence problem\u201d remains a challenge.  In conclusion, AI should be seen as a complementary tool rather than a replacement. It enhances generalizability and adaptability, but its application must be strategic, balancing benefits against resource requirements and domain-specific constraints.</p>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#books","title":"Books","text":""},{"location":"resources/#online-courses","title":"Online Courses","text":""},{"location":"resources/#research-papers","title":"Research Papers","text":""},{"location":"resources/#tools-libraries","title":"Tools &amp; Libraries","text":""},{"location":"deep_learning/","title":"Deep Learning Module","text":""},{"location":"deep_learning/#course-title","title":"Course Title","text":"<p>In-Depth Coding: Building a Deep Neural Network (DNN) from End to End</p>"},{"location":"deep_learning/#purpose","title":"Purpose","text":"<p>This course provides a structured, experiment-driven journey through the entire process of developing, training, and optimizing a deep neural network (DNN) from scratch. Each module builds upon the previous one, illustrating how incremental architectural and training modifications impact performance, generalization, and computational efficiency.</p> <p>The lessons are designed to be self-contained, progressive, and practical, making it easy to understand both the conceptual reasoning and the coding implementation behind each stage.</p>"},{"location":"deep_learning/#topics-covered","title":"Topics Covered","text":"Module Focus Area Key Concepts Expected Outcome Code 1 \u2013 Model Setup Initial Environment Setup Data transforms, loaders, baseline training/testing Establish functional training loop and baseline performance Code 2 \u2013 Model Skeleton Core Architecture Simplified base network design Create a reusable, consistent model structure Code 3 \u2013 Lighter Model Parameter Reduction Model compactness, efficiency Achieve similar accuracy with fewer parameters Code 4 \u2013 Batch Normalization Training Stability BatchNorm layers, faster convergence Improve learning efficiency and accuracy Code 5 \u2013 Regularization Overfitting Control Dropout layers, regularization theory Enhance generalization and prevent overfitting Code 6 \u2013 Global Average Pooling (GAP) Architectural Simplification GAP layer, removing dense kernels Reduce model complexity while maintaining performance Code 7 \u2013 Increasing Capacity Balanced Expansion Deeper architecture, added layers Recover performance by controlled capacity increase Code 8 \u2013 Correct MaxPooling Location Receptive Field Optimization Pooling placement, Dropout tuning Improve feature abstraction and stability Code 9 \u2013 Image Augmentation Data Diversity Rotation, transformation, augmentation Strengthen robustness and reduce underfitting Code 10 \u2013 Learning Rate Scheduling Convergence Optimization Step-based LR scheduling Achieve stable high accuracy efficiently"},{"location":"deep_learning/#core-learning-dimensions","title":"Core Learning Dimensions","text":"<ol> <li>Architectural Design \u2013 How convolutional layers, pooling, and normalization influence feature extraction and model efficiency.  </li> <li>Regularization Techniques \u2013 Methods to prevent overfitting and balance capacity vs. generalization.  </li> <li>Training Dynamics \u2013 The importance of learning rate control, optimizers, and schedulers.  </li> <li>Data Engineering \u2013 Using augmentation to enhance diversity and improve model robustness.  </li> <li>Performance Analysis \u2013 Interpreting accuracy, loss, and parameter trade-offs across model versions.  </li> </ol>"},{"location":"deep_learning/#progression-summary","title":"Progression Summary","text":"<p>The course starts with a large, over-parameterized model (6.3M parameters) and methodically refines it to a compact, efficient version (~13.8k parameters) that still achieves ~99.5% accuracy. By the end, learners gain a deep understanding of the engineering decisions that drive real-world neural network optimization.</p>"},{"location":"deep_learning/#end-goal","title":"End Goal","text":"<p>Design a compact DNN that meets the following performance criteria:</p> Criterion Target Test Accuracy \u2265 99.4% (consistent across final epochs) Training Epochs \u2264 15 Total Parameters \u2264 8,000 <p>This overview provides the conceptual map for the entire learning journey. Each subsequent section (Code 1\u201310) will dive deeper into the how and why behind every improvement.</p>"},{"location":"deep_learning/#module-contents","title":"Module Contents","text":"<ul> <li>Introduction to Git | Python 101 | Pytorch 101 - Basic intution into Git, Python and Pytorch </li> <li>Introduction to Deep Learning - Overview of course</li> <li>Code 1 - Model Setup</li> <li>Quiz 1</li> <li>Code 2 - Model Skeleton</li> <li>Quiz 2</li> <li>Code 3 - Lighter Model</li> <li>Quiz 3</li> <li>Code 4 - Batch Normalization Integration</li> <li>Quiz 4</li> <li>Code 5 - Regularization</li> <li>Quiz 5</li> <li>Code 6 - Global Average Pooling</li> <li>Quiz 6</li> <li>Code 7 - Increasing Model Capacity</li> <li>Quiz 7</li> <li>Code 8 - Correcting MaxPooling Location</li> <li>Quiz 8</li> <li>Code 9 - Image Augmentation</li> <li>Quiz 9</li> <li>Code 10 - Learning Rate Scheduling</li> <li>Quiz 10</li> <li>Summary - Summary</li> <li>Quiz</li> <li>Backward Propagation - Inroducion to Backward Propagation</li> </ul> <p>Start with Introduction to Git | Python 101 | Pytorch 101</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/","title":"Introduction to Git &amp; Version Control Systems","text":""},{"location":"deep_learning/0_Git_Python_Pytorch/#1-introduction-to-git","title":"1. Introduction to Git","text":"<p>Git is a free, open-source, distributed Version Control System (VCS) designed to efficiently manage projects of all sizes\u2014from small scripts to large-scale software systems. It enables developers to track changes, collaborate effectively, and maintain a complete history of project evolution.</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#2-what-is-a-version-control-system-vcs","title":"2. What is a Version Control System (VCS)?","text":"<p>A Version Control System is a software tool that helps developers:</p> <ul> <li>Manage and track changes to their code over time  </li> <li>Collaborate without overwriting each other\u2019s work  </li> <li>Preserve a complete history of project modifications</li> </ul> <p>As developers, we continuously create, save, modify, and refine our work. A VCS ensures these iterative changes are captured, organized, and recoverable.</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#3-why-we-need-a-version-control-system","title":"3. Why We Need a Version Control System","text":""},{"location":"deep_learning/0_Git_Python_Pytorch/#a-track-and-manage-code-changes","title":"a. Track and Manage Code Changes","text":"<p>A VCS records: - What changed - Who made the change - When it was made - Why the change was necessary (via commit messages)</p> <p>This clarity is essential for debugging, audits, and understanding long-term development decisions.</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#b-enable-collaboration","title":"b. Enable Collaboration","text":"<p>Multiple developers can work on the same project without the risk of: - Overwriting each other\u2019s code - Losing progress - Introducing conflicting changes</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#c-roll-back-when-needed","title":"c. Roll Back When Needed","text":"<p>Mistakes happen. A VCS allows developers to: - Restore previous versions - Recover deleted or broken code - Experiment freely, knowing they can revert at any time</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#d-maintain-a-complete-project-history","title":"d. Maintain a Complete Project History","text":"<p>Every change becomes part of a structured timeline of development. This allows teams to: - Analyze how the codebase evolved - Compare versions - Understand feature progression</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#in-summary-a-vcs-helps-answer","title":"In summary, a VCS helps answer:","text":"<ul> <li>When was a change made?  </li> <li>Why was it made (commit message)?  </li> <li>What exactly changed?  </li> <li>Who changed it?  </li> <li>Can we revert to an earlier version? </li> <li>Can we branch, experiment, and merge safely?</li> </ul>"},{"location":"deep_learning/0_Git_Python_Pytorch/#4-essential-git-commands","title":"4. Essential Git Commands","text":""},{"location":"deep_learning/0_Git_Python_Pytorch/#41-git-init","title":"4.1 <code>git init</code>","text":"<p>Initializes a new Git repository in the current directory. This directory becomes your working directory where Git tracks changes.</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#42-git-add","title":"4.2 <code>git add</code>","text":"<p>Stages files for commit. The staging area acts as a temporary holding space before you record changes.</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#43-git-commit","title":"4.3 <code>git commit</code>","text":"<p>Creates a snapshot of the project at a specific moment. Each commit includes: - The staged changes - A descriptive message explaining what and why</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#44-git-push","title":"4.4 <code>git push</code>","text":"<p>Uploads your local commits to a remote repository so others can access your updates.</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#45-git-pull","title":"4.5 <code>git pull</code>","text":"<p>Downloads and integrates changes from a remote repository so you can stay up to date with the latest updates from collaborators.</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#46-git-checkout","title":"4.6 <code>git checkout</code>","text":"<p>Switches between branches or different versions of the project. This allows developers to: - Try new features - Test ideas - Work safely without affecting the main branch</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#5-git-cheat-sheet-for-quick-reference","title":"5 Git Cheat Sheet for quick reference","text":""},{"location":"deep_learning/0_Git_Python_Pytorch/#51-create-a-repository","title":"5.1 Create a Repository","text":"Action Command Initialize a new local repository <code>git init [project name]</code> Clone/download an existing repository <code>git clone my_url</code>"},{"location":"deep_learning/0_Git_Python_Pytorch/#52-observe-your-repository","title":"5.2 Observe Your Repository","text":"Action Command List new/modified files not yet committed <code>git status</code> Show changes in unstaged files <code>git diff</code> Show changes in staged files <code>git diff --cached</code> Show all staged &amp; unstaged changes <code>git diff HEAD</code> Show differences between two commits <code>git diff commit1 commit2</code> Show who changed each line of a file <code>git blame [file]</code> Show file changes for a commit/file <code>git show [commit]:[file]</code> Show full commit history <code>git log</code> Show commit history for file/dir with diffs <code>git log -p [file/directory]</code>"},{"location":"deep_learning/0_Git_Python_Pytorch/#53-working-with-branches","title":"5.3 Working with Branches","text":"Action Command List all local branches <code>git branch</code> List all branches (local + remote) <code>git branch -av</code> Switch to a branch <code>git checkout my_branch</code> Create a new branch <code>git branch new_branch</code> Delete a branch <code>git branch -d my_branch</code> Merge branch_a into branch_b <code>git checkout branch_b</code> \u2192 <code>git merge branch_a</code> Tag the current commit <code>git tag my_tag</code>"},{"location":"deep_learning/0_Git_Python_Pytorch/#54-make-a-change","title":"5.4 Make a Change","text":"Action Command Stage a file <code>git add [file]</code> Stage all modified files <code>git add .</code> Commit staged changes <code>git commit -m \"message\"</code> Commit all tracked files <code>git commit -am \"message\"</code> Unstage a file (keep changes) <code>git reset [file]</code> Revert everything to last commit <code>git reset --hard</code>"},{"location":"deep_learning/0_Git_Python_Pytorch/#55-synchronize-remote-operations","title":"5.5. Synchronize (Remote Operations)","text":"Action Command Fetch latest changes (no merge) <code>git fetch</code> Fetch + merge latest changes <code>git pull</code> Fetch + rebase (clean history) <code>git pull --rebase</code> Push local commits to remote <code>git push</code>"},{"location":"deep_learning/0_Git_Python_Pytorch/#56-help","title":"5.6 Help","text":"Action Command Show help for a Git command <code>git command --help</code> Official GitHub training https://training.github.com/"},{"location":"deep_learning/0_Git_Python_Pytorch/#end-of-module-summary","title":"End of Module Summary","text":"<p>By understanding the purpose of version control and mastering basic Git commands, developers can work more confidently, collaborate more effectively, and maintain an organized, reliable codebase.</p>"},{"location":"deep_learning/0_Git_Python_Pytorch/#-","title":"---","text":""},{"location":"deep_learning/0_Git_Python_Pytorch/#link-to-the-google-colab-for-python-101","title":"Link to the Google Colab for Python 101","text":""},{"location":"deep_learning/0_Git_Python_Pytorch/#link-to-the-google-colab-for-pytorch-101","title":"Link to the Google Colab for Pytorch 101","text":""},{"location":"deep_learning/0_Git_Python_Pytorch/#_1","title":"Introduction to Git | Python 101 | Pytorch 101","text":"<p>\u2190 Back to Index Start with Introduction to Deep Learning \u2192</p>"},{"location":"deep_learning/10_Code/","title":"Learning Rate Scheduling","text":""},{"location":"deep_learning/10_Code/#objectives","title":"Objectives","text":"<ul> <li>Introduce a Learning Rate (LR) Scheduler to optimize convergence  </li> <li>Reduce learning rate dynamically after defined epochs  </li> <li>Observe improvements in stability and final accuracy  </li> </ul>"},{"location":"deep_learning/10_Code/#model-configuration","title":"Model Configuration","text":""},{"location":"deep_learning/10_Code/#adjustments","title":"Adjustments","text":"<ul> <li>Implemented step-based LR reduction (\u00f7 10 after 6th epoch)  </li> <li>Maintained 13.8k-parameter architecture  </li> <li>Trained for 20 epochs to monitor LR effects  </li> </ul>"},{"location":"deep_learning/10_Code/#results","title":"Results","text":"Metric Value Total Parameters 13,800 Best Training Accuracy 99.21% Best Test Accuracy 99.45% (9th Epoch), 99.48% (20th Epoch)"},{"location":"deep_learning/10_Code/#analysis","title":"Analysis","text":""},{"location":"deep_learning/10_Code/#observations","title":"Observations","text":"<ul> <li>Scheduler accelerates early convergence  </li> <li>Plateau accuracy similar to previous best (~ 99.5%)  </li> <li>Demonstrates importance of fine-tuned LR strategy  </li> </ul>"},{"location":"deep_learning/10_Code/#link-to-the-google-colab","title":"Link to the Google Colab","text":""},{"location":"deep_learning/10_Code/#_1","title":"Learning Rate Scheduling","text":""},{"location":"deep_learning/10_Code/#check-your-understanding","title":"Check your understanding","text":""},{"location":"deep_learning/10_Code/#quiz-10","title":"Quiz 10","text":"<p>\u2190 Back to Image Augmentation Start with Summary \u2192</p>"},{"location":"deep_learning/10_mcq/","title":"Quiz 10","text":"Submit Quiz"},{"location":"deep_learning/10_mcq/#question-1","title":"Question 1","text":"<p> <p>What new feature was introduced in Code 10?</p> <ul><li> <p>Weight decay regularization</p></li><li> <p>Learning Rate (LR) Scheduler</p></li><li> <p>Additional convolutional layer</p></li><li> <p>New activation function</p></li></ul> </p>"},{"location":"deep_learning/10_mcq/#question-2","title":"Question 2","text":"<p> <p>What was the main purpose of adding a Learning Rate Scheduler?</p> <ul><li> <p>To improve convergence by reducing the learning rate over time</p></li><li> <p>To randomize gradients during training</p></li><li> <p>To keep the learning rate constant</p></li><li> <p>To increase the model depth</p></li></ul> </p>"},{"location":"deep_learning/10_mcq/#question-3","title":"Question 3","text":"<p> <p>How was the scheduler configured in this implementation?</p> <ul><li> <p>Increase LR every 5 epochs</p></li><li> <p>Decrease LR by a factor of 10 after the 6th epoch</p></li><li> <p>Keep LR constant for all epochs</p></li><li> <p>Randomly change LR between 0.001 and 0.1</p></li></ul> </p>"},{"location":"deep_learning/10_mcq/#question-4","title":"Question 4","text":"<p> <p>What was the final test accuracy achieved after adding the LR scheduler?</p> <ul><li> <p>98 %</p></li><li> <p>99.5 %</p></li><li> <p>97 %</p></li><li> <p>100 %</p></li></ul> </p>"},{"location":"deep_learning/10_mcq/#question-5","title":"Question 5","text":"<p> <p>What is the main challenge when using a Learning Rate Scheduler?</p> <ul><li> <p>Choosing the right schedule for stable and efficient convergence</p></li><li> <p>Deciding the correct batch size</p></li><li> <p>Removing normalization layers</p></li><li> <p>Increasing the dropout rate at each step</p></li></ul> </p>"},{"location":"deep_learning/11_Summary/","title":"Final Summary","text":""},{"location":"deep_learning/11_Summary/#overview","title":"Overview","text":"<p>This concludes the progressive development of a Deep Neural Network (DNN) trained end-to-end. Across the ten iterations, we explored multiple strategies to optimize performance, reduce model size, and improve generalization through a structured, experimental approach.</p>"},{"location":"deep_learning/11_Summary/#key-learnings","title":"Key Learnings","text":""},{"location":"deep_learning/11_Summary/#architectural-evolution","title":"Architectural Evolution","text":"<ul> <li>Started from a 6.3M-parameter baseline and optimized down to 13.8k parameters with comparable accuracy.  </li> <li>Introduced and analyzed various architectural components:</li> <li>Batch Normalization for improved convergence  </li> <li>Dropout and Regularization to reduce overfitting  </li> <li>Global Average Pooling (GAP) for efficiency  </li> <li>MaxPooling Placement adjustments for receptive field alignment  </li> <li>Learning Rate Scheduling for faster and more stable optimization  </li> <li>Demonstrated the impact of data augmentation on generalization and robustness.  </li> </ul>"},{"location":"deep_learning/11_Summary/#performance-summary","title":"Performance Summary","text":"Stage Parameters Best Train Accuracy Best Test Accuracy Initial Setup 6.3M 99.99% 99.24% Optimized Final 13.8k 99.21% 99.48%"},{"location":"deep_learning/11_Summary/#observations","title":"Observations","text":"<ul> <li>The systematic reduction in model capacity improved efficiency without a significant drop in accuracy.  </li> <li>Proper use of normalization, pooling, and regularization provided stability in both training and testing.  </li> <li>Learning rate control and augmentations ensured consistent results across epochs.  </li> <li>Iterative experimentation is crucial for identifying the most effective architectural changes.  </li> </ul>"},{"location":"deep_learning/11_Summary/#assignment","title":"Assignment","text":""},{"location":"deep_learning/11_Summary/#target","title":"Target","text":"<p>You are now challenged to design and train a compact DNN that meets all of the following criteria:</p> Criterion Goal Test Accuracy \u2265 99.4% (consistently across final epochs) Training Epochs \u2264 15 Model Parameters \u2264 8,000"},{"location":"deep_learning/11_Summary/#check-your-understanding","title":"Check your understanding","text":""},{"location":"deep_learning/11_Summary/#quiz","title":"Quiz","text":"<p>\u2190 Back to Learning Rate Scheduling</p>"},{"location":"deep_learning/12_Introduction/","title":"Introduction","text":""},{"location":"deep_learning/12_Introduction/#course-overview","title":"Course Overview","text":"<p>This module, \u201cBuilding a Deep Neural Network (DNN) from End to End,\u201d is designed as a structured, hands-on exploration of the step-by-step development process behind a modern deep learning model. It walks through the evolution of a neural network starting from a heavy, over-parameterized baseline to an efficient, high-performing architecture \u2014 all while maintaining clarity, reproducibility, and analytical insight at each stage.</p> <p>Through ten iterative coding experiments, you will progressively refine the model\u2019s structure, optimize its learning behavior, and achieve consistent high accuracy with minimal parameters.</p>"},{"location":"deep_learning/12_Introduction/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, you will be able to: - Understand the full workflow of training and evaluating a DNN from scratch - Identify and control overfitting, underfitting, and model capacity issues - Apply Batch Normalization, Dropout, and Global Average Pooling (GAP) effectively - Use data augmentation and learning rate scheduling to improve generalization - Build and train a compact, efficient network that achieves strong test accuracy - Analyze and justify architectural changes through results and observations  </p>"},{"location":"deep_learning/12_Introduction/#progressive-learning-path","title":"Progressive Learning Path","text":""},{"location":"deep_learning/12_Introduction/#code-1-model-setup","title":"Code 1 \u2013 Model Setup","text":"<p>Establishes the fundamental training environment: defining transforms, loading data, and implementing the baseline training/testing loops. This step produces a large, overfitted model (~6.3M parameters) and acts as the control case for all subsequent refinements.</p>"},{"location":"deep_learning/12_Introduction/#code-2-model-skeleton","title":"Code 2 \u2013 Model Skeleton","text":"<p>Creates a reusable, simplified model skeleton with around 194k parameters. Serves as the architectural blueprint for all future experiments while maintaining performance above 99% accuracy.</p>"},{"location":"deep_learning/12_Introduction/#code-3-lighter-model","title":"Code 3 \u2013 Lighter Model","text":"<p>Reduces parameter count drastically (~10.7k) to explore compact design. Demonstrates that even lightweight architectures can perform competitively on simple datasets when properly designed.</p>"},{"location":"deep_learning/12_Introduction/#code-4-batch-normalization","title":"Code 4 \u2013 Batch Normalization","text":"<p>Introduces Batch Normalization to stabilize learning, improve gradient flow, and enable higher learning rates. Results in faster convergence and better overall training accuracy.</p>"},{"location":"deep_learning/12_Introduction/#code-5-regularization","title":"Code 5 \u2013 Regularization","text":"<p>Adds Dropout to combat overfitting. Shows that regularization can improve test accuracy and robustness without changing model size.</p>"},{"location":"deep_learning/12_Introduction/#code-6-global-average-pooling-gap","title":"Code 6 \u2013 Global Average Pooling (GAP)","text":"<p>Replaces large, dense kernels with Global Average Pooling, reducing parameters (~6k). Simplifies architecture and demonstrates the trade-off between capacity and generalization.</p>"},{"location":"deep_learning/12_Introduction/#code-7-increasing-capacity","title":"Code 7 \u2013 Increasing Capacity","text":"<p>Reintroduces controlled depth and width expansion to compensate for capacity loss. Balances compactness with performance by adding layers after GAP.</p>"},{"location":"deep_learning/12_Introduction/#code-8-correct-maxpooling-location","title":"Code 8 \u2013 Correct MaxPooling Location","text":"<p>Refines receptive field management by placing MaxPooling at an optimal position (RF = 5). Applies Dropout uniformly and achieves consistent 99.4% test accuracy with no overfitting.</p>"},{"location":"deep_learning/12_Introduction/#code-9-image-augmentation","title":"Code 9 \u2013 Image Augmentation","text":"<p>Incorporates light image augmentation (rotation \u2248 5\u20137\u00b0) to improve robustness. Illustrates how carefully chosen augmentations help the model generalize better to unseen data.</p>"},{"location":"deep_learning/12_Introduction/#code-10-learning-rate-scheduling","title":"Code 10 \u2013 Learning Rate Scheduling","text":"<p>Implements a learning rate scheduler to optimize convergence speed and stability. Reduces the learning rate after fixed epochs, resulting in faster convergence and reproducible accuracy near 99.5%.</p>"},{"location":"deep_learning/12_Introduction/#conceptual-themes","title":"Conceptual Themes","text":"Concept Description Model Simplification Reducing parameters while preserving accuracy Normalization &amp; Regularization Ensuring stable and generalizable learning Pooling Strategies Balancing feature abstraction and efficiency Data Augmentation Expanding dataset diversity for better generalization Learning Rate Control Managing convergence dynamics for optimal performance"},{"location":"deep_learning/12_Introduction/#summary-of-model-evolution","title":"Summary of Model Evolution","text":"Stage Focus Area Parameters Test Accuracy Code 1 Baseline Setup 6.3 M 99.24 % Code 2 Basic Skeleton 194 k 99.02 % Code 3 Lightweight Design 10.7 k 98.98 % Code 4 Batch Normalization 10.9 k 99.3 % Code 5 Dropout Regularization 10.9 k 99.3 % Code 6 GAP Integration 6 k 98.13 % Code 7 Capacity Increase 11.9 k 99.04 % Code 8 Pooling Optimization 13.8 k 99.41 % Code 9 Data Augmentation 13.8 k 99.50 % Code 10 LR Scheduling 13.8 k 99.48 %"},{"location":"deep_learning/12_Introduction/#expected-learning-outcome","title":"Expected Learning Outcome","text":"<p>By following these ten progressive experiments, you will: - Develop an intuitive understanding of architectural trade-offs - Learn to balance model capacity, regularization, and training efficiency - Gain the ability to iterate experimentally toward target accuracy efficiently - Be equipped to design compact, high-performing models for practical applications  </p>"},{"location":"deep_learning/12_Introduction/#how-to-use-this-course","title":"How to Use This Course","text":"<ul> <li>Study each code block sequentially \u2014 each builds upon the previous one  </li> <li>Review the Analysis and Next Steps after each experiment to understand the rationale  </li> <li>Use the final Assignment to consolidate learning and demonstrate mastery  </li> <li>Modify and document your experiments to compare results systematically  </li> </ul> <p>Goal: Achieve a consistent 99.4%+ accuracy with \u2264 8,000 parameters in \u2264 15 epochs, demonstrating mastery of efficient DNN design and optimization.</p> <p>\u2190 Back to Introduction to Git | Python 101 | Pytorch 101 Start with Model Setup \u2192</p>"},{"location":"deep_learning/1_Code/","title":"Model Setup","text":""},{"location":"deep_learning/1_Code/#objectives","title":"Objectives","text":"<ul> <li>Configure the complete training environment and pipeline  </li> <li>Define and apply data transforms  </li> <li>Build data loaders for training and testing  </li> <li>Implement a baseline model architecture  </li> <li>Write the basic training and test loops  </li> <li>Evaluate performance and summarize initial results  </li> </ul>"},{"location":"deep_learning/1_Code/#model-configuration","title":"Model Configuration","text":""},{"location":"deep_learning/1_Code/#setup-completed","title":"Setup Completed","text":"<ul> <li>Training environment initialized  </li> <li>Transforms defined (normalization, augmentation)  </li> <li>Data loaders implemented for train and test datasets  </li> <li>Baseline model code successfully executed end-to-end  </li> <li>Training and testing loops functional  </li> </ul>"},{"location":"deep_learning/1_Code/#results","title":"Results","text":"Metric Value Total Parameters 6.3 Million Best Training Accuracy 99.99% Best Test Accuracy 99.24%"},{"location":"deep_learning/1_Code/#analysis","title":"Analysis","text":""},{"location":"deep_learning/1_Code/#observations","title":"Observations","text":"<ul> <li>The model is significantly heavy relative to the dataset complexity (6.3M parameters).  </li> <li>There is noticeable overfitting:  </li> <li>Training accuracy is near perfect.  </li> <li>Test accuracy shows a generalization gap.  </li> <li>A lighter and more efficient model architecture is required.</li> </ul>"},{"location":"deep_learning/1_Code/#next-steps","title":"Next Steps","text":"<ul> <li>Redesign the model to reduce total parameters  </li> <li>Apply regularization (dropout, additional augmentations)  </li> <li>Tune the learning rate and experiment with schedulers  </li> <li>Introduce early stopping  </li> <li>Track additional metrics (loss curves, confusion matrix, per-class accuracy)</li> </ul>"},{"location":"deep_learning/1_Code/#link-to-the-google-colab","title":"Link to the Google Colab","text":""},{"location":"deep_learning/1_Code/#_1","title":"Model Setup","text":""},{"location":"deep_learning/1_Code/#check-your-understanding","title":"Check your understanding","text":"<p>Quiz 1</p> <p>\u2190 Back to Introduction Start with Model Skeleton \u2192</p>"},{"location":"deep_learning/1_mcq/","title":"Quiz 1","text":"Submit Quiz"},{"location":"deep_learning/1_mcq/#question-1","title":"Question 1","text":"<p> <p>What is the main purpose of Code 1?</p> <ul><li> <p>To tune the learning rate</p></li><li> <p>To set up data loading, transforms, and training loop</p></li><li> <p>To apply batch normalization</p></li><li> <p>To reduce parameters</p></li></ul> </p>"},{"location":"deep_learning/1_mcq/#question-2","title":"Question 2","text":"<p> <p>Approximately how many parameters does the initial model contain?</p> <ul><li> <p>63 k</p></li><li> <p>6.3 M</p></li><li> <p>630 k</p></li><li> <p>630</p></li></ul> </p>"},{"location":"deep_learning/1_mcq/#question-3","title":"Question 3","text":"<p> <p>What issue is most evident in the baseline model?</p> <ul><li> <p>Underfitting</p></li><li> <p>Gradient explosion</p></li><li> <p>Overfitting</p></li><li> <p>Low variance</p></li></ul> </p>"},{"location":"deep_learning/1_mcq/#question-4","title":"Question 4","text":"<p> <p>Why is Code 1 considered a baseline?</p> <ul><li> <p>It has perfect generalization</p></li><li> <p>It is the lightest model possible</p></li><li> <p>It provides reference performance for future experiments</p></li><li> <p>It includes all normalization layers</p></li></ul> </p>"},{"location":"deep_learning/1_mcq/#question-5","title":"Question 5","text":"<p> <p>Which accuracy metric shows model overfitting in this step?</p> <ul><li> <p>Training accuracy &gt;&gt; Test accuracy</p></li><li> <p>Training accuracy \u2248 Test accuracy</p></li><li> <p>Training accuracy &lt; Test accuracy</p></li><li> <p>Both accuracies are low</p></li></ul> </p>"},{"location":"deep_learning/2_Code/","title":"Model Skeleton","text":""},{"location":"deep_learning/2_Code/#objectives","title":"Objectives","text":"<ul> <li>Establish a clean and reusable model skeleton  </li> <li>Maintain structural consistency across future experiments  </li> <li>Keep the implementation minimal without adding complexity  </li> <li>Serve as the foundation for subsequent architectural improvements  </li> </ul>"},{"location":"deep_learning/2_Code/#model-configuration","title":"Model Configuration","text":""},{"location":"deep_learning/2_Code/#skeleton-implemented","title":"Skeleton Implemented","text":"<ul> <li>Core model structure finalized (input \u2192 convolutional blocks \u2192 classifier)  </li> <li>Avoided unnecessary layers or parameters  </li> <li>Forward and backward passes verified  </li> <li>Training and evaluation pipelines remain unchanged  </li> <li>Acts as a stable baseline for further refinements  </li> </ul>"},{"location":"deep_learning/2_Code/#results","title":"Results","text":"Metric Value Total Parameters 194,000 Best Training Accuracy 99.35% Best Test Accuracy 99.02%"},{"location":"deep_learning/2_Code/#analysis","title":"Analysis","text":""},{"location":"deep_learning/2_Code/#observations","title":"Observations","text":"<ul> <li>Parameter count reduced significantly from the initial setup (6.3M \u2192 194k)  </li> <li>Model achieves strong accuracy while being much smaller  </li> <li>Slight overfitting persists, suggesting potential need for regularization  </li> <li>Architecture demonstrates good balance between performance and simplicity  </li> </ul>"},{"location":"deep_learning/2_Code/#next-steps","title":"Next Steps","text":"<ul> <li>Investigate which layers contribute most to overfitting  </li> <li>Introduce dropout or weight decay to improve generalization  </li> <li>Visualize feature maps to confirm proper layer activations  </li> <li>Prepare for parameter reduction and normalization layers in the next stage  </li> <li>Use this skeleton as a consistent base for all future experiments  </li> </ul>"},{"location":"deep_learning/2_Code/#link-to-the-google-colab","title":"Link to the Google Colab","text":""},{"location":"deep_learning/2_Code/#_1","title":"Model Skeleton","text":""},{"location":"deep_learning/2_Code/#check-your-understanding","title":"Check your understanding","text":"<p>Quiz 2</p> <p>\u2190 Back to Model Skeleton Start with Lighter Model \u2192</p>"},{"location":"deep_learning/2_mcq/","title":"Quiz 2","text":"Submit Quiz"},{"location":"deep_learning/2_mcq/#question-1","title":"Question 1","text":"<p> <p>What is the main goal of Code 2?</p> <ul><li> <p>To finalize the optimizer choice</p></li><li> <p>To establish a consistent base architecture</p></li><li> <p>To perform data augmentation</p></li><li> <p>To add Dropout layers</p></li></ul> </p>"},{"location":"deep_learning/2_mcq/#question-2","title":"Question 2","text":"<p> <p>What is the approximate parameter count in Code 2?</p> <ul><li> <p>1 M</p></li><li> <p>194 k</p></li><li> <p>10 k</p></li><li> <p>6 M</p></li></ul> </p>"},{"location":"deep_learning/2_mcq/#question-3","title":"Question 3","text":"<p> <p>Which statement best describes Code 2\u2019s outcome?</p> <ul><li> <p>Overfitting eliminated</p></li><li> <p>Model fails to converge</p></li><li> <p>Some overfitting remains, but stable accuracy</p></li><li> <p>Training accuracy decreased drastically</p></li></ul> </p>"},{"location":"deep_learning/2_mcq/#question-4","title":"Question 4","text":"<p> <p>Why should the skeleton remain mostly unchanged in later steps?</p> <ul><li> <p>To make model training slower</p></li><li> <p>To ensure comparable results across experiments</p></li><li> <p>To increase randomness</p></li><li> <p>To add more normalization layers</p></li></ul> </p>"},{"location":"deep_learning/2_mcq/#question-5","title":"Question 5","text":"<p> <p>The \u201cskeleton\u201d concept in model design mainly ensures what?</p> <ul><li> <p>Constant input shape</p></li><li> <p>Architectural consistency and comparability</p></li><li> <p>Parameter sharing across epochs</p></li><li> <p>Removal of activations</p></li></ul> </p>"},{"location":"deep_learning/3_Code/","title":"Lighter Model","text":""},{"location":"deep_learning/3_Code/#objectives","title":"Objectives","text":"<ul> <li>Reduce overall model size while maintaining accuracy  </li> <li>Simplify architecture and decrease total parameter count  </li> <li>Evaluate efficiency vs. performance trade-off  </li> <li>Retain training and evaluation pipelines unchanged  </li> </ul>"},{"location":"deep_learning/3_Code/#model-configuration","title":"Model Configuration","text":""},{"location":"deep_learning/3_Code/#adjustments","title":"Adjustments","text":"<ul> <li>Reduced number of convolutional filters per layer  </li> <li>Simplified fully connected layer structure  </li> <li>Preserved model skeleton for consistency  </li> <li>Verified end-to-end functionality post-reduction  </li> </ul>"},{"location":"deep_learning/3_Code/#results","title":"Results","text":"Metric Value Total Parameters 10,700 Best Training Accuracy 99.00% Best Test Accuracy 98.98%"},{"location":"deep_learning/3_Code/#analysis","title":"Analysis","text":""},{"location":"deep_learning/3_Code/#observations","title":"Observations","text":"<ul> <li>Significant parameter reduction (194k \u2192 10.7k) with minimal accuracy loss  </li> <li>Model generalizes well and shows no signs of overfitting  </li> <li>Indicates an efficient baseline suitable for additional improvements  </li> </ul>"},{"location":"deep_learning/3_Code/#next-steps","title":"Next Steps","text":"<ul> <li>Introduce normalization to accelerate convergence  </li> <li>Test model robustness under different batch sizes  </li> <li>Compare performance-per-parameter ratio with previous version  </li> <li>Begin experimenting with advanced regularization in later stages  </li> </ul>"},{"location":"deep_learning/3_Code/#link-to-the-google-colab","title":"Link to the Google Colab","text":""},{"location":"deep_learning/3_Code/#_1","title":"Lighter Model","text":""},{"location":"deep_learning/3_Code/#check-your-understanding","title":"Check your understanding","text":""},{"location":"deep_learning/3_Code/#quiz-3","title":"Quiz 3","text":"<p>\u2190 Back to Model Skeleton Start with Batch Normalization Integration \u2192</p>"},{"location":"deep_learning/3_mcq/","title":"Quiz 3","text":""},{"location":"deep_learning/3_mcq/#question-1","title":"Question 1","text":"<p> <p>What modification is introduced in Code 3?</p> <ul><li> <p>Batch normalization</p></li><li> <p>Dropout</p></li><li> <p>Reduced model capacity</p></li><li> <p>Data augmentation</p></li></ul> </p>"},{"location":"deep_learning/3_mcq/#question-2","title":"Question 2","text":"<p> <p>How does the lighter model affect performance?</p> <ul><li> <p>Significantly reduces accuracy</p></li><li> <p>Improves test accuracy through reduced overfitting</p></li><li> <p>Increases model overfitting</p></li><li> <p>Makes training unstable</p></li></ul> </p>"},{"location":"deep_learning/3_mcq/#question-3","title":"Question 3","text":"<p> <p>Approximately how many parameters remain in Code 3?</p> <ul><li> <p>10.7 k</p></li><li> <p>107 k</p></li><li> <p>1 M</p></li><li> <p>194 k</p></li></ul> </p>"},{"location":"deep_learning/3_mcq/#question-4","title":"Question 4","text":"Error processing MCQ: mapping values are not allowed here   in \"\", line 3, column 53:      ... eason to lighten the model is to:                                          ^<pre>Traceback (most recent call last):\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/mkdocs_mcq/plugin.py\", line 22, in format_mcq\n    config = yaml.safe_load(frontmatter)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/__init__.py\", line 125, in safe_load\n    return load(stream, SafeLoader)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/__init__.py\", line 81, in load\n    return loader.get_single_data()\n           ~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/constructor.py\", line 49, in get_single_data\n    node = self.get_single_node()\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 36, in get_single_node\n    document = self.compose_document()\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 55, in compose_document\n    node = self.compose_node(None, None)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 127, in compose_mapping_node\n    while not self.check_event(MappingEndEvent):\n              ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/parser.py\", line 98, in check_event\n    self.current_event = self.state()\n                         ~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/parser.py\", line 428, in parse_block_mapping_key\n    if self.check_token(KeyToken):\n       ~~~~~~~~~~~~~~~~^^^^^^^^^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/scanner.py\", line 116, in check_token\n    self.fetch_more_tokens()\n    ~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/scanner.py\", line 223, in fetch_more_tokens\n    return self.fetch_value()\n           ~~~~~~~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/scanner.py\", line 577, in fetch_value\n    raise ScannerError(None, None,\n            \"mapping values are not allowed here\",\n            self.get_mark())\nyaml.scanner.ScannerError: mapping values are not allowed here\n  in \"\", line 3, column 53:\n     ... eason to lighten the model is to:\n                                         ^"},{"location":"deep_learning/3_mcq/#question-5","title":"Question 5","text":"Error processing MCQ: mapping values are not allowed here\n  in \"\", line 3, column 48:\n     ...  Code 3, the model is considered:\n                                         ^<pre>Traceback (most recent call last):\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/mkdocs_mcq/plugin.py\", line 22, in format_mcq\n    config = yaml.safe_load(frontmatter)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/__init__.py\", line 125, in safe_load\n    return load(stream, SafeLoader)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/__init__.py\", line 81, in load\n    return loader.get_single_data()\n           ~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/constructor.py\", line 49, in get_single_data\n    node = self.get_single_node()\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 36, in get_single_node\n    document = self.compose_document()\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 55, in compose_document\n    node = self.compose_node(None, None)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 127, in compose_mapping_node\n    while not self.check_event(MappingEndEvent):\n              ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/parser.py\", line 98, in check_event\n    self.current_event = self.state()\n                         ~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/parser.py\", line 428, in parse_block_mapping_key\n    if self.check_token(KeyToken):\n       ~~~~~~~~~~~~~~~~^^^^^^^^^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/scanner.py\", line 116, in check_token\n    self.fetch_more_tokens()\n    ~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/scanner.py\", line 223, in fetch_more_tokens\n    return self.fetch_value()\n           ~~~~~~~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/scanner.py\", line 577, in fetch_value\n    raise ScannerError(None, None,\n            \"mapping values are not allowed here\",\n            self.get_mark())\nyaml.scanner.ScannerError: mapping values are not allowed here\n  in \"\", line 3, column 48:\n     ...  Code 3, the model is considered:\n                                         ^\n\n            \n                Submit Quiz"},{"location":"deep_learning/4_Code/","title":"Batch Normalization Integration","text":""},{"location":"deep_learning/4_Code/#objectives","title":"Objectives","text":"<ul> <li>Introduce Batch Normalization layers for faster and more stable training  </li> <li>Improve gradient flow and enable higher learning rates  </li> <li>Observe effects on accuracy and generalization  </li> </ul>"},{"location":"deep_learning/4_Code/#model-configuration","title":"Model Configuration","text":""},{"location":"deep_learning/4_Code/#adjustments","title":"Adjustments","text":"<ul> <li>Added BatchNorm after convolutional layers  </li> <li>Kept architecture lightweight (\u2248 10.9k parameters)  </li> <li>Maintained same optimizer and data pipeline for fair comparison  </li> </ul>"},{"location":"deep_learning/4_Code/#results","title":"Results","text":"Metric Value Total Parameters 10,900 Best Training Accuracy 99.90% Best Test Accuracy 99.30%"},{"location":"deep_learning/4_Code/#analysis","title":"Analysis","text":""},{"location":"deep_learning/4_Code/#observations","title":"Observations","text":"<ul> <li>Training accuracy improves markedly due to normalization  </li> <li>Minor overfitting appears as model learns faster  </li> <li>Confirms BatchNorm\u2019s effectiveness in smaller networks  </li> </ul>"},{"location":"deep_learning/4_Code/#next-steps","title":"Next Steps","text":"<ul> <li>Apply dropout to mitigate overfitting  </li> <li>Monitor training and validation losses per epoch  </li> <li>Combine BatchNorm with other regularization techniques  </li> <li>Document convergence rate changes for comparison  </li> </ul>"},{"location":"deep_learning/4_Code/#link-to-the-google-colab","title":"Link to the Google Colab","text":""},{"location":"deep_learning/4_Code/#_1","title":"Batch Normalization Integration","text":""},{"location":"deep_learning/4_Code/#check-your-understanding","title":"Check your understanding","text":""},{"location":"deep_learning/4_Code/#quiz-4","title":"Quiz 4","text":"<p>\u2190 Back to Lighter Model Start with BRegularization \u2192</p>"},{"location":"deep_learning/4_mcq/","title":"Quiz 4","text":"Submit Quiz"},{"location":"deep_learning/4_mcq/#question-1","title":"Question 1","text":"<p> <p>Why is Batch Normalization added in Code 4?</p> <ul><li> <p>To randomize inputs</p></li><li> <p>To stabilize learning and normalize activations</p></li><li> <p>To increase dropout rate</p></li><li> <p>To reduce dataset size</p></li></ul> </p>"},{"location":"deep_learning/4_mcq/#question-2","title":"Question 2","text":"<p> <p>What effect does Batch Normalization have on convergence?</p> <ul><li> <p>Slows it down</p></li><li> <p>Speeds it up</p></li><li> <p>Prevents training</p></li><li> <p>Randomizes gradients</p></li></ul> </p>"},{"location":"deep_learning/4_mcq/#question-3","title":"Question 3","text":"<p> <p>After applying Batch Normalization, what new challenge appears?</p> <ul><li> <p>Overfitting</p></li><li> <p>Underfitting</p></li><li> <p>Gradient explosion</p></li><li> <p>Loss plateau</p></li></ul> </p>"},{"location":"deep_learning/4_mcq/#question-4","title":"Question 4","text":"<p> <p>What is the updated parameter count after applying Batch Normalization?</p> <ul><li> <p>6 k</p></li><li> <p>10.9 k</p></li><li> <p>194 k</p></li><li> <p>11.9 k</p></li></ul> </p>"},{"location":"deep_learning/4_mcq/#question-5","title":"Question 5","text":"<p> <p>Batch Normalization is typically inserted at which position in a CNN block?</p> <ul><li> <p>After activation</p></li><li> <p>After each convolutional layer (before activation)</p></li><li> <p>At the output layer only</p></li><li> <p>Between pooling layers</p></li></ul> </p>"},{"location":"deep_learning/5_Code/","title":"Regularization","text":""},{"location":"deep_learning/5_Code/#objectives","title":"Objectives","text":"<ul> <li>Add Dropout to reduce overfitting and enhance generalization  </li> <li>Retain same parameter count to isolate Dropout\u2019s effect  </li> <li>Assess impact on convergence and accuracy  </li> </ul>"},{"location":"deep_learning/5_Code/#model-configuration","title":"Model Configuration","text":""},{"location":"deep_learning/5_Code/#adjustments","title":"Adjustments","text":"<ul> <li>Introduced Dropout layers after convolutional blocks  </li> <li>Maintained total parameters (~ 10.9k)  </li> <li>Trained for 25 epochs to evaluate stability  </li> </ul>"},{"location":"deep_learning/5_Code/#results","title":"Results","text":"Metric Value Total Parameters 10,900 Best Training Accuracy 99.47% Best Test Accuracy 99.30%"},{"location":"deep_learning/5_Code/#analysis","title":"Analysis","text":""},{"location":"deep_learning/5_Code/#observations","title":"Observations","text":"<ul> <li>Regularization works effectively; test accuracy stabilized  </li> <li>Training accuracy slightly reduced \u2014 expected for Dropout usage  </li> <li>Model capacity still limits further gains; architecture refinement needed  </li> </ul>"},{"location":"deep_learning/5_Code/#next-steps","title":"Next Steps","text":"<ul> <li>Replace large kernel with Global Average Pooling to simplify design  </li> <li>Tune dropout rate (0.1 \u2013 0.3) and record accuracy trends  </li> <li>Explore additional regularization such as weight decay  </li> </ul>"},{"location":"deep_learning/5_Code/#link-to-the-google-colab","title":"Link to the Google Colab","text":""},{"location":"deep_learning/5_Code/#_1","title":"Regularization","text":""},{"location":"deep_learning/5_Code/#check-your-understanding","title":"Check your understanding","text":""},{"location":"deep_learning/5_Code/#quiz-5","title":"Quiz 5","text":"<p>\u2190 Back to Batch Normalization Integration Start with Global Average Pooling \u2192</p>"},{"location":"deep_learning/5_mcq/","title":"Quiz 5","text":"Submit Quiz"},{"location":"deep_learning/5_mcq/#question-1","title":"Question 1","text":"<p> <p>What regularization technique is introduced in Code 5?</p> <ul><li> <p>Weight decay</p></li><li> <p>Dropout regularization</p></li><li> <p>Batch normalization</p></li><li> <p>Gradient clipping</p></li></ul> </p>"},{"location":"deep_learning/5_mcq/#question-2","title":"Question 2","text":"<p> <p>What is the primary goal of adding Dropout in Code 5?</p> <ul><li> <p>To improve training speed</p></li><li> <p>To reduce overfitting</p></li><li> <p>To increase model depth</p></li><li> <p>To normalize activations</p></li></ul> </p>"},{"location":"deep_learning/5_mcq/#question-3","title":"Question 3","text":"<p> <p>How does Dropout function during training?</p> <ul><li> <p>It freezes specific neurons permanently</p></li><li> <p>It randomly deactivates neurons during training to prevent co-adaptation</p></li><li> <p>It adds extra convolutional layers</p></li><li> <p>It changes the learning rate dynamically</p></li></ul> </p>"},{"location":"deep_learning/5_mcq/#question-4","title":"Question 4","text":"<p> <p>What is the observed result of adding Dropout?</p> <ul><li> <p>Slightly lower training accuracy but improved test accuracy</p></li><li> <p>Higher overfitting</p></li><li> <p>Model collapse</p></li><li> <p>Unstable training</p></li></ul> </p>"},{"location":"deep_learning/5_mcq/#question-5","title":"Question 5","text":"<p> <p>Why was it difficult to push performance higher at this stage?</p> <ul><li> <p>Model capacity was too large</p></li><li> <p>Model capacity was too small</p></li><li> <p>Dropout rate was too high</p></li><li> <p>The optimizer was not adaptive</p></li></ul> </p>"},{"location":"deep_learning/6_Code/","title":"Global Average Pooling","text":""},{"location":"deep_learning/6_Code/#objectives","title":"Objectives","text":"<ul> <li>Replace large final convolutional kernel with Global Average Pooling (GAP)  </li> <li>Reduce parameter count further while simplifying architecture  </li> <li>Evaluate performance differences between heavy and light endings  </li> </ul>"},{"location":"deep_learning/6_Code/#model-configuration","title":"Model Configuration","text":""},{"location":"deep_learning/6_Code/#adjustments","title":"Adjustments","text":"<ul> <li>Implemented GAP layer before final classifier  </li> <li>Removed dense layer with large kernel size  </li> <li>Achieved significant reduction in total parameters  </li> </ul>"},{"location":"deep_learning/6_Code/#results","title":"Results","text":"Metric Value Total Parameters 6,000 Best Training Accuracy 99.86% Best Test Accuracy 98.13%"},{"location":"deep_learning/6_Code/#analysis","title":"Analysis","text":""},{"location":"deep_learning/6_Code/#observations","title":"Observations","text":"<ul> <li>Accuracy drop expected due to lower capacity  </li> <li>GAP improves architectural efficiency and reduces computation cost  </li> <li>Confirms capacity\u2013performance trade-off relationship  </li> </ul>"},{"location":"deep_learning/6_Code/#next-steps","title":"Next Steps","text":"<ul> <li>Increase network depth slightly to regain lost accuracy  </li> <li>Consider adding a 1\u00d71 convolution before GAP  </li> <li>Begin experimenting with feature-map visualizations for interpretability  </li> </ul>"},{"location":"deep_learning/6_Code/#link-to-the-google-colab","title":"Link to the Google Colab","text":""},{"location":"deep_learning/6_Code/#_1","title":"Global Average Pooling","text":""},{"location":"deep_learning/6_Code/#check-your-understanding","title":"Check your understanding","text":""},{"location":"deep_learning/6_Code/#quiz-6","title":"Quiz 6","text":"<p>\u2190 Back to Regularization Start with Increasing Model Capacity \u2192</p>"},{"location":"deep_learning/6_mcq/","title":"Quiz 6","text":"Submit Quiz"},{"location":"deep_learning/6_mcq/#question-1","title":"Question 1","text":"<p> <p>What major architectural change is introduced in Code 6?</p> <ul><li> <p>Added more dense layers</p></li><li> <p>Replaced the fully connected layer with Global Average Pooling (GAP)</p></li><li> <p>Increased kernel size</p></li><li> <p>Introduced a new activation function</p></li></ul> </p>"},{"location":"deep_learning/6_mcq/#question-2","title":"Question 2","text":"<p> <p>What is the main benefit of using Global Average Pooling (GAP)?</p> <ul><li> <p>Increases the number of parameters</p></li><li> <p>Reduces parameters and overfitting while keeping key spatial information</p></li><li> <p>Adds regularization noise</p></li><li> <p>Improves non-linearity</p></li></ul> </p>"},{"location":"deep_learning/6_mcq/#question-3","title":"Question 3","text":"<p> <p>Why does test accuracy slightly drop after introducing GAP?</p> <ul><li> <p>Overfitting increases</p></li><li> <p>The model\u2019s overall capacity is reduced</p></li><li> <p>Learning rate is too high</p></li><li> <p>Batch size is too small</p></li></ul> </p>"},{"location":"deep_learning/6_mcq/#question-4","title":"Question 4","text":"<p> <p>Approximately how many parameters remain after applying GAP?</p> <ul><li> <p>6 k</p></li><li> <p>10.9 k</p></li><li> <p>11.9 k</p></li><li> <p>194 k</p></li></ul> </p>"},{"location":"deep_learning/6_mcq/#question-5","title":"Question 5","text":"<p> <p>What is a key difference between GAP and a dense layer?</p> <ul><li> <p>GAP averages spatial features and eliminates trainable weights</p></li><li> <p>GAP multiplies all features by a learnable parameter</p></li><li> <p>GAP adds extra normalization</p></li><li> <p>GAP increases spatial resolution</p></li></ul> </p>"},{"location":"deep_learning/7_Code/","title":"Increasing Model Capacity","text":""},{"location":"deep_learning/7_Code/#objectives","title":"Objectives","text":"<ul> <li>Compensate for performance loss after GAP by adding capacity  </li> <li>Add additional layers after GAP to improve representation power  </li> <li>Maintain efficient parameter usage  </li> </ul>"},{"location":"deep_learning/7_Code/#model-configuration","title":"Model Configuration","text":""},{"location":"deep_learning/7_Code/#adjustments","title":"Adjustments","text":"<ul> <li>Introduced new convolutional layer(s) post-GAP  </li> <li>Expanded channel depth in later stages  </li> <li>Retained BatchNorm + Dropout combination  </li> </ul>"},{"location":"deep_learning/7_Code/#results","title":"Results","text":"Metric Value Total Parameters 11,900 Best Training Accuracy 99.33% Best Test Accuracy 99.04%"},{"location":"deep_learning/7_Code/#analysis","title":"Analysis","text":""},{"location":"deep_learning/7_Code/#observations","title":"Observations","text":"<ul> <li>Model shows slight overfitting; Dropout placement may need adjustment  </li> <li>Added layers successfully recover lost accuracy  </li> <li>Indicates end-layer capacity is influential in MNIST-scale tasks  </li> </ul>"},{"location":"deep_learning/7_Code/#next-steps","title":"Next Steps","text":"<ul> <li>Re-evaluate Dropout locations and probabilities  </li> <li>Analyze receptive field coverage (\u2248 5\u00d75 RF patterns visible)  </li> <li>Test adding one more layer after GAP to further enhance capacity  </li> </ul>"},{"location":"deep_learning/7_Code/#link-to-the-google-colab","title":"Link to the Google Colab","text":""},{"location":"deep_learning/7_Code/#_1","title":"Increasing Model Capacity","text":""},{"location":"deep_learning/7_Code/#check-your-understanding","title":"Check your understanding","text":""},{"location":"deep_learning/7_Code/#quiz-7","title":"Quiz 7","text":"<p>\u2190 Back to Global Average Pooling Start with Correcting MaxPooling Location \u2192</p>"},{"location":"deep_learning/7_mcq/","title":"Quiz 7","text":""},{"location":"deep_learning/7_mcq/#question-1","title":"Question 1","text":"<p> <p>Why was the model capacity increased in Code 7?</p> <ul><li> <p>Because Global Average Pooling (GAP) reduced too many parameters</p></li><li> <p>To make the model lighter</p></li><li> <p>To fix an overfitting issue</p></li><li> <p>To reduce training time</p></li></ul> </p>"},{"location":"deep_learning/7_mcq/#question-2","title":"Question 2","text":"<p> <p>What is the new parameter count after increasing capacity?</p> <ul><li> <p>6 k</p></li><li> <p>11.9 k</p></li><li> <p>13.8 k</p></li><li> <p>10.7 k</p></li></ul> </p>"},{"location":"deep_learning/7_mcq/#question-3","title":"Question 3","text":"<p> <p>What was the observed effect of increasing the model\u2019s capacity?</p> <ul><li> <p>Slight return of overfitting but improved accuracy</p></li><li> <p>Decreased training accuracy and underfitting</p></li><li> <p>Training divergence</p></li><li> <p>Dropout became ineffective</p></li></ul> </p>"},{"location":"deep_learning/7_mcq/#question-4","title":"Question 4","text":"<p> <p>What design insight was gained from this experiment?</p> <ul><li> <p>BatchNorm is unnecessary for convergence</p></li><li> <p>End-layer capacity strongly influences model performance</p></li><li> <p>Dropout rate must always be high</p></li><li> <p>More convolutional filters always reduce accuracy</p></li></ul> </p>"},{"location":"deep_learning/7_mcq/#question-5","title":"Question 5","text":"Error processing MCQ: mapping values are not allowed here   in \"\", line 3, column 53:      ... ers after GAP helps the model by:                                          ^<pre>Traceback (most recent call last):\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/mkdocs_mcq/plugin.py\", line 22, in format_mcq\n    config = yaml.safe_load(frontmatter)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/__init__.py\", line 125, in safe_load\n    return load(stream, SafeLoader)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/__init__.py\", line 81, in load\n    return loader.get_single_data()\n           ~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/constructor.py\", line 49, in get_single_data\n    node = self.get_single_node()\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 36, in get_single_node\n    document = self.compose_document()\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 55, in compose_document\n    node = self.compose_node(None, None)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/composer.py\", line 127, in compose_mapping_node\n    while not self.check_event(MappingEndEvent):\n              ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/parser.py\", line 98, in check_event\n    self.current_event = self.state()\n                         ~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/parser.py\", line 428, in parse_block_mapping_key\n    if self.check_token(KeyToken):\n       ~~~~~~~~~~~~~~~~^^^^^^^^^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/scanner.py\", line 116, in check_token\n    self.fetch_more_tokens()\n    ~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/scanner.py\", line 223, in fetch_more_tokens\n    return self.fetch_value()\n           ~~~~~~~~~~~~~~~~^^\n  File \"/home/runner/work/ai4rob_course/ai4rob_course/.pixi/envs/default/lib/python3.14/site-packages/yaml/scanner.py\", line 577, in fetch_value\n    raise ScannerError(None, None,\n            \"mapping values are not allowed here\",\n            self.get_mark())\nyaml.scanner.ScannerError: mapping values are not allowed here\n  in \"\", line 3, column 53:\n     ... ers after GAP helps the model by:\n                                         ^\n\n            \n                Submit Quiz"},{"location":"deep_learning/8_Code/","title":"Correcting MaxPooling Location","text":""},{"location":"deep_learning/8_Code/#objectives","title":"Objectives","text":"<ul> <li>Place MaxPooling at correct receptive field (RF = 5)  </li> <li>Add new layer after GAP to expand capacity  </li> <li>Apply Dropout uniformly across layers  </li> </ul>"},{"location":"deep_learning/8_Code/#model-configuration","title":"Model Configuration","text":""},{"location":"deep_learning/8_Code/#adjustments","title":"Adjustments","text":"<ul> <li>Revised MaxPooling placement based on RF analysis  </li> <li>Ensured consistent Dropout after each convolutional block  </li> <li>Added post-GAP layer for richer features  </li> </ul>"},{"location":"deep_learning/8_Code/#results","title":"Results","text":"Metric Value Total Parameters 13,800 Best Training Accuracy 99.39% Best Test Accuracy 99.41% (9th Epoch)"},{"location":"deep_learning/8_Code/#analysis","title":"Analysis","text":""},{"location":"deep_learning/8_Code/#observations","title":"Observations","text":"<ul> <li>Model generalizes well with balanced train/test performance  </li> <li>Overfitting minimized through uniform regularization  </li> <li>Achieves consistent 99.4 % accuracy range  </li> </ul>"},{"location":"deep_learning/8_Code/#next-steps","title":"Next Steps","text":"<ul> <li>Introduce light image augmentation for further improvement  </li> <li>Test pooling alternatives (average vs max)  </li> <li>Examine learning-curve stability across multiple runs  </li> </ul>"},{"location":"deep_learning/8_Code/#link-to-the-google-colab","title":"Link to the Google Colab","text":""},{"location":"deep_learning/8_Code/#_1","title":"Correcting MaxPooling Location","text":""},{"location":"deep_learning/8_Code/#check-your-understanding","title":"Check your understanding","text":""},{"location":"deep_learning/8_Code/#quiz-8","title":"Quiz 8","text":"<p>\u2190 Back to Increasing Model Capacity Start with Image Augmentation \u2192</p>"},{"location":"deep_learning/8_mcq/","title":"Quiz 8","text":"Submit Quiz"},{"location":"deep_learning/8_mcq/#question-1","title":"Question 1","text":"<p> <p>What correction was made in Code 8?</p> <ul><li> <p>The learning rate scheduler was added</p></li><li> <p>Dropout rate was changed</p></li><li> <p>MaxPooling was moved to the correct position for receptive field alignment</p></li><li> <p>GAP layer was removed</p></li></ul> </p>"},{"location":"deep_learning/8_mcq/#question-2","title":"Question 2","text":"<p> <p>Why does correct MaxPooling placement matter in a CNN?</p> <ul><li> <p>It affects the receptive field coverage and feature hierarchy</p></li><li> <p>It increases the number of filters</p></li><li> <p>It reduces GPU memory usage only</p></li><li> <p>It changes the batch normalization behavior</p></li></ul> </p>"},{"location":"deep_learning/8_mcq/#question-3","title":"Question 3","text":"<p> <p>What is the total parameter count after adjusting MaxPooling placement?</p> <ul><li> <p>11.9 k</p></li><li> <p>13.8 k</p></li><li> <p>6 k</p></li><li> <p>10.7 k</p></li></ul> </p>"},{"location":"deep_learning/8_mcq/#question-4","title":"Question 4","text":"<p> <p>What test accuracy was achieved after correcting MaxPooling placement?</p> <ul><li> <p>99.4 %</p></li><li> <p>97.0 %</p></li><li> <p>98.1 %</p></li><li> <p>99.0 %</p></li></ul> </p>"},{"location":"deep_learning/8_mcq/#question-5","title":"Question 5","text":"<p> <p>What is the key observation from Code 8?</p> <ul><li> <p>Model achieved stable accuracy with no overfitting</p></li><li> <p>Model became underfitted</p></li><li> <p>Dropout caused divergence</p></li><li> <p>Loss continued increasing</p></li></ul> </p>"},{"location":"deep_learning/9_Code/","title":"Image Augmentation","text":""},{"location":"deep_learning/9_Code/#objectives","title":"Objectives","text":"<ul> <li>Enhance model robustness via rotation-based augmentation  </li> <li>Compensate for limited dataset variability  </li> <li>Evaluate influence on training dynamics and test accuracy  </li> </ul>"},{"location":"deep_learning/9_Code/#model-configuration","title":"Model Configuration","text":""},{"location":"deep_learning/9_Code/#adjustments","title":"Adjustments","text":"<ul> <li>Applied rotation between 5\u00b0 \u2013 7\u00b0 on training images  </li> <li>Retained identical architecture (13.8k parameters)  </li> <li>Re-trained model to convergence  </li> </ul>"},{"location":"deep_learning/9_Code/#results","title":"Results","text":"Metric Value Total Parameters 13,800 Best Training Accuracy 99.15% Best Test Accuracy 99.50% (18th Epoch)"},{"location":"deep_learning/9_Code/#analysis","title":"Analysis","text":""},{"location":"deep_learning/9_Code/#observations","title":"Observations","text":"<ul> <li>Training accuracy slightly reduced due to harder data  </li> <li>Test accuracy improved, indicating stronger generalization  </li> <li>Data augmentation effectively reduces overfitting  </li> </ul>"},{"location":"deep_learning/9_Code/#next-steps","title":"Next Steps","text":"<ul> <li>Add additional transformations (shear, scale, brightness)  </li> <li>Compare augmentation-on vs. augmentation-off performance curves  </li> <li>Log augmented samples to validate transform integrity  </li> </ul>"},{"location":"deep_learning/9_Code/#link-to-the-google-colab","title":"Link to the Google Colab","text":""},{"location":"deep_learning/9_Code/#_1","title":"Image Augmentation","text":""},{"location":"deep_learning/9_Code/#check-your-understanding","title":"Check your understanding","text":""},{"location":"deep_learning/9_Code/#quiz-9","title":"Quiz 9","text":"<p>\u2190 Back to Correcting MaxPooling Location Start with Learning Rate Scheduling \u2192</p>"},{"location":"deep_learning/9_mcq/","title":"Quiz 9","text":"Submit Quiz"},{"location":"deep_learning/9_mcq/#question-1","title":"Question 1","text":"<p> <p>What new concept is introduced in Code 9?</p> <ul><li> <p>Weight initialization</p></li><li> <p>Image augmentation through small rotations</p></li><li> <p>Layer normalization</p></li><li> <p>Regularization removal</p></li></ul> </p>"},{"location":"deep_learning/9_mcq/#question-2","title":"Question 2","text":"<p> <p>What was the approximate range of rotation used for augmentation?</p> <ul><li> <p>1\u20133\u00b0</p></li><li> <p>5\u20137\u00b0</p></li><li> <p>10\u201315\u00b0</p></li><li> <p>20\u00b0</p></li></ul> </p>"},{"location":"deep_learning/9_mcq/#question-3","title":"Question 3","text":"<p> <p>What is the main benefit of applying image augmentation?</p> <ul><li> <p>It makes training data more diverse and improves generalization</p></li><li> <p>It reduces training data size</p></li><li> <p>It increases overfitting</p></li><li> <p>It slows down learning intentionally</p></li></ul> </p>"},{"location":"deep_learning/9_mcq/#question-4","title":"Question 4","text":"<p> <p>Why did training accuracy slightly drop after adding augmentation?</p> <ul><li> <p>Because augmented data made the training task slightly harder</p></li><li> <p>Due to a lower learning rate</p></li><li> <p>Because BatchNorm was removed</p></li><li> <p>Because of a smaller model capacity</p></li></ul> </p>"},{"location":"deep_learning/9_mcq/#question-5","title":"Question 5","text":"<p> <p>Why did test accuracy improve after augmentation?</p> <ul><li> <p>The model generalized better to unseen data variations</p></li><li> <p>The model memorized training images more effectively</p></li><li> <p>The learning rate increased automatically</p></li><li> <p>The dropout rate was reduced</p></li></ul> </p>"},{"location":"deep_learning/Backward_Propagation/","title":"Backpropagation \u2013 Part 1","text":""},{"location":"deep_learning/Backward_Propagation/#overview","title":"Overview","text":"<p>This notebook explains the forward and backward propagation process of a simple neural network through both mathematical derivation and spreadsheet implementation. The goal is to compute how the network learns by adjusting weights to minimize the total error.</p> <p>This Markdown file contains: - Complete forward and backward pass equations - Gradient derivations - Visual diagrams - Excel-based loss tracking - Learning rate comparisons  </p>"},{"location":"deep_learning/Backward_Propagation/#1-network-architecture","title":"1. Network Architecture","text":"Layer Neurons Activation Description Input Layer i\u2081, i\u2082 \u2014 Two input features Hidden Layer h\u2081, h\u2082 \u03c3 (sigmoid) Hidden activations Output Layer o\u2081, o\u2082 \u03c3 (sigmoid) Two outputs Loss E\u2081, E\u2082 MSE Mean-Squared Error per output"},{"location":"deep_learning/Backward_Propagation/#diagram","title":"Diagram","text":"<p>The model consists of: - 2 input nodes (i\u2081, i\u2082) - 2 hidden neurons (h\u2081, h\u2082) - 2 output neurons (o\u2081, o\u2082) - 8 weights (w\u2081\u2013w\u2088) connecting all layers.</p>"},{"location":"deep_learning/Backward_Propagation/#2-forward-propagation","title":"2. Forward Propagation","text":""},{"location":"deep_learning/Backward_Propagation/#equations","title":"Equations","text":"<p>```text h1 = w1i1 + w2i2 h2 = w3i1 + w4i2</p> <p>a_h1 = \u03c3(h1) = 1 / (1 + e^(\u2212h1)) a_h2 = \u03c3(h2) = 1 / (1 + e^(\u2212h2))</p> <p>o1 = w5a_h1 + w6a_h2 o2 = w7a_h1 + w8a_h2</p> <p>a_o1 = \u03c3(o1) a_o2 = \u03c3(o2)</p> <p>E1 = \u00bd (t1 \u2212 a_o1)\u00b2 E2 = \u00bd (t2 \u2212 a_o2)\u00b2 E_total = E1 + E2</p>"},{"location":"deep_learning/Backward_Propagation/#3-terminologies","title":"3. Terminologies","text":"<p>This section defines all symbols and notations used throughout the forward and backward propagation derivations.</p> Symbol Definition Layer Notes i\u2081, i\u2082 Input feature values Input Layer Given input data for the network w\u2081\u2013w\u2084 Weights connecting input \u2192 hidden layer Between Input\u2013Hidden Responsible for linear transformation of inputs h\u2081, h\u2082 Weighted sum of inputs (pre-activation) Hidden Layer \\( h\u2081 = w\u2081i\u2081 + w\u2082i\u2082 \\), \\( h\u2082 = w\u2083i\u2081 + w\u2084i\u2082 \\) a\u2095\u2081, a\u2095\u2082 Activated hidden layer outputs Hidden Layer \\( a\u2095 = \u03c3(h) = \\frac{1}{1 + e^{-h}} \\) w\u2085\u2013w\u2088 Weights connecting hidden \u2192 output layer Between Hidden\u2013Output Used to compute output layer weighted sums o\u2081, o\u2082 Weighted sum at output neurons (pre-activation) Output Layer \\( o\u2081 = w\u2085a\u2095\u2081 + w\u2086a\u2095\u2082 \\), \\( o\u2082 = w\u2087a\u2095\u2081 + w\u2088a\u2095\u2082 \\) a\u2092\u2081, a\u2092\u2082 Activated output neuron values Output Layer \\( a\u2092 = \u03c3(o) = \\frac{1}{1 + e^{-o}} \\) t\u2081, t\u2082 Target outputs Output Layer Ground truth labels for comparison E\u2081, E\u2082 Individual output errors Loss Computation \\( E\u2081 = \\frac{1}{2}(t\u2081 - a\u2092\u2081)^2 \\), \\( E\u2082 = \\frac{1}{2}(t\u2082 - a\u2092\u2082)^2 \\) E_total Total error (sum of all output errors) Loss Function \\( E_{total} = E\u2081 + E\u2082 \\) \u03b7 (eta) Learning rate Training Parameter Controls update magnitude for each iteration \u03c3(x) Sigmoid activation function Activation Function \\( \u03c3(x) = \\frac{1}{1 + e^{-x}} \\) \u2202E/\u2202w Partial derivative of loss w.r.t. weight Gradient Used to compute weight updates \u0394w Weight update Optimization Step \\( \u0394w = -\u03b7 \\frac{\u2202E}{\u2202w} \\)"},{"location":"deep_learning/Backward_Propagation/#notes","title":"Notes","text":"<ul> <li>Hidden Layer: transforms inputs into intermediate representations.  </li> <li>Output Layer: produces final predictions through a sigmoid activation.  </li> <li>Loss Function: quantifies the difference between prediction and target.  </li> <li>Gradients: measure how much each weight influences the error.  </li> </ul>"},{"location":"deep_learning/Backward_Propagation/#3-terminologies_1","title":"3. Terminologies","text":"<p>This section defines all symbols used throughout forward and backward propagation.</p> Symbol Meaning i\u2081, i\u2082 Input features h\u2081, h\u2082 Weighted hidden neuron sums a\u2095\u2081, a\u2095\u2082 Activated hidden outputs o\u2081, o\u2082 Weighted output neuron sums a\u2092\u2081, a\u2092\u2082 Activated output values t\u2081, t\u2082 Target (expected) values E\u2081, E\u2082 Individual output errors E<sub>total</sub> Total loss = (E\u2081 + E\u2082)"},{"location":"deep_learning/Backward_Propagation/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>For the total error to be minimized, we must compute partial derivatives of \\(E_{total}\\) with respect to all weights \\(w\u2081\\) \u2013 \\(w\u2088\\):</p> \\[ \\frac{\\partial E_{total}}{\\partial w_i}, \\quad i = 1,2,\\dots,8 \\]"},{"location":"deep_learning/Backward_Propagation/#4-forward-pass-visualization","title":"4. Forward Pass Visualization","text":""},{"location":"deep_learning/Backward_Propagation/#initial-weight-setup-and-flow","title":"Initial Weight Setup and Flow","text":"<ol> <li>Inputs (\\(i\u2081\\), \\(i\u2082\\)) are multiplied by weights \\(w\u2081\\)\u2013\\(w\u2084\\) to produce \\(h\u2081\\), \\(h\u2082\\).  </li> <li>Apply the sigmoid activation function:    $$    a_{h} = \\sigma(h) = \\frac{1}{1 + e^{-h}}    $$</li> <li>Outputs from hidden neurons (\\(a_{h\u2081}\\), \\(a_{h\u2082}\\)) connect to output weights \\(w\u2085\\)\u2013\\(w\u2088\\).  </li> <li>Compute output sums \\(o\u2081\\), \\(o\u2082\\) and apply activation:    $$    a_o = \\sigma(o) = \\frac{1}{1 + e^{-o}}    $$</li> <li>Compute losses for each output neuron using Mean Squared Error:    $$    E_k = \\frac{1}{2}(t_k - a_{ok})^2    $$</li> <li>Total loss:    $$    E_{total} = E_1 + E_2    $$</li> </ol>"},{"location":"deep_learning/Backward_Propagation/#diagram-forward-propagation","title":"Diagram \u2013 Forward Propagation","text":""},{"location":"deep_learning/Backward_Propagation/#summary-of-forward-computation","title":"Summary of Forward Computation","text":"Computation Equation Hidden sum \\(h_j = \\sum_i w_{ij} i_i\\) Hidden activation \\(a_{h_j} = \\sigma(h_j)\\) Output sum \\(o_k = \\sum_j w_{jk} a_{h_j}\\) Output activation \\(a_{o_k} = \\sigma(o_k)\\) Loss \\(E_{total} = \\tfrac{1}{2} \\sum_k (t_k - a_{o_k})^2\\)"},{"location":"deep_learning/Backward_Propagation/#backpropagation-detailed-explanation-part-2","title":"Backpropagation \u2014 Detailed Explanation (Part 2)","text":""},{"location":"deep_learning/Backward_Propagation/#5-backpropagation-derivations","title":"5. Backpropagation ( Derivations )","text":"<p>Backpropagation applies the chain rule of calculus to compute gradients of the total error with respect to each network weight.</p>"},{"location":"deep_learning/Backward_Propagation/#step-1-derivatives-for-output-layer","title":"Step 1 \u2013 Derivatives for Output Layer","text":"\\[ \\begin{aligned} \\frac{\\partial E_{total}}{\\partial w_5} &amp;= (a_{o1} - t_1)\\, a_{o1}(1 - a_{o1})\\, a_{h1} \\\\ \\frac{\\partial E_{total}}{\\partial w_6} &amp;= (a_{o1} - t_1)\\, a_{o1}(1 - a_{o1})\\, a_{h2} \\\\ \\frac{\\partial E_{total}}{\\partial w_7} &amp;= (a_{o2} - t_2)\\, a_{o2}(1 - a_{o2})\\, a_{h1} \\\\ \\frac{\\partial E_{total}}{\\partial w_8} &amp;= (a_{o2} - t_2)\\, a_{o2}(1 - a_{o2})\\, a_{h2} \\end{aligned} \\]"},{"location":"deep_learning/Backward_Propagation/#step-2-hidden-layer-gradients","title":"Step 2 \u2013 Hidden Layer Gradients","text":"\\[ \\begin{aligned} \\frac{\\partial E_{total}}{\\partial a_{h1}} &amp;= (a_{o1} - t_1)\\, a_{o1}(1 - a_{o1})\\, w_5 \\;+\\; (a_{o2} - t_2)\\, a_{o2}(1 - a_{o2})\\, w_7 \\\\ \\frac{\\partial E_{total}}{\\partial a_{h2}} &amp;= (a_{o1} - t_1)\\, a_{o1}(1 - a_{o1})\\, w_6 \\;+\\; (a_{o2} - t_2)\\, a_{o2}(1 - a_{o2})\\, w_8 \\end{aligned} \\]"},{"location":"deep_learning/Backward_Propagation/#step-3-derivatives-with-respect-to-input-weights","title":"Step 3 \u2013 Derivatives with Respect to Input Weights","text":"\\[ \\begin{aligned} \\frac{\\partial E_{total}}{\\partial w_1} &amp;= \\frac{\\partial E_{total}}{\\partial a_{h1}}\\, a_{h1}(1 - a_{h1})\\, i_1 \\\\ \\frac{\\partial E_{total}}{\\partial w_2} &amp;= \\frac{\\partial E_{total}}{\\partial a_{h1}}\\, a_{h1}(1 - a_{h1})\\, i_2 \\\\ \\frac{\\partial E_{total}}{\\partial w_3} &amp;= \\frac{\\partial E_{total}}{\\partial a_{h2}}\\, a_{h2}(1 - a_{h2})\\, i_1 \\\\ \\frac{\\partial E_{total}}{\\partial w_4} &amp;= \\frac{\\partial E_{total}}{\\partial a_{h2}}\\, a_{h2}(1 - a_{h2})\\, i_2 \\end{aligned} \\]"},{"location":"deep_learning/Backward_Propagation/#step-4-consolidated-chain-rule-summary","title":"Step 4 \u2013 Consolidated Chain Rule Summary","text":""},{"location":"deep_learning/Backward_Propagation/#forward-pass","title":"Forward Pass","text":"\\[ \\begin{aligned} h_j &amp;= \\sum_i (w_{ij}\\, i_i) \\\\ a_{h_j} &amp;= \\sigma(h_j) \\\\ o_k &amp;= \\sum_j (w_{jk}\\, a_{h_j}) \\\\ a_{o_k} &amp;= \\sigma(o_k) \\\\ E_{total} &amp;= \\frac{1}{2} \\sum_k (t_k - a_{o_k})^2 \\end{aligned} \\]"},{"location":"deep_learning/Backward_Propagation/#backward-pass","title":"Backward Pass","text":"\\[ \\begin{aligned} \\delta_k &amp;= (a_{o_k} - t_k)\\, a_{o_k}(1 - a_{o_k}) \\\\ \\delta_j &amp;= \\Big(\\sum_k \\delta_k\\, w_{jk}\\Big)\\, a_{h_j}(1 - a_{h_j}) \\\\ \\Delta w_{jk} &amp;= -\\eta\\, \\frac{\\partial E_{total}}{\\partial w_{jk}} = \\eta\\, \\delta_k\\, a_{h_j} \\\\ \\Delta w_{ij} &amp;= -\\eta\\, \\frac{\\partial E_{total}}{\\partial w_{ij}} = \\eta\\, \\delta_j\\, i_i \\end{aligned} \\]"},{"location":"deep_learning/Backward_Propagation/#weight-update-equation","title":"Weight Update Equation","text":"<p>Each weight is updated according to:</p> \\[ w^{new} = w^{old} - \\eta \\, \\frac{\\partial E_{total}}{\\partial w} \\] <p>where \\(\\eta\\) is the learning rate.</p>"},{"location":"deep_learning/Backward_Propagation/#visualization-of-gradient-flow","title":"Visualization of Gradient Flow","text":""},{"location":"deep_learning/Backward_Propagation/#summary-of-steps","title":"Summary of Steps","text":"<ol> <li>Compute forward pass (activations and outputs).  </li> <li>Evaluate total loss \\(E_{total}\\).  </li> <li>Compute gradients for output weights (\\(w\u2085\\)\u2013\\(w\u2088\\)).  </li> <li>Propagate gradients back to hidden weights (\\(w\u2081\\)\u2013\\(w\u2084\\)).  </li> <li>Update all weights using the learning rate \u03b7.  </li> </ol>"},{"location":"deep_learning/Backward_Propagation/#backpropagation-detailed-explanation-part-3","title":"Backpropagation \u2014 Detailed Explanation (Part 3)","text":""},{"location":"deep_learning/Backward_Propagation/#6-weight-update-rule","title":"6. Weight Update Rule","text":"<p>Every iteration of training updates the network\u2019s weights to minimize the total loss.</p>"},{"location":"deep_learning/Backward_Propagation/#general-update-equation","title":"General Update Equation","text":"\\[ w_{new} = w_{old} - \\eta \\, \\frac{\\partial E_{total}}{\\partial w} \\] <p>where:</p> Symbol Meaning \\(w\\) Weight parameter being updated \\(\\eta\\) Learning rate controlling the update step \\(\\frac{\\partial E_{total}}{\\partial w}\\) Gradient of loss w.r.t. the weight"},{"location":"deep_learning/Backward_Propagation/#concept","title":"Concept","text":"<ul> <li>If \\(\\frac{\\partial E_{total}}{\\partial w} &gt; 0\\), decrease the weight.  </li> <li>If \\(\\frac{\\partial E_{total}}{\\partial w} &lt; 0\\), increase the weight.  </li> <li>The magnitude of the update depends on \u03b7 (learning rate).</li> </ul>"},{"location":"deep_learning/Backward_Propagation/#visualization","title":"Visualization","text":""},{"location":"deep_learning/Backward_Propagation/#7-spreadsheet-implementation","title":"7. Spreadsheet Implementation","text":"<p>The accompanying Excel sheet <code>bp.xlsx</code> demonstrates how each gradient and weight update evolves during training.</p>"},{"location":"deep_learning/Backward_Propagation/#excel-contents","title":"Excel Contents","text":"Sheet Component Description Weight columns (\\(w_1\\)\u2013\\(w_8\\)) Iterative weight values across epochs Activation columns (\\(a_{h1}\\), \\(a_{h2}\\), \\(a_{o1}\\), \\(a_{o2}\\)) Hidden &amp; output activations Gradient columns (\\(\\partial E / \\partial w_i\\)) Computed partial derivatives Error metrics (\\(E_1\\), \\(E_2\\), \\(E_{total}\\)) Loss values per iteration Learning Rate (\u03b7) Adjustable parameter in the header Loss Graph Visual convergence of \\(E_{total}\\) over time"},{"location":"deep_learning/Backward_Propagation/#screenshot-example","title":"Screenshot Example","text":""},{"location":"deep_learning/Backward_Propagation/#8-learning-rate-experiments","title":"8. Learning Rate Experiments","text":"<p>Different learning rates (\u03b7) were tested to analyze convergence speed and stability.</p> Learning Rate (\u03b7) Behavior / Observation 0.1 Stable but slow convergence 0.2 Faster convergence, smooth decay 0.5 Optimal speed\u2013stability trade-off 0.8 Slight oscillation, partial overshoot 1.0 Unstable convergence 2.0 Divergent (error increases)"},{"location":"deep_learning/Backward_Propagation/#visual-comparison","title":"Visual Comparison","text":"<p>Each loss curve (from Excel) demonstrates how the choice of \u03b7 affects \\(E_{total}\\):</p> \\[ E_{total}^{(t)} = \\frac{1}{2}\\sum_k \\big(t_k - a_{o_k}^{(t)}\\big)^2 \\] <p>Lower \u03b7 \u2192 slower convergence\u2003\u2003Higher \u03b7 \u2192 instability.</p>"},{"location":"deep_learning/Backward_Propagation/#9-loss-visualization-and-interpretation","title":"9. Loss Visualization and Interpretation","text":"<p>Observation from Excel: - The loss curve gradually decreases across epochs, confirming correct implementation of gradient descent. - Occasional oscillations appear at higher learning rates.</p> <p>Graphical Trend: </p>"},{"location":"deep_learning/Backward_Propagation/#10-key-insights","title":"10. Key Insights","text":"<ol> <li>Sigmoid activation introduces non-linearity but may cause vanishing gradients.  </li> <li>Proper gradient flow is essential for stable weight updates.  </li> <li>Learning rate tuning governs convergence quality.  </li> <li>Backpropagation mathematically ensures each parameter moves toward minimizing \\(E_{total}\\).  </li> <li>Visualization in Excel clarifies the relationship between parameter change and error reduction.</li> </ol>"},{"location":"deep_learning/mcq/","title":"Quiz","text":"Submit Quiz"},{"location":"deep_learning/mcq/#question-1","title":"Question 1","text":"<p> <p>What is the role of a baseline model in a deep learning project?</p> <ul><li> <p>Final optimized model</p></li><li> <p>A reference architecture to compare later improvements</p></li><li> <p>Model used only for validation</p></li><li> <p>Pretrained model on ImageNet</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-2","title":"Question 2","text":"<p> <p>Why do large models with millions of parameters often overfit small datasets?</p> <ul><li> <p>They memorize training data due to excessive capacity</p></li><li> <p>They have too few features</p></li><li> <p>They use small kernels</p></li><li> <p>They skip normalization</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-3","title":"Question 3","text":"<p> <p>What is the purpose of establishing a model \u201cskeleton\u201d before experimentation?</p> <ul><li> <p>To ensure consistent structure for controlled experiments</p></li><li> <p>To maximize training speed</p></li><li> <p>To randomize layer order</p></li><li> <p>To perform data augmentation</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-4","title":"Question 4","text":"<p> <p>Why is reducing model parameters often beneficial?</p> <ul><li> <p>It helps reduce overfitting and improves generalization</p></li><li> <p>It increases computational cost</p></li><li> <p>It always increases accuracy</p></li><li> <p>It improves visualization quality</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-5","title":"Question 5","text":"<p> <p>What is the main purpose of Batch Normalization in neural networks?</p> <ul><li> <p>To stabilize learning by normalizing activations</p></li><li> <p>To increase regularization strength</p></li><li> <p>To freeze gradients</p></li><li> <p>To modify optimizer behavior</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-6","title":"Question 6","text":"<p> <p>Why can Batch Normalization sometimes increase overfitting?</p> <ul><li> <p>It allows faster convergence, causing the model to memorize data more easily</p></li><li> <p>It randomly drops neurons</p></li><li> <p>It reduces model size too much</p></li><li> <p>It introduces noisy gradients</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-7","title":"Question 7","text":"<p> <p>How does Dropout help improve model generalization?</p> <ul><li> <p>By randomly deactivating neurons during training</p></li><li> <p>By increasing learning rate dynamically</p></li><li> <p>By freezing selected layers</p></li><li> <p>By normalizing weights</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-8","title":"Question 8","text":"<p> <p>What is the main effect of Global Average Pooling in a CNN?</p> <ul><li> <p>It replaces dense layers to reduce parameters while preserving spatial information</p></li><li> <p>It adds additional convolutional filters</p></li><li> <p>It increases receptive field size</p></li><li> <p>It removes spatial structure completely</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-9","title":"Question 9","text":"<p> <p>Why is proper MaxPooling placement important in CNN architectures?</p> <ul><li> <p>It ensures correct receptive field coverage and hierarchical feature extraction</p></li><li> <p>It increases GPU memory usage</p></li><li> <p>It improves training speed only</p></li><li> <p>It has no real impact on performance</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-10","title":"Question 10","text":"<p> <p>How does data augmentation improve model performance?</p> <ul><li> <p>It introduces variations that help the model generalize better to unseen data</p></li><li> <p>It increases dataset redundancy</p></li><li> <p>It reduces training speed</p></li><li> <p>It adds noise to labels</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-11","title":"Question 11","text":"<p> <p>Why might increasing model capacity improve accuracy in certain cases?</p> <ul><li> <p>It allows the network to capture more complex representations of data</p></li><li> <p>It prevents overfitting automatically</p></li><li> <p>It reduces the need for regularization</p></li><li> <p>It slows down gradient updates intentionally</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-12","title":"Question 12","text":"<p> <p>Which part of a CNN architecture typically has the greatest influence on classification accuracy?</p> <ul><li> <p>The final fully connected or classification layers</p></li><li> <p>The input normalization layer</p></li><li> <p>The pooling layers</p></li><li> <p>The data loader configuration</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-13","title":"Question 13","text":"<p> <p>What is the main reason to use a Learning Rate Scheduler during training?</p> <ul><li> <p>To reduce the learning rate over time for stable convergence</p></li><li> <p>To randomly change the learning rate each epoch</p></li><li> <p>To speed up training by increasing LR</p></li><li> <p>To disable gradient updates periodically</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-14","title":"Question 14","text":"<p> <p>What problem can occur if the learning rate is too high?</p> <ul><li> <p>The model may diverge or oscillate around the minimum</p></li><li> <p>The model underfits slowly</p></li><li> <p>Training stops immediately</p></li><li> <p>Gradients vanish completely</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-15","title":"Question 15","text":"<p> <p>How does reducing the learning rate later in training improve results?</p> <ul><li> <p>It allows finer weight adjustments for improved generalization</p></li><li> <p>It increases gradient magnitude</p></li><li> <p>It resets optimizer states</p></li><li> <p>It prevents backpropagation entirely</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-16","title":"Question 16","text":"<p> <p>Why does Dropout sometimes reduce training accuracy but improve test accuracy?</p> <ul><li> <p>It prevents overfitting by randomly disabling parts of the network</p></li><li> <p>It increases regularization loss</p></li><li> <p>It slows batch normalization</p></li><li> <p>It introduces bias in weight updates</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-17","title":"Question 17","text":"<p> <p>How does Global Average Pooling differ from Flatten + Dense layers?</p> <ul><li> <p>GAP uses averaging instead of trainable connections, reducing parameters significantly</p></li><li> <p>GAP adds additional bias terms</p></li><li> <p>GAP multiplies feature maps by filters</p></li><li> <p>GAP removes activation functions</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-18","title":"Question 18","text":"<p> <p>What happens if MaxPooling is applied too early in a CNN?</p> <ul><li> <p>Important spatial features may be lost prematurely</p></li><li> <p>It increases computation time</p></li><li> <p>It enhances feature granularity</p></li><li> <p>It doubles the receptive field automatically</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-19","title":"Question 19","text":"<p> <p>Why does data augmentation improve robustness to unseen images?</p> <ul><li> <p>It exposes the model to diverse transformations of the same data</p></li><li> <p>It decreases effective dataset size</p></li><li> <p>It alters the optimizer dynamics</p></li><li> <p>It freezes convolutional kernels</p></li></ul> </p>"},{"location":"deep_learning/mcq/#question-20","title":"Question 20","text":"<p> <p>What overall benefit does combining normalization, dropout, and LR scheduling achieve?</p> <ul><li> <p>It balances convergence speed, stability, and generalization</p></li><li> <p>It reduces model depth</p></li><li> <p>It eliminates the need for backpropagation</p></li><li> <p>It guarantees 100% accuracy</p></li></ul> </p>"},{"location":"foundations/","title":"Foundations Module","text":""},{"location":"foundations/Challenges_in_ai_for_robotics/","title":"Challenges within AI for robotics","text":"<p>If it is so promising and helpful, why is AI not the standard within robotics? If you look at large language model (LLM) like Chatgpt and Copilot they can already do amazing things. So it should be to difficult to map this knowledge towards the robotics world right? That is an understable thought, but there is a big difference between the LLM and the AI that is used within robotics. The availablity of the existing data. Chatgpt could use everything from the internet to learn from.  They scraped the whole internet for learning data. This is similar towards object detection based upon images, there exists already a large and a lot of data labelled data sets. As already suspected from this the data is key in ai. The data type also determines the quality of what is learned, if you for example only learn to catch a red ball and the ball is suddenly blue it is like that the  algorithm does not recognize the ball. </p> <p>The usage of AI within industrial robotics is limited, but it starts to emerge.  Most used AI algorithms are part of the machine learning, meaning it recognizes patterns in data. In order to learn these pattern data is required, but within robotics there is just a limited data available. As not every movement a robot is does is logged and store on location accessible to everyone (in contrary to a lot of things on the internet).  It is currently one of the biggest challenges faced within ai for robotics, how can we learn/train with the minimal amount of data? A solution is to just increase the data on which AI learns. However, this is tedious as it requires to run a lot of tasks on robots.  Therefore, synthetic data is generated. It means that the ai algorithms are trained op data which has been created in simulations. </p> <p>In addition to that,  preferably one algorithm could be used for one robot. This is less feasible, because there is not just one type of robot, a robot dog executes a certain task differently as compared to a humanoid. There is thus also the issue of cross embodiment. The data gathered is typically specifically for just one type of robot.   Another option is just by hand, at the time of writing the company 1x announced a humanoid which can be bought. This humanoid has the potential to do task autonomously. If turns out during the deployment of this humanoid that it can not deploy the task you can schedule a meeting with someone who can teleoperate the robot. In doing so more data is generated by hand. Again the disadvantage of this is that it only creates data for this specific robot.  Sim 2 real</p> <p>Another issue is now that robotics are getting trained with artificial data and in simulations (which typically the case in reinforcement learning). In the case for reinforcement learning it is also not possible/smart decision to first train on a physical system, because in the beginning of training the algorithm is doing random things. If you apply such a policy on a robot this leads to unexpected and sometimes even dangerous situations, because it is not know what this robot is doing. Therefore, the initial phase of such a policy is always trained in simulations. When the results in simulations are satisfactory the algorithm is translated from simulations towards the real robot. This typically lead to a non-working algorithm on the real robot, because in simulation simplification were performed. In addition, the not all dynamics and events are captured in simulation. The AI typically has to do some retaining when it is deployed on the real robot. This issue is also called sim2real gap, the translation from simulations to the real robot. This gap can stem from:</p> <ul> <li> <p>Physics inaccuracies: Simulators often simplify or approximate physical interactions (e.g., friction, contact dynamics), which may not match real-world behavior.</p> </li> <li> <p>Sensor noise and latency: Real sensors introduce noise, delays, and calibration issues that are hard to model accurately in simulation.</p> </li> <li> <p>Actuator differences: Motors and joints in real robots may behave differently due to wear, backlash, or manufacturing variability.</p> </li> <li> <p>Environmental variability: Lighting, textures, and unexpected disturbances in the real world are difficult to replicate in simulation.</p> </li> </ul> <p>One common method to encounter sim2real is to apply domain randomization, meaning you are adding random components in the simulation to make the AI more robust to the uncertainties when deployed on the real robot.  Generalization</p> <p>Lastly, generalization is a key challenge in robotics. As now know machine learning extracts patterns from pattern and uses this pattern to perform action. However, current algorithms are only limited to data represented.  A simple example is that a robot knows how to tie shoelace for brand a, but it does not directly know how to tie shoelaces for brand b. Another example is picking up cubes, if it only knows how to learn and pick up red cubes it does not directly also know how to pickup blue cubes. It is something we as humans can relatively easy do, but these way of thinking/seeing connection is something difficult to convey to robotics systems.  Possible solutions to this is to use already within the data sets randomization and expanding the data set as fast as possible. Another solution to be found is the usage of one shot learning, which lead to the fact that humans can show case the robot what should be done if they do not know. A more new and upcoming method is considered are the foundation models, these are trained with a lot and different data's and can be used as base algorithm if these are implemented only fine tuning would be required for tasks it allows for better understanding.  Hence also the name foundation model, because they are seen as foundation for more specific AI practices.  </p>"},{"location":"foundations/Combine_ai_and_robotics/","title":"How does AI help in robotics?","text":"<p>Now that we understand the general concept of robotics and AI, the question arises: how can they help each other? As mentioned before, traditional robotics has often been limited by its dependence on controlled environments. Robots typically perform well when everything behaves as expected. However, as soon as unexpected changes happen, robots tend to fail. The systems do not know what to do in those situations. This is where AI can help. As mentioned before AI can be used to teach robotics systems without programming, it only requires data. AI therefore extends the operations range of the robots.  </p>"},{"location":"foundations/Combine_ai_and_robotics/#examples","title":"Examples","text":"<p>An example of this is how industrial manipulators are typically controlled. For instance, ABB robots are operated using a teach pendant and programmed with a language called RAPID. To simplify this process, manufacturers have introduced tools like wizard programming, which guide users through tasks. However, even these tools require a certain level of programming knowledge. In an ideal scenario, one could simply demonstrate a task, and the robotic system would learn to perform it. This approach would allow for intuitive training of robots and is related to the concept of one-shot learning, where a system learns from just a single example. </p> <p>Another well-known example of AI-enhanced robotics is Spot, the robot dog developed by Boston Dynamics. Spot was previously controlled using Model Predictive Control (MPC). As the name suggests, MPC uses a model to predict the robot\u2019s future movements and selects the most optimal control input based on those predictions.  However, as soon  Spot was walks on a slippery surface the robot fails. It will falls down in an error stage. This issue has been resolved by extending the model predictive controller with reinforcement learning. </p> <p>The last example is the self-driving car, as mentioned earlier. Without AI, it is extremely difficult to build a reliable autonomous vehicle due to the infinite number of possible scenarios. AI helps by learning to recognize and respond to external factor, such as pedestrians, traffic signs, and other vehicles, without needing explicit programming for every situation. It specifically helps with understanding the surroundings and making real-time decisions.</p>"},{"location":"foundations/Combine_ai_and_robotics/#how-to-deal-with-ai","title":"How to deal with AI","text":"<p>The use of machine learning is helping robotics break through its current limitations, especially when it comes to capturing tasks or understanding systems that are hard to model with traditional methods. As mentioned earlier, machine learning shines at spotting patterns in data that are often too complex or subtle for humans to estimate accurately.    But it\u2019s important to see AI not as a replacement, but as an extension of existing techniques. Training AI models can be time consuming and resource intensive, so combining them with well established methods is often the smartest approach. Plus, sticking with known techniques gives engineers and researchers more control over the system. It\u2019s easier to understand what\u2019s happening and have an idea of what the limitations of the system are. </p>"},{"location":"foundations/Introduction_to_robotics/","title":"Introduction to robotics","text":"<p>Before exploring how artificial intelligence (AI) can enhance robotics, it\u2019s important to understand what robotics actually is. The definition isn\u2019t as straightforward as it seems because robotics is a broad field.</p>"},{"location":"foundations/Introduction_to_robotics/#what-is-robotics","title":"What Is Robotics?","text":"<p>Robotics is about designing, building, and using robots. It combines knowledge from mechanical, electrical and software engineering. It is a multidisciplenary fields, which makes it also difficult to explicitly define what a robot is. There\u2019s no universally agreed definition of a robot. In general, it\u2019s a machine that can do tasks on its own or with just a little help from people. So, something like a drill doesn\u2019t count as a robot, because you still have to use it yourself. It\u2019s just a tool. But a robot vacuum cleaner, like a Roomba, is considered a robot, because it can clean your floor without you doing anything.</p>"},{"location":"foundations/Introduction_to_robotics/#common-types-of-robots","title":"Common Types of Robots","text":"<p>When people hear the word \u201crobot,\u201d they often think of human-like machines or robotic arms, but the world of robotics is much bigger. There are lots of different types of robots, each built for specific jobs, and each with its own set of challenges. Robotics includes many types of robots, each designed for specific tasks and facing unique challenges. </p>"},{"location":"foundations/Introduction_to_robotics/#1-mobile-ground-robots","title":"1. Mobile Ground Robots","text":"<p>These are robots that move around on wheels or legs and use sensors like GPS, cameras, and gyroscopes to figure out where they are and what\u2019s around them. They\u2019re often used for inspections or surveillance. For example, in the CHARISMA project, a four-legged robot was used to detect gas leaks on the street. These robots do well in places that don\u2019t dynamically change much, like homes or warehouses. Once they\u2019ve mapped the area, they can move around pretty easily. But things get tricky when the environment is unpredictable. In a forest or a busy street, the robot might run into unexpected obstacles, moving objects, or changes in lighting. For example , a human suddenly jumping in front of the robot.  These things can confuse the sensors or mess up the robot\u2019s navigation. Most of these robots rely on a map they made earlier, and if something changes, they don\u2019t always know how to react.</p>"},{"location":"foundations/Introduction_to_robotics/#2-aerial-robots-drones","title":"2. Aerial Robots (Drones)","text":"<p>Aerial robots, like drones, face similar problems but with extra layers of difficulty. Flying is harder than driving, and it\u2019s easier to lose control. If a drone\u2019s battery dies, the drone will just fall out of the sky, which can be dangerous. Drones are used for inspections and search-and-rescue missions, but they struggle when something unexpected happens. For example, if the GPS signal drops, the drone might lose track of where it is. Wind, birds, or sudden changes in terrain can also throw it off. And because drones are often far from the operator, fixing problems on the fly isn\u2019t easy.</p>"},{"location":"foundations/Introduction_to_robotics/#3-underwater-robots","title":"3. Underwater Robots","text":"<p>Underwater robots have their own set of challenges. They work below the surface, where everything has to be waterproof and visibility is poor. Cameras don\u2019t work as well underwater, and sensors can get distorted by water pressure or particles. Communication is also harder, radio signals don\u2019t travel well underwater, so robots often have to rely on cables or very limited acoustic signals. All of this makes underwater robotics a tough area to work in. Even simple tasks like identifying objects or navigating through narrow spaces become complicated.</p>"},{"location":"foundations/Introduction_to_robotics/#4-manipulators","title":"4. Manipulators","text":"<p>Manipulators are robots that don\u2019t move around but are built to handle objects. Think of robotic arms in factories. These are pretty advanced and are used a lot in manufacturing, where everything is controlled and predictable. For example, in a car factory, the same parts are put together in the same way over and over again. That\u2019s perfect for a robot. But if you throw in some randomness, like a bin full of mixed-up objects, things get a lot more complicated. The robot has to figure out what the object is, how big it is, what angle it\u2019s sitting at, and how to pick it up without dropping it. These are things humans do without thinking, but for robots, it\u2019s a real challenge.</p>"},{"location":"foundations/Introduction_to_robotics/#5-humanoid-robots","title":"5. Humanoid Robots","text":"<p>Humanoid robots are designed to look and move like people. They\u2019ve been around for a while, but they haven\u2019t really made it into everyday life. Only recently, thanks to better learning techniques and control systems, have they started to show more promise. But they\u2019re still mostly used in research or special projects. They\u2019re expensive, complex, and hard to control. Walking, balancing, and interacting with objects in a human-like way is incredibly difficult. And because they\u2019re meant to operate in human environments, they need to deal with all the unpredictability that comes with it.</p>"},{"location":"foundations/Introduction_to_robotics/#the-big-challenge-in-robotics","title":"The Big Challenge in Robotics","text":"<p>If you look at all these types of robots, you\u2019ll notice they all run into the same kind of problem. They\u2019re great when things are predictable, but they struggle when something unexpected happens. Whether it\u2019s a new obstacle, a change in the environment, or a different kind of task, robots often don\u2019t know what to do. They\u2019re not great at adapting.</p> <p>And that\u2019s the big challenge in robotics right now. How do we build robots that can deal with new, unfamiliar situations without needing someone to reprogram them every time? That leads to something which is simple impossible, because for each task it should be reprogrammed. This is not a feasible solution as there are infinitely many tasks that exists.</p> <p>This is where artificial intelligence might be able to help. But before we get into that, it\u2019s important to understand what AI actually is, and how it connects to things like machine learning, deep learning, and reinforcement learning. These terms get thrown around a lot, and it\u2019s easy to mix them up. So before we talk about how AI could make a difference in robotics, let\u2019s first break down what these ideas really mean.</p>"},{"location":"foundations/introduction_to_ai/","title":"Artificial Intelligence and Its Role in Robotics","text":"<p>Before the emerge of AI, robots had to be programmed line by line. If you wanted to learn the robotic, each part had to be programmed. That works fine for simple and well defined tasks, but it quickly falls apart when things get messy or unpredictable. That\u2019s where AI comes in. Artificial Intelligence, or AI, is the idea that computers can be made intelligent. The core of AI is that machine/algorithms can do things which typically require human intelligence. That includes stuff like:</p> <ul> <li>Making decisions</li> <li>Solving problems</li> <li>Learning from experience</li> <li>Understanding language</li> <li>Recognizing what\u2019s in an image</li> </ul> <p>In its most ambitious form, this is called Artificial General Intelligence (AGI). That\u2019s the kind of AI that could learn and adapt completely on its own, without needing help from humans. It\u2019s like giving a machine a brain that works like ours, which means that it does not explicitly need to be programmed on how the learn things. It can perceive and learn things by itself.</p> <p>But let\u2019s be honest, most of the AI we see today isn\u2019t that advanced. What we actually have is called narrow AI or weak AI. It\u2019s designed to do one specific thing really well, but it doesn\u2019t handle anything outside the task it has been programmed/learned for. </p> <p>For example,  ChatGPT is great at working with text. However, especially in the beginning it could not handle even simpler things like <code>1 + 1</code>, it was easily tricked. The reason is that the AI does not have consciousness, but makes predictions on what is the most likely next word based upon context [1]. It is lacking the logic reasoning that we as humans have.  </p>"},{"location":"foundations/introduction_to_ai/#ai-as-a-field","title":"AI as a Field","text":"<p>Now, AI isn\u2019t just one thing. It\u2019s a whole field made up of different techniques and approaches. One of the most used areas within AI right now is machine learning. In this subfield, algorithms/computers are learned based upon patterns from data instead of being directly programmed. The data from which is learned can come from: - Experiments - Sensors - Simulations - Artificially generated data</p> <p>It is important to keep in mind that also for artificial intelligence the rule of garbage in, garbage out applies. In other words, the algorithm is only as good as the data. If the data does not makes sense, it is nearly impossible to create a properply trained model. </p>"},{"location":"foundations/introduction_to_ai/#types-of-machine-learning","title":"Types of Machine Learning","text":"<p>The field of machine learning can be divided in three smaller groups:</p>"},{"location":"foundations/introduction_to_ai/#1-supervised-learning","title":"1. Supervised Learning","text":"<p>Supervised learning is probably the most straightforward type. You give the computer a bunch of examples where the correct answer is already known. The model tries to learn the relationship between the input and the output. During training, it makes predictions and checks if they\u2019re right. If not, it adjusts itself to do better next time. This kind of learning is great for tasks like classifying images or predicting values. The downside? You need a lot of labeled data, and labeling can be time-consuming and expensive. On top of that, this labelling is still done by humans [2].</p>"},{"location":"foundations/introduction_to_ai/#2-unsupervised-learning","title":"2. Unsupervised Learning","text":"<p>In unsupervised learning the data doesn\u2019t come with labels. The model has to figure things out on its own. It looks for patterns, groups similar items together, or tries to simplify the data by reducing its dimensions. This is useful for things like clustering customers based on behavior or finding hidden structures in data. Since there\u2019s no \u201ccorrect\u201d answer, it\u2019s more about exploration than precision. In the end during training and also testing it is important that the engineer/developer also reflects the perforamcne of the system towards its own expectation, because there is no clear right or wrong. </p>"},{"location":"foundations/introduction_to_ai/#3-reinforcement-learning","title":"3. Reinforcement Learning","text":"<p>Finally, we have reinforcement learning. This one\u2019s a bit different compared to the other two types of learning. In this type t. The model learns by interacting with its environment. It tries things out, gets feedback in the form of rewards or penalties, and uses that to improve its behavior over time. Think of it like training a dog, you give it treats when it does something right. After some point in time, the dog will start to listen as it assumes it will get a treat.  But here\u2019s the catch: if the feedback isn\u2019t clear, the model might learn the wrong thing. That\u2019s why designing the reward system is super important. </p>"},{"location":"foundations/introduction_to_ai/#recap","title":"Recap","text":"<p>Artificial Intelligence has transformed robotics from rigid, line-by-line programming into systems that can learn, adapt, and improve over time. While most AI today is still narrow and task-specific, techniques like supervised, unsupervised, and reinforcement learning allow robots to handle more complex and dynamic environments than ever before. </p> <p>However, AI is not magic\u2014it depends heavily on the quality of data and the design of learning systems. The principle of Garbage In, Garbage Out reminds us that poor input leads to poor results. As research advances, the ultimate goal remains creating robots that can operate reliably in unpredictable situations without constant human intervention.</p> <p>Next, we will explore how these AI techniques are applied in real-world robotics and what challenges still need to be solved.   </p>"},{"location":"foundations/introduction_to_deep_learning/","title":"Introduction to deep learning","text":""},{"location":"foundations/introduction_to_deep_learning/#what-is-a-neural-network","title":"What is a Neural Network?","text":"<p>Neural networks were invented by researchers who aimed to replicate the human brain. In 1943, Warren McCulloch and Walter Pitts published a paper describing a mathematical model of how the brain works. This led to the first notion of an artificial neural network. Their model treated neurons as binary units, meaning they either fire or do not fire. This concept eventually became the foundation for modern (deep) neural networks.</p> <p>In 1953, the perceptron was created, which is considered the first artificial neural network. A visual representation of the perceptron can be seen below:</p> <p></p> <p>Works</p> <p>The perceptron consists of one layer of inputs and one layer of outputs. Let\u2019s go through the diagram step by step:</p> <ol> <li>Start at the input layer.</li> <li>Each input is multiplied by a weight and then summed together.</li> <li>A bias term is added to the summation.</li> </ol> <p>This results in the following equation:</p> \\[ z = w_1x_1 + w_2x_2 + w_3x_3 + w_4b \\] <p>Here:</p> <p>\\( b \\) stands for bias and is always added to the summation. The bias allows the network to shift linearly, independent of the input.</p>"},{"location":"foundations/introduction_to_deep_learning/#why-activation-functions-are-needed","title":"Why Activation Functions Are Needed","text":"<p>To fully propagate through the network, an activation function is required. Without it, multiple layers would just perform weighted summations, and the network would fail to approximate complex functions. Activation functions solve this problem by introducing non-linearity. The activation function determines the output of the perceptron. The difference between a perceptron and a neuron is that:</p> <ul> <li> <p>A perceptron uses only the step function as its activation.</p> </li> <li> <p>Neurons can use different activation functions.</p> </li> </ul> <p>The formula for one neuron or perceptron is:</p> \\[ y = \\mathcal{f}\\Big(b + \\sum_{i=1}^n x_i w_i\\Big) \\] <p>Where:</p> <ul> <li> <p>\\( \\mathcal{f} \\) is the activation function.</p> </li> <li> <p>\\( b \\) is the bias.</p> </li> <li> <p>\\( n \\) is the number of input nodes.</p> </li> <li> <p>\\( x_i \\) is an input feature.</p> </li> <li> <p>\\( w_i \\) is the weight for the corresponding input feature.</p> </li> </ul> <p>Activation functions allow non-linear function approximation. The simplest example is a threshold function: if the weighted sum \\( z \\) is higher than a specific number, the neuron outputs true.</p>"},{"location":"foundations/introduction_to_deep_learning/#common-activation-functions","title":"Common Activation Functions","text":"<p>There are many activation functions. The most common ones are shown below:</p> Name Shape Name Shape Sigmoid ReLU Tanh Step <p>A perceptron is limited to only linear classification. However, it cannot estimate non-linear functions. The reason for this is that it only calculates a linear combination of inputs and applies a single non-linear transformation.  It means that the classification is done based upon a linear line, because the input (defined in equation) is a linear summation. Therefore, only one neuron will never be able to estimate non-linear functions.  This means classification is based on a linear decision boundary because the input equation is a linear summation. Therefore, a single neuron will never be able to estimate non-linear functions. To approximate non-linear functions, multiple layers of perceptrons (neurons) are required. ``</p> <p></p> <p>In this example, there is only one hidden layer, but you can expand the network with as many layers as you like. The hidden layer is simply another set of neurons. After training, these layers can be used for tasks such as classification or regression. Now let\u2019s take a closer look at the diagram. The inputs and their weights are color-coded to make it easier to read. Before we can propagate through this diagram, it is important to understand how to write and refer to the different components. For example, if we want to talk about the weight that connects input neuron 1 to hidden layer neuron 1, we need a consistent notation.</p>"},{"location":"foundations/introduction_to_deep_learning/#notation","title":"Notation","text":"<ul> <li> <p>Inputs are noted as \\( x_j \\), which means input number \\( j \\).   It is important to note that \\( x_0 \\) is clamped at 1 because it represents the bias term.</p> </li> <li> <p>Weights are noted as \\( W_{ij}^{[l]} \\), where:</p> </li> <li>\\( W \\) stands for weight.</li> <li>It connects neuron \\( j \\) in layer \\( l-1 \\) to neuron \\( i \\) in layer \\( l \\).</li> <li> <p>The weight \\( W_{i0}^{[l]} \\) is reserved for the bias in layer \\( l \\).</p> </li> <li> <p>The activation function for neuron \\( j \\) in layer \\( l \\) is noted as \\( a_j^{[l]} \\).</p> </li> <li> <p>The output of neuron \\( j \\) in layer \\( l \\) is noted as \\( z_j^{[l]} \\).   Again, note that \\( z_0 \\) is clamped at value 1 because of the bias.</p> </li> <li> <p>The final output of the network is noted as \\( y_i \\).</p> </li> </ul>"},{"location":"foundations/introduction_to_deep_learning/#forward-propagation","title":"Forward Propagation","text":"<p>Now that the notation is clear, we can calculate the forward propagation through the network.</p> <p>For neuron 1 in the hidden layer:</p> \\[ z_1^{[1]} = a_1^{[1]}(W_{11}^{[1]}x_1 + W_{21}^{[1]}x_2 + W_{31}^{[1]}x_3 + W_{01}^{[1]}x_0) \\] <p>For neuron 2:</p> \\[ z_2^{[1]} = a_2^{[1]}(W_{12}^{[1]}x_1 + W_{22}^{[1]}x_2 + W_{32}^{[1]}x_3 + W_{02}^{[1]}x_0) \\] <p>For neuron 3:</p> \\[ z_3^{[1]} = a_3^{[1]}(W_{13}^{[1]}x_1 + W_{23}^{[1]}x_2 + W_{33}^{[1]}x_3 + W_{03}^{[1]}x_0) \\] <p>You might notice the similarity between these equations. A more efficient way to represent them is by using matrix notation, which allows faster computation and is easier for computers to process.</p>"},{"location":"foundations/introduction_to_deep_learning/#matrix-representation","title":"Matrix Representation","text":"<p>The notation can also be expressed in vector form, which is advantageous because computers can perform matrix operations much faster than iterating through loops.  The calculation for \\(Z^{[l]}\\) can be written as:</p> \\[ Z^{l} = \\begin{bmatrix}  z_1^{[l]} \\\\  z_2^{[l]} \\\\ z_3^{[l]}  \\end{bmatrix} \\] <p>The \\(Z^{[l]}\\) values can be found via:</p> \\[ Z^{[l]} = \\begin{bmatrix} W_{01} &amp; W_{11} &amp; W_{21} &amp; W_{31} \\\\ W_{02} &amp; W_{12} &amp; W_{22} &amp; W_{32} \\\\  W_{03} &amp; W_{13} &amp; W_{23} &amp; W_{33} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\] <p>Each row in the weight matrix corresponds to the set of weights for one neuron. The advantage of writing it this way is that it allows for faster calculations. After this multiplication, the activation function should be applied to each neuron in the hidden layer. This process continues through the output layer.</p> <p>In summary, neural networks are inspired by the human brain and built from simple units called neurons or perceptrons. While a single perceptron can only handle linear classification, stacking multiple layers introduces non-linearity, enabling networks to solve complex problems. Using matrix-based notation and activation functions makes these models computationally efficient and scalable, forming the foundation of modern deep learning.</p>"},{"location":"foundations/why_start_with_deep_learning/","title":"Why start with deep learning?","text":"<p>In robotics, two major types of artificial intelligence are commonly used: deep learning and reinforcement learning. Each has its own strengths and ideal use cases. Reinforcement learning (RL) shines in situations where it's hard to model the environment or define a clear control strategy. Think of it like teaching a robot through trial and error, just like training a dog with treats. The robot learns by interacting with its environment and receiving feedback in the form of rewards or penalties. This makes RL especially useful in complex, dynamic tasks like autonomous navigation or robotic manipulation. On the other hand, deep learning (DL) is a powerhouse for broader applications. It\u2019s like giving the robot a supercharged brain that can recognize patterns, interpret images, understand speech, and even predict outcomes. Deep learning is essentially a method for function approximation, it helps estimate complex relationships between inputs and outputs, which is why it's closely related to regression techniques in statistics. Interestingly, deep learning and reinforcement learning aren\u2019t mutually exclusive. In fact, deep learning often plays a key role within reinforcement learning. For example, deep neural networks can be used to approximate value functions or policies, enabling RL to scale to high-dimensional problem. This combo is known as deep reinforcement learning. In the next section, we\u2019ll dive deeper into how deep learning connects to regression, and why this relationship is so powerful in robotics and beyond.</p> <p>Understanding the basics of deep learning is essential before diving into reinforcement learning. Since deep learning provides the foundation for approximating functions and handling complex data, it sets the stage for more advanced AI techniques used in robotics. In the next section, we\u2019ll explore how deep learning connects to regression, and why this relationship is so crucial for building intelligent robotic systems.</p>"},{"location":"notebooks/deep_learning/Code_10/","title":"Code 10","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n</pre> from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Train Phase transformations\ntrain_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\n\n# Test Phase transformations\ntest_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n</pre> # Train Phase transformations train_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.RandomRotation((-7.0, 7.0), fill=(1,)),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ])  # Test Phase transformations test_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,))                                        ])  In\u00a0[\u00a0]: Copied! <pre>train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n</pre> train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 205331419.96it/s]</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n</pre> <pre>\n</pre> <pre>\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 12614359.45it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 68000190.72it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 15589630.74it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n</pre> SEED = 1  # CUDA? cuda = torch.cuda.is_available() print(\"CUDA Available?\", cuda)  # For reproducibility torch.manual_seed(SEED)  if cuda:     torch.cuda.manual_seed(SEED)  # dataloader arguments - something you'll fetch these from cmdprmt dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)  # train dataloader train_loader = torch.utils.data.DataLoader(train, **dataloader_args)  # test dataloader test_loader = torch.utils.data.DataLoader(test, **dataloader_args) <pre>CUDA Available? True\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</pre> In\u00a0[\u00a0]: Copied! <pre>import torch.nn.functional as F\ndropout_value = 0.1\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Input Block\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 26\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Dropout(dropout_value)\n        ) # output_size = 24\n\n        # TRANSITION BLOCK 1\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n        ) # output_size = 24\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12\n\n        # CONVOLUTION BLOCK 2\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 10\n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 8\n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 6\n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 6\n\n        # OUTPUT BLOCK\n        self.gap = nn.Sequential(\n            nn.AvgPool2d(kernel_size=6)\n        ) # output_size = 1\n\n        self.convblock8 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            # nn.BatchNorm2d(10),\n            # nn.ReLU(),\n            # nn.Dropout(dropout_value)\n        )\n\n\n        self.dropout = nn.Dropout(dropout_value)\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        x = self.pool1(x)\n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        x = self.convblock6(x)\n        x = self.convblock7(x)\n        x = self.gap(x)\n        x = self.convblock8(x)\n\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n</pre> import torch.nn.functional as F dropout_value = 0.1 class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         # Input Block         self.convblock1 = nn.Sequential(             nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 26          # CONVOLUTION BLOCK 1         self.convblock2 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(32),             nn.Dropout(dropout_value)         ) # output_size = 24          # TRANSITION BLOCK 1         self.convblock3 = nn.Sequential(             nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),         ) # output_size = 24         self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12          # CONVOLUTION BLOCK 2         self.convblock4 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 10         self.convblock5 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 8         self.convblock6 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 6         self.convblock7 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 6          # OUTPUT BLOCK         self.gap = nn.Sequential(             nn.AvgPool2d(kernel_size=6)         ) # output_size = 1          self.convblock8 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             # nn.BatchNorm2d(10),             # nn.ReLU(),             # nn.Dropout(dropout_value)         )           self.dropout = nn.Dropout(dropout_value)      def forward(self, x):         x = self.convblock1(x)         x = self.convblock2(x)         x = self.convblock3(x)         x = self.pool1(x)         x = self.convblock4(x)         x = self.convblock5(x)         x = self.convblock6(x)         x = self.convblock7(x)         x = self.gap(x)         x = self.convblock8(x)          x = x.view(-1, 10)         return F.log_softmax(x, dim=-1) In\u00a0[\u00a0]: Copied! <pre>!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))\n</pre> !pip install torchsummary from torchsummary import summary use_cuda = torch.cuda.is_available() device = torch.device(\"cuda\" if use_cuda else \"cpu\") print(device) model = Net().to(device) summary(model, input_size=(1, 28, 28)) <pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 26, 26]             144\n              ReLU-2           [-1, 16, 26, 26]               0\n       BatchNorm2d-3           [-1, 16, 26, 26]              32\n           Dropout-4           [-1, 16, 26, 26]               0\n            Conv2d-5           [-1, 32, 24, 24]           4,608\n              ReLU-6           [-1, 32, 24, 24]               0\n       BatchNorm2d-7           [-1, 32, 24, 24]              64\n           Dropout-8           [-1, 32, 24, 24]               0\n            Conv2d-9           [-1, 10, 24, 24]             320\n        MaxPool2d-10           [-1, 10, 12, 12]               0\n           Conv2d-11           [-1, 16, 10, 10]           1,440\n             ReLU-12           [-1, 16, 10, 10]               0\n      BatchNorm2d-13           [-1, 16, 10, 10]              32\n          Dropout-14           [-1, 16, 10, 10]               0\n           Conv2d-15             [-1, 16, 8, 8]           2,304\n             ReLU-16             [-1, 16, 8, 8]               0\n      BatchNorm2d-17             [-1, 16, 8, 8]              32\n          Dropout-18             [-1, 16, 8, 8]               0\n           Conv2d-19             [-1, 16, 6, 6]           2,304\n             ReLU-20             [-1, 16, 6, 6]               0\n      BatchNorm2d-21             [-1, 16, 6, 6]              32\n          Dropout-22             [-1, 16, 6, 6]               0\n           Conv2d-23             [-1, 16, 6, 6]           2,304\n             ReLU-24             [-1, 16, 6, 6]               0\n      BatchNorm2d-25             [-1, 16, 6, 6]              32\n          Dropout-26             [-1, 16, 6, 6]               0\n        AvgPool2d-27             [-1, 16, 1, 1]               0\n           Conv2d-28             [-1, 10, 1, 1]             160\n================================================================\nTotal params: 13,808\nTrainable params: 13,808\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 1.06\nParams size (MB): 0.05\nEstimated Total Size (MB): 1.12\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))\n</pre> from tqdm import tqdm  train_losses = [] test_losses = [] train_acc = [] test_acc = []  def train(model, device, train_loader, optimizer, epoch):   model.train()   pbar = tqdm(train_loader)   correct = 0   processed = 0   for batch_idx, (data, target) in enumerate(pbar):     # get samples     data, target = data.to(device), target.to(device)      # Init     optimizer.zero_grad()     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.      # Predict     y_pred = model(data)      # Calculate loss     loss = F.nll_loss(y_pred, target)     train_losses.append(loss)      # Backpropagation     loss.backward()     optimizer.step()      # Update pbar-tqdm      pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability     correct += pred.eq(target.view_as(pred)).sum().item()     processed += len(data)      pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')     train_acc.append(100*correct/processed)  def test(model, device, test_loader):     model.eval()     test_loss = 0     correct = 0     with torch.no_grad():         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()      test_loss /= len(test_loader.dataset)     test_losses.append(test_loss)      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(         test_loss, correct, len(test_loader.dataset),         100. * correct / len(test_loader.dataset)))      test_acc.append(100. * correct / len(test_loader.dataset)) In\u00a0[\u00a0]: Copied! <pre>from torch.optim.lr_scheduler import StepLR\n\nmodel =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n\n\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    scheduler.step()\n    test(model, device, test_loader)\n</pre> from torch.optim.lr_scheduler import StepLR  model =  Net().to(device) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler = StepLR(optimizer, step_size=6, gamma=0.1)   EPOCHS = 20 for epoch in range(EPOCHS):     print(\"EPOCH:\", epoch)     train(model, device, train_loader, optimizer, epoch)     scheduler.step()     test(model, device, test_loader) <pre>EPOCH: 0\n</pre> <pre>Loss=0.13546454906463623 Batch_id=468 Accuracy=86.80: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:29&lt;00:00, 15.75it/s]\n</pre> <pre>\nTest set: Average loss: 0.0655, Accuracy: 9813/10000 (98.13%)\n\nEPOCH: 1\n</pre> <pre>Loss=0.08001220971345901 Batch_id=468 Accuracy=97.63: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 20.80it/s]\n</pre> <pre>\nTest set: Average loss: 0.0400, Accuracy: 9878/10000 (98.78%)\n\nEPOCH: 2\n</pre> <pre>Loss=0.0536758191883564 Batch_id=468 Accuracy=98.13: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 20.49it/s]\n</pre> <pre>\nTest set: Average loss: 0.0318, Accuracy: 9900/10000 (99.00%)\n\nEPOCH: 3\n</pre> <pre>Loss=0.10773385316133499 Batch_id=468 Accuracy=98.38: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 20.62it/s]\n</pre> <pre>\nTest set: Average loss: 0.0288, Accuracy: 9909/10000 (99.09%)\n\nEPOCH: 4\n</pre> <pre>Loss=0.049423400312662125 Batch_id=468 Accuracy=98.55: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:26&lt;00:00, 17.84it/s]\n</pre> <pre>\nTest set: Average loss: 0.0320, Accuracy: 9896/10000 (98.96%)\n\nEPOCH: 5\n</pre> <pre>Loss=0.06574732065200806 Batch_id=468 Accuracy=98.65: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 20.40it/s]\n</pre> <pre>\nTest set: Average loss: 0.0256, Accuracy: 9920/10000 (99.20%)\n\nEPOCH: 6\n</pre> <pre>Loss=0.060677338391542435 Batch_id=468 Accuracy=98.67: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 21.08it/s]\n</pre> <pre>\nTest set: Average loss: 0.0232, Accuracy: 9931/10000 (99.31%)\n\nEPOCH: 7\n</pre> <pre>Loss=0.023230433464050293 Batch_id=468 Accuracy=98.70: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:23&lt;00:00, 19.85it/s]\n</pre> <pre>\nTest set: Average loss: 0.0219, Accuracy: 9929/10000 (99.29%)\n\nEPOCH: 8\n</pre> <pre>Loss=0.03589601069688797 Batch_id=468 Accuracy=98.75: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.52it/s]\n</pre> <pre>\nTest set: Average loss: 0.0232, Accuracy: 9918/10000 (99.18%)\n\nEPOCH: 9\n</pre> <pre>Loss=0.08764439821243286 Batch_id=468 Accuracy=98.89: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 21.02it/s]\n</pre> <pre>\nTest set: Average loss: 0.0205, Accuracy: 9934/10000 (99.34%)\n\nEPOCH: 10\n</pre> <pre>Loss=0.040558915585279465 Batch_id=468 Accuracy=98.94: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.39it/s]\n</pre> <pre>\nTest set: Average loss: 0.0216, Accuracy: 9930/10000 (99.30%)\n\nEPOCH: 11\n</pre> <pre>Loss=0.025339946150779724 Batch_id=468 Accuracy=98.92: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.54it/s]\n</pre> <pre>\nTest set: Average loss: 0.0202, Accuracy: 9928/10000 (99.28%)\n\nEPOCH: 12\n</pre> <pre>Loss=0.008392252959311008 Batch_id=468 Accuracy=98.97: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 20.40it/s]\n</pre> <pre>\nTest set: Average loss: 0.0226, Accuracy: 9923/10000 (99.23%)\n\nEPOCH: 13\n</pre> <pre>Loss=0.028040634468197823 Batch_id=468 Accuracy=98.92: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:23&lt;00:00, 20.19it/s]\n</pre> <pre>\nTest set: Average loss: 0.0233, Accuracy: 9925/10000 (99.25%)\n\nEPOCH: 14\n</pre> <pre>Loss=0.03529603034257889 Batch_id=468 Accuracy=99.05: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:25&lt;00:00, 18.54it/s]\n</pre> <pre>\nTest set: Average loss: 0.0212, Accuracy: 9925/10000 (99.25%)\n\nEPOCH: 15\n</pre> <pre>Loss=0.0667828693985939 Batch_id=468 Accuracy=99.03: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:23&lt;00:00, 20.34it/s]\n</pre> <pre>\nTest set: Average loss: 0.0221, Accuracy: 9929/10000 (99.29%)\n\nEPOCH: 16\n</pre> <pre>Loss=0.021697329357266426 Batch_id=468 Accuracy=99.00: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:23&lt;00:00, 20.26it/s]\n</pre> <pre>\nTest set: Average loss: 0.0202, Accuracy: 9941/10000 (99.41%)\n\nEPOCH: 17\n</pre> <pre>Loss=0.006857152562588453 Batch_id=468 Accuracy=99.11: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:23&lt;00:00, 19.75it/s]\n</pre> <pre>\nTest set: Average loss: 0.0197, Accuracy: 9937/10000 (99.37%)\n\nEPOCH: 18\n</pre> <pre>Loss=0.02798357605934143 Batch_id=468 Accuracy=99.13: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:24&lt;00:00, 19.13it/s]\n</pre> <pre>\nTest set: Average loss: 0.0175, Accuracy: 9941/10000 (99.41%)\n\nEPOCH: 19\n</pre> <pre>Loss=0.0014073350466787815 Batch_id=468 Accuracy=99.02: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:23&lt;00:00, 19.99it/s]\n</pre> <pre>\nTest set: Average loss: 0.0179, Accuracy: 9936/10000 (99.36%)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(2,2,figsize=(15,10))\naxs[0, 0].plot(train_losses)\naxs[0, 0].set_title(\"Training Loss\")\naxs[1, 0].plot(train_acc[4000:])\naxs[1, 0].set_title(\"Training Accuracy\")\naxs[0, 1].plot(test_losses)\naxs[0, 1].set_title(\"Test Loss\")\naxs[1, 1].plot(test_acc)\naxs[1, 1].set_title(\"Test Accuracy\")\n</pre> %matplotlib inline import matplotlib.pyplot as plt  fig, axs = plt.subplots(2,2,figsize=(15,10)) axs[0, 0].plot(train_losses) axs[0, 0].set_title(\"Training Loss\") axs[1, 0].plot(train_acc[4000:]) axs[1, 0].set_title(\"Training Accuracy\") axs[0, 1].plot(test_losses) axs[0, 1].set_title(\"Test Loss\") axs[1, 1].plot(test_acc) axs[1, 1].set_title(\"Test Accuracy\") Out[\u00a0]: <pre>Text(0.5, 1.0, 'Test Accuracy')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/deep_learning/Code_10/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"notebooks/deep_learning/Code_10/#data-transformations","title":"Data Transformations\u00b6","text":"<p>We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.</p>"},{"location":"notebooks/deep_learning/Code_10/#dataset-and-creating-traintest-split","title":"Dataset and Creating Train/Test Split\u00b6","text":""},{"location":"notebooks/deep_learning/Code_10/#dataloader-arguments-testtrain-dataloaders","title":"Dataloader Arguments &amp; Test/Train Dataloaders\u00b6","text":""},{"location":"notebooks/deep_learning/Code_10/#the-model","title":"The model\u00b6","text":"<p>Let's start with the model we first saw</p>"},{"location":"notebooks/deep_learning/Code_10/#model-params","title":"Model Params\u00b6","text":"<p>Can't emphasize on how important viewing Model Summary is. Unfortunately, there is no in-built model visualizer, so we have to take external help</p>"},{"location":"notebooks/deep_learning/Code_10/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>Looking at logs can be boring, so we'll introduce tqdm progressbar to get cooler logs.</p> <p>Let's write train and test functions</p>"},{"location":"notebooks/deep_learning/Code_10/#lets-train-and-test-our-model","title":"Let's Train and test our model\u00b6","text":"<p>This time let's add a scheduler for out LR.</p>"},{"location":"notebooks/deep_learning/Code_2/","title":"Code 2","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n</pre> from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Train Phase transformations\ntrain_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\n\n# Test Phase transformations\ntest_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n</pre> # Train Phase transformations train_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ])  # Test Phase transformations test_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,))                                        ])  In\u00a0[\u00a0]: Copied! <pre>train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n</pre> train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 152658811.08it/s]</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n</pre> <pre>\n</pre> <pre>\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 103182022.00it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 38481264.83it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 19823651.16it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n</pre> SEED = 1  # CUDA? cuda = torch.cuda.is_available() print(\"CUDA Available?\", cuda)  # For reproducibility torch.manual_seed(SEED)  if cuda:     torch.cuda.manual_seed(SEED)  # dataloader arguments - something you'll fetch these from cmdprmt dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)  # train dataloader train_loader = torch.utils.data.DataLoader(train, **dataloader_args)  # test dataloader test_loader = torch.utils.data.DataLoader(test, **dataloader_args) <pre>CUDA Available? True\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</pre> In\u00a0[\u00a0]: Copied! <pre># We'd need to convert it into Numpy! Remember above we have converted it into tensors already\ntrain_data = train.train_data\ntrain_data = train.transform(train_data.numpy())\n\nprint('[Train]')\nprint(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\nprint(' - Tensor Shape:', train.train_data.size())\nprint(' - min:', torch.min(train_data))\nprint(' - max:', torch.max(train_data))\nprint(' - mean:', torch.mean(train_data))\nprint(' - std:', torch.std(train_data))\nprint(' - var:', torch.var(train_data))\n\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\nprint(images.shape)\nprint(labels.shape)\n\n# Let's visualize some of the images\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n</pre> # We'd need to convert it into Numpy! Remember above we have converted it into tensors already train_data = train.train_data train_data = train.transform(train_data.numpy())  print('[Train]') print(' - Numpy Shape:', train.train_data.cpu().numpy().shape) print(' - Tensor Shape:', train.train_data.size()) print(' - min:', torch.min(train_data)) print(' - max:', torch.max(train_data)) print(' - mean:', torch.mean(train_data)) print(' - std:', torch.std(train_data)) print(' - var:', torch.var(train_data))  dataiter = iter(train_loader) images, labels = next(dataiter)  print(images.shape) print(labels.shape)  # Let's visualize some of the images %matplotlib inline import matplotlib.pyplot as plt  plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')  <pre>/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n</pre> <pre>[Train]\n - Numpy Shape: (60000, 28, 28)\n - Tensor Shape: torch.Size([60000, 28, 28])\n - min: tensor(-0.4242)\n - max: tensor(2.8215)\n - mean: tensor(-0.0001)\n - std: tensor(1.0000)\n - var: tensor(1.0001)\ntorch.Size([128, 1, 28, 28])\ntorch.Size([128])\n</pre> Out[\u00a0]: <pre>&lt;matplotlib.image.AxesImage at 0x7f00e95735b0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>figure = plt.figure()\nnum_of_images = 60\nfor index in range(1, num_of_images + 1):\n    plt.subplot(6, 10, index)\n    plt.axis('off')\n    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n</pre> figure = plt.figure() num_of_images = 60 for index in range(1, num_of_images + 1):     plt.subplot(6, 10, index)     plt.axis('off')     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r') In\u00a0[\u00a0]: Copied! <pre>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Input Block\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU()\n        ) # output_size = 26\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU()\n        ) # output_size = 24\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU()\n        ) # output_size = 22\n\n        # TRANSITION BLOCK 1\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1), padding=0, bias=False),\n            nn.ReLU()\n        ) # output_size = 11\n\n        # CONVOLUTION BLOCK 2\n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU()\n        ) # output_size = 9\n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU()\n        ) # output_size = 7\n\n        # OUTPUT BLOCK\n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            nn.ReLU()\n        ) # output_size = 7\n        self.convblock8 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(7, 7), padding=0, bias=False),\n            # nn.ReLU() NEVER!\n        ) # output_size = 1 7x7x10 | 7x7x10x10 | 1x1x10\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        x = self.pool1(x)\n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        x = self.convblock6(x)\n        x = self.convblock7(x)\n        x = self.convblock8(x)\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n</pre> class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         # Input Block         self.convblock1 = nn.Sequential(             nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU()         ) # output_size = 26          # CONVOLUTION BLOCK 1         self.convblock2 = nn.Sequential(             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU()         ) # output_size = 24         self.convblock3 = nn.Sequential(             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU()         ) # output_size = 22          # TRANSITION BLOCK 1         self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11         self.convblock4 = nn.Sequential(             nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(1, 1), padding=0, bias=False),             nn.ReLU()         ) # output_size = 11          # CONVOLUTION BLOCK 2         self.convblock5 = nn.Sequential(             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU()         ) # output_size = 9         self.convblock6 = nn.Sequential(             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU()         ) # output_size = 7          # OUTPUT BLOCK         self.convblock7 = nn.Sequential(             nn.Conv2d(in_channels=128, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             nn.ReLU()         ) # output_size = 7         self.convblock8 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(7, 7), padding=0, bias=False),             # nn.ReLU() NEVER!         ) # output_size = 1 7x7x10 | 7x7x10x10 | 1x1x10      def forward(self, x):         x = self.convblock1(x)         x = self.convblock2(x)         x = self.convblock3(x)         x = self.pool1(x)         x = self.convblock4(x)         x = self.convblock5(x)         x = self.convblock6(x)         x = self.convblock7(x)         x = self.convblock8(x)         x = x.view(-1, 10)         return F.log_softmax(x, dim=-1) In\u00a0[\u00a0]: Copied! <pre>!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))\n</pre> !pip install torchsummary from torchsummary import summary use_cuda = torch.cuda.is_available() device = torch.device(\"cuda\" if use_cuda else \"cpu\") print(device) model = Net().to(device) summary(model, input_size=(1, 28, 28)) <pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 26, 26]             288\n              ReLU-2           [-1, 32, 26, 26]               0\n            Conv2d-3           [-1, 64, 24, 24]          18,432\n              ReLU-4           [-1, 64, 24, 24]               0\n            Conv2d-5          [-1, 128, 22, 22]          73,728\n              ReLU-6          [-1, 128, 22, 22]               0\n         MaxPool2d-7          [-1, 128, 11, 11]               0\n            Conv2d-8           [-1, 32, 11, 11]           4,096\n              ReLU-9           [-1, 32, 11, 11]               0\n           Conv2d-10             [-1, 64, 9, 9]          18,432\n             ReLU-11             [-1, 64, 9, 9]               0\n           Conv2d-12            [-1, 128, 7, 7]          73,728\n             ReLU-13            [-1, 128, 7, 7]               0\n           Conv2d-14             [-1, 10, 7, 7]           1,280\n             ReLU-15             [-1, 10, 7, 7]               0\n           Conv2d-16             [-1, 10, 1, 1]           4,900\n================================================================\nTotal params: 194,884\nTrainable params: 194,884\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 2.20\nParams size (MB): 0.74\nEstimated Total Size (MB): 2.94\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))\n</pre> from tqdm import tqdm  train_losses = [] test_losses = [] train_acc = [] test_acc = []  def train(model, device, train_loader, optimizer, epoch):   model.train()   pbar = tqdm(train_loader)   correct = 0   processed = 0   for batch_idx, (data, target) in enumerate(pbar):     # get samples     data, target = data.to(device), target.to(device)      # Init     optimizer.zero_grad()     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.      # Predict     y_pred = model(data)      # Calculate loss     loss = F.nll_loss(y_pred, target)     train_losses.append(loss)      # Backpropagation     loss.backward()     optimizer.step()      # Update pbar-tqdm      pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability     correct += pred.eq(target.view_as(pred)).sum().item()     processed += len(data)      pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')     train_acc.append(100*correct/processed)  def test(model, device, test_loader):     model.eval()     test_loss = 0     correct = 0     with torch.no_grad():         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()      test_loss /= len(test_loader.dataset)     test_losses.append(test_loss)      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(         test_loss, correct, len(test_loader.dataset),         100. * correct / len(test_loader.dataset)))      test_acc.append(100. * correct / len(test_loader.dataset)) In\u00a0[\u00a0]: Copied! <pre>model =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)\n</pre> model =  Net().to(device) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) EPOCHS = 20 for epoch in range(EPOCHS):     print(\"EPOCH:\", epoch)     train(model, device, train_loader, optimizer, epoch)     test(model, device, test_loader) <pre>EPOCH: 0\n</pre> <pre>Loss=0.27435874938964844 Batch_id=468 Accuracy=62.29: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.90it/s]\n</pre> <pre>\nTest set: Average loss: 0.1418, Accuracy: 9592/10000 (95.92%)\n\nEPOCH: 1\n</pre> <pre>Loss=0.08054310083389282 Batch_id=468 Accuracy=96.31: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.44it/s]\n</pre> <pre>\nTest set: Average loss: 0.0820, Accuracy: 9737/10000 (97.37%)\n\nEPOCH: 2\n</pre> <pre>Loss=0.07694163918495178 Batch_id=468 Accuracy=97.49: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 20.83it/s]\n</pre> <pre>\nTest set: Average loss: 0.0643, Accuracy: 9799/10000 (97.99%)\n\nEPOCH: 3\n</pre> <pre>Loss=0.06593039631843567 Batch_id=468 Accuracy=98.06: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:23&lt;00:00, 19.89it/s]\n</pre> <pre>\nTest set: Average loss: 0.0550, Accuracy: 9829/10000 (98.29%)\n\nEPOCH: 4\n</pre> <pre>Loss=0.04631030559539795 Batch_id=468 Accuracy=98.29: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.14it/s]\n</pre> <pre>\nTest set: Average loss: 0.0494, Accuracy: 9835/10000 (98.35%)\n\nEPOCH: 5\n</pre> <pre>Loss=0.07957585901021957 Batch_id=468 Accuracy=98.53: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.94it/s]\n</pre> <pre>\nTest set: Average loss: 0.0503, Accuracy: 9828/10000 (98.28%)\n\nEPOCH: 6\n</pre> <pre>Loss=0.01763545162975788 Batch_id=468 Accuracy=98.65: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.75it/s]\n</pre> <pre>\nTest set: Average loss: 0.0407, Accuracy: 9870/10000 (98.70%)\n\nEPOCH: 7\n</pre> <pre>Loss=0.00864641834050417 Batch_id=468 Accuracy=98.78: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.75it/s]\n</pre> <pre>\nTest set: Average loss: 0.0360, Accuracy: 9888/10000 (98.88%)\n\nEPOCH: 8\n</pre> <pre>Loss=0.029195209965109825 Batch_id=468 Accuracy=98.91: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.26it/s]\n</pre> <pre>\nTest set: Average loss: 0.0416, Accuracy: 9873/10000 (98.73%)\n\nEPOCH: 9\n</pre> <pre>Loss=0.018922191113233566 Batch_id=468 Accuracy=98.97: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.57it/s]\n</pre> <pre>\nTest set: Average loss: 0.0392, Accuracy: 9860/10000 (98.60%)\n\nEPOCH: 10\n</pre> <pre>Loss=0.0038033053278923035 Batch_id=468 Accuracy=99.05: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.01it/s]\n</pre> <pre>\nTest set: Average loss: 0.0374, Accuracy: 9870/10000 (98.70%)\n\nEPOCH: 11\n</pre> <pre>Loss=0.016801899299025536 Batch_id=468 Accuracy=99.14: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.04it/s]\n</pre> <pre>\nTest set: Average loss: 0.0373, Accuracy: 9868/10000 (98.68%)\n\nEPOCH: 12\n</pre> <pre>Loss=0.003156515071168542 Batch_id=468 Accuracy=99.15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.09it/s]\n</pre> <pre>\nTest set: Average loss: 0.0332, Accuracy: 9891/10000 (98.91%)\n\nEPOCH: 13\n</pre> <pre>Loss=0.004151341039687395 Batch_id=468 Accuracy=99.23: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.43it/s]\n</pre> <pre>\nTest set: Average loss: 0.0323, Accuracy: 9900/10000 (99.00%)\n\nEPOCH: 14\n</pre> <pre>Loss=0.0066026379354298115 Batch_id=468 Accuracy=99.26: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.16it/s]\n</pre> <pre>\nTest set: Average loss: 0.0387, Accuracy: 9880/10000 (98.80%)\n\nEPOCH: 15\n</pre> <pre>Loss=0.0038049109280109406 Batch_id=468 Accuracy=99.33: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.04it/s]\n</pre> <pre>\nTest set: Average loss: 0.0381, Accuracy: 9895/10000 (98.95%)\n\nEPOCH: 16\n</pre> <pre>Loss=0.0218046847730875 Batch_id=468 Accuracy=99.38: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.09it/s]\n</pre> <pre>\nTest set: Average loss: 0.0314, Accuracy: 9897/10000 (98.97%)\n\nEPOCH: 17\n</pre> <pre>Loss=0.00030396366491913795 Batch_id=468 Accuracy=99.43: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.64it/s]\n</pre> <pre>\nTest set: Average loss: 0.0350, Accuracy: 9879/10000 (98.79%)\n\nEPOCH: 18\n</pre> <pre>Loss=0.00502537889406085 Batch_id=468 Accuracy=99.44: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.66it/s]\n</pre> <pre>\nTest set: Average loss: 0.0409, Accuracy: 9869/10000 (98.69%)\n\nEPOCH: 19\n</pre> <pre>Loss=0.01610145904123783 Batch_id=468 Accuracy=99.52: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.14it/s]\n</pre> <pre>\nTest set: Average loss: 0.0326, Accuracy: 9899/10000 (98.99%)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>t = [t_items.item() for t_items in train_losses]\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(2,2,figsize=(15,10))\naxs[0, 0].plot(t)\naxs[0, 0].set_title(\"Training Loss\")\naxs[1, 0].plot(train_acc)\naxs[1, 0].set_title(\"Training Accuracy\")\naxs[0, 1].plot(test_losses)\naxs[0, 1].set_title(\"Test Loss\")\naxs[1, 1].plot(test_acc)\naxs[1, 1].set_title(\"Test Accuracy\")\n</pre> t = [t_items.item() for t_items in train_losses] %matplotlib inline import matplotlib.pyplot as plt fig, axs = plt.subplots(2,2,figsize=(15,10)) axs[0, 0].plot(t) axs[0, 0].set_title(\"Training Loss\") axs[1, 0].plot(train_acc) axs[1, 0].set_title(\"Training Accuracy\") axs[0, 1].plot(test_losses) axs[0, 1].set_title(\"Test Loss\") axs[1, 1].plot(test_acc) axs[1, 1].set_title(\"Test Accuracy\") Out[\u00a0]: <pre>Text(0.5, 1.0, 'Test Accuracy')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/deep_learning/Code_2/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"notebooks/deep_learning/Code_2/#data-transformations","title":"Data Transformations\u00b6","text":"<p>We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.</p>"},{"location":"notebooks/deep_learning/Code_2/#dataset-and-creating-traintest-split","title":"Dataset and Creating Train/Test Split\u00b6","text":""},{"location":"notebooks/deep_learning/Code_2/#dataloader-arguments-testtrain-dataloaders","title":"Dataloader Arguments &amp; Test/Train Dataloaders\u00b6","text":""},{"location":"notebooks/deep_learning/Code_2/#data-statistics","title":"Data Statistics\u00b6","text":"<p>It is important to know your data very well. Let's check some of the statistics around our data and how it actually looks like</p>"},{"location":"notebooks/deep_learning/Code_2/#more","title":"MORE\u00b6","text":"<p>It is important that we view as many images as possible. This is required to get some idea on image augmentation later on</p>"},{"location":"notebooks/deep_learning/Code_2/#the-model","title":"The model\u00b6","text":"<p>Let's start with the model we first saw</p>"},{"location":"notebooks/deep_learning/Code_2/#model-params","title":"Model Params\u00b6","text":"<p>Can't emphasize on how important viewing Model Summary is. Unfortunately, there is no in-built model visualizer, so we have to take external help</p>"},{"location":"notebooks/deep_learning/Code_2/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>Looking at logs can be boring, so we'll introduce tqdm progressbar to get cooler logs.</p> <p>Let's write train and test functions</p>"},{"location":"notebooks/deep_learning/Code_2/#lets-train-and-test-our-model","title":"Let's Train and test our model\u00b6","text":""},{"location":"notebooks/deep_learning/Code_3/","title":"Code 3","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n</pre> from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Train Phase transformations\ntrain_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\n\n# Test Phase transformations\ntest_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n</pre> # Train Phase transformations train_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ])  # Test Phase transformations test_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,))                                        ])  In\u00a0[\u00a0]: Copied! <pre>train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n</pre> train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 90250529.11it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 100444190.57it/s]</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 29173836.77it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 21333178.91it/s]</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n</pre> SEED = 1  # CUDA? cuda = torch.cuda.is_available() print(\"CUDA Available?\", cuda)  # For reproducibility torch.manual_seed(SEED)  if cuda:     torch.cuda.manual_seed(SEED)  # dataloader arguments - something you'll fetch these from cmdprmt dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)  # train dataloader train_loader = torch.utils.data.DataLoader(train, **dataloader_args)  # test dataloader test_loader = torch.utils.data.DataLoader(test, **dataloader_args) <pre>CUDA Available? True\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</pre> In\u00a0[\u00a0]: Copied! <pre># We'd need to convert it into Numpy! Remember above we have converted it into tensors already\ntrain_data = train.train_data\ntrain_data = train.transform(train_data.numpy())\n\nprint('[Train]')\nprint(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\nprint(' - Tensor Shape:', train.train_data.size())\nprint(' - min:', torch.min(train_data))\nprint(' - max:', torch.max(train_data))\nprint(' - mean:', torch.mean(train_data))\nprint(' - std:', torch.std(train_data))\nprint(' - var:', torch.var(train_data))\n\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(labels.shape)\n\n# Let's visualize some of the images\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n</pre> # We'd need to convert it into Numpy! Remember above we have converted it into tensors already train_data = train.train_data train_data = train.transform(train_data.numpy())  print('[Train]') print(' - Numpy Shape:', train.train_data.cpu().numpy().shape) print(' - Tensor Shape:', train.train_data.size()) print(' - min:', torch.min(train_data)) print(' - max:', torch.max(train_data)) print(' - mean:', torch.mean(train_data)) print(' - std:', torch.std(train_data)) print(' - var:', torch.var(train_data))  dataiter = iter(train_loader) images, labels = dataiter.next()  print(images.shape) print(labels.shape)  # Let's visualize some of the images %matplotlib inline import matplotlib.pyplot as plt  plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')  <pre>/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n</pre> <pre>[Train]\n - Numpy Shape: (60000, 28, 28)\n - Tensor Shape: torch.Size([60000, 28, 28])\n - min: tensor(-0.4242)\n - max: tensor(2.8215)\n - mean: tensor(-0.0001)\n - std: tensor(1.0000)\n - var: tensor(1.0001)\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-5-f75c6547409a&gt; in &lt;cell line: 15&gt;()\n     13 \n     14 dataiter = iter(train_loader)\n---&gt; 15 images, labels = dataiter.next()\n     16 \n     17 print(images.shape)\n\nAttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'</pre> In\u00a0[\u00a0]: Copied! <pre>figure = plt.figure()\nnum_of_images = 60\nfor index in range(1, num_of_images + 1):\n    plt.subplot(6, 10, index)\n    plt.axis('off')\n    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n</pre> figure = plt.figure() num_of_images = 60 for index in range(1, num_of_images + 1):     plt.subplot(6, 10, index)     plt.axis('off')     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r') In\u00a0[\u00a0]: Copied! <pre>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Input Block\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n\n            nn.ReLU()\n        ) # output_size = 26\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n\n            nn.ReLU()\n        ) # output_size = 24\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n\n\n            nn.ReLU()\n        ) # output_size = 22\n\n        # TRANSITION BLOCK 1\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            nn.ReLU()\n        ) # output_size = 11\n\n        # CONVOLUTION BLOCK 2\n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n\n\n            nn.ReLU()\n        ) # output_size = 9\n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU()\n        ) # output_size = 7\n\n        # OUTPUT BLOCK\n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            nn.ReLU()\n        ) # output_size = 7\n        self.convblock8 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(7, 7), padding=0, bias=False),\n            # nn.BatchNorm2d(10), NEVER\n            # nn.ReLU() NEVER!\n        ) # output_size = 1\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        x = self.pool1(x)\n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        x = self.convblock6(x)\n        x = self.convblock7(x)\n        x = self.convblock8(x)\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n</pre> class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         # Input Block         self.convblock1 = nn.Sequential(             nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),              nn.ReLU()         ) # output_size = 26          # CONVOLUTION BLOCK 1         self.convblock2 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),              nn.ReLU()         ) # output_size = 24         self.convblock3 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),               nn.ReLU()         ) # output_size = 22          # TRANSITION BLOCK 1         self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11         self.convblock4 = nn.Sequential(             nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             nn.ReLU()         ) # output_size = 11          # CONVOLUTION BLOCK 2         self.convblock5 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),               nn.ReLU()         ) # output_size = 9         self.convblock6 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU()         ) # output_size = 7          # OUTPUT BLOCK         self.convblock7 = nn.Sequential(             nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             nn.ReLU()         ) # output_size = 7         self.convblock8 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(7, 7), padding=0, bias=False),             # nn.BatchNorm2d(10), NEVER             # nn.ReLU() NEVER!         ) # output_size = 1      def forward(self, x):         x = self.convblock1(x)         x = self.convblock2(x)         x = self.convblock3(x)         x = self.pool1(x)         x = self.convblock4(x)         x = self.convblock5(x)         x = self.convblock6(x)         x = self.convblock7(x)         x = self.convblock8(x)         x = x.view(-1, 10)         return F.log_softmax(x, dim=-1) In\u00a0[\u00a0]: Copied! <pre>!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))\n</pre> !pip install torchsummary from torchsummary import summary use_cuda = torch.cuda.is_available() device = torch.device(\"cuda\" if use_cuda else \"cpu\") print(device) model = Net().to(device) summary(model, input_size=(1, 28, 28)) <pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 10, 26, 26]              90\n              ReLU-2           [-1, 10, 26, 26]               0\n            Conv2d-3           [-1, 10, 24, 24]             900\n              ReLU-4           [-1, 10, 24, 24]               0\n            Conv2d-5           [-1, 20, 22, 22]           1,800\n              ReLU-6           [-1, 20, 22, 22]               0\n         MaxPool2d-7           [-1, 20, 11, 11]               0\n            Conv2d-8           [-1, 10, 11, 11]             200\n              ReLU-9           [-1, 10, 11, 11]               0\n           Conv2d-10             [-1, 10, 9, 9]             900\n             ReLU-11             [-1, 10, 9, 9]               0\n           Conv2d-12             [-1, 20, 7, 7]           1,800\n             ReLU-13             [-1, 20, 7, 7]               0\n           Conv2d-14             [-1, 10, 7, 7]             200\n             ReLU-15             [-1, 10, 7, 7]               0\n           Conv2d-16             [-1, 10, 1, 1]           4,900\n================================================================\nTotal params: 10,790\nTrainable params: 10,790\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.41\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.45\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))\n</pre> from tqdm import tqdm  train_losses = [] test_losses = [] train_acc = [] test_acc = []  def train(model, device, train_loader, optimizer, epoch):   model.train()   pbar = tqdm(train_loader)   correct = 0   processed = 0   for batch_idx, (data, target) in enumerate(pbar):     # get samples     data, target = data.to(device), target.to(device)      # Init     optimizer.zero_grad()     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.      # Predict     y_pred = model(data)      # Calculate loss     loss = F.nll_loss(y_pred, target)     train_losses.append(loss)      # Backpropagation     loss.backward()     optimizer.step()      # Update pbar-tqdm      pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability     correct += pred.eq(target.view_as(pred)).sum().item()     processed += len(data)      pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')     train_acc.append(100*correct/processed)  def test(model, device, test_loader):     model.eval()     test_loss = 0     correct = 0     with torch.no_grad():         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()      test_loss /= len(test_loader.dataset)     test_losses.append(test_loss)      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(         test_loss, correct, len(test_loader.dataset),         100. * correct / len(test_loader.dataset)))      test_acc.append(100. * correct / len(test_loader.dataset)) In\u00a0[\u00a0]: Copied! <pre>model =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)\n</pre> model =  Net().to(device) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) EPOCHS = 20 for epoch in range(EPOCHS):     print(\"EPOCH:\", epoch)     train(model, device, train_loader, optimizer, epoch)     test(model, device, test_loader) <pre>EPOCH: 0\n</pre> <pre>Loss=0.26803091168403625 Batch_id=468 Accuracy=57.91: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.47it/s]\n</pre> <pre>\nTest set: Average loss: 0.2121, Accuracy: 9339/10000 (93.39%)\n\nEPOCH: 1\n</pre> <pre>Loss=0.21970321238040924 Batch_id=468 Accuracy=95.05: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.81it/s]\n</pre> <pre>\nTest set: Average loss: 0.1155, Accuracy: 9643/10000 (96.43%)\n\nEPOCH: 2\n</pre> <pre>Loss=0.09726456552743912 Batch_id=468 Accuracy=96.92: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.74it/s]\n</pre> <pre>\nTest set: Average loss: 0.0686, Accuracy: 9771/10000 (97.71%)\n\nEPOCH: 3\n</pre> <pre>Loss=0.044223129749298096 Batch_id=468 Accuracy=97.45: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:16&lt;00:00, 27.82it/s]\n</pre> <pre>\nTest set: Average loss: 0.0661, Accuracy: 9801/10000 (98.01%)\n\nEPOCH: 4\n</pre> <pre>Loss=0.016858240589499474 Batch_id=468 Accuracy=97.81: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.34it/s]\n</pre> <pre>\nTest set: Average loss: 0.0553, Accuracy: 9818/10000 (98.18%)\n\nEPOCH: 5\n</pre> <pre>Loss=0.03236464783549309 Batch_id=468 Accuracy=98.04: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:16&lt;00:00, 27.89it/s]\n</pre> <pre>\nTest set: Average loss: 0.0520, Accuracy: 9833/10000 (98.33%)\n\nEPOCH: 6\n</pre> <pre>Loss=0.027982281520962715 Batch_id=468 Accuracy=98.22: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:16&lt;00:00, 27.59it/s]\n</pre> <pre>\nTest set: Average loss: 0.0476, Accuracy: 9844/10000 (98.44%)\n\nEPOCH: 7\n</pre> <pre>Loss=0.021029813215136528 Batch_id=468 Accuracy=98.46: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:16&lt;00:00, 27.87it/s]\n</pre> <pre>\nTest set: Average loss: 0.0503, Accuracy: 9840/10000 (98.40%)\n\nEPOCH: 8\n</pre> <pre>Loss=0.04216528311371803 Batch_id=468 Accuracy=98.45: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:16&lt;00:00, 27.85it/s]\n</pre> <pre>\nTest set: Average loss: 0.0495, Accuracy: 9843/10000 (98.43%)\n\nEPOCH: 9\n</pre> <pre>Loss=0.02685135416686535 Batch_id=468 Accuracy=98.64: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.95it/s]\n</pre> <pre>\nTest set: Average loss: 0.0436, Accuracy: 9847/10000 (98.47%)\n\nEPOCH: 10\n</pre> <pre>Loss=0.03768262639641762 Batch_id=468 Accuracy=98.72: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.59it/s]\n</pre> <pre>\nTest set: Average loss: 0.0398, Accuracy: 9867/10000 (98.67%)\n\nEPOCH: 11\n</pre> <pre>Loss=0.09216612577438354 Batch_id=468 Accuracy=98.81: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.98it/s]\n</pre> <pre>\nTest set: Average loss: 0.0427, Accuracy: 9868/10000 (98.68%)\n\nEPOCH: 12\n</pre> <pre>Loss=0.046782661229372025 Batch_id=468 Accuracy=98.85: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:16&lt;00:00, 27.73it/s]\n</pre> <pre>\nTest set: Average loss: 0.0408, Accuracy: 9872/10000 (98.72%)\n\nEPOCH: 13\n</pre> <pre>Loss=0.03296060115098953 Batch_id=468 Accuracy=98.92: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.90it/s]\n</pre> <pre>\nTest set: Average loss: 0.0427, Accuracy: 9866/10000 (98.66%)\n\nEPOCH: 14\n</pre> <pre>Loss=0.007678658235818148 Batch_id=468 Accuracy=98.97: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:16&lt;00:00, 27.77it/s]\n</pre> <pre>\nTest set: Average loss: 0.0379, Accuracy: 9882/10000 (98.82%)\n\nEPOCH: 15\n</pre> <pre>Loss=0.019363677129149437 Batch_id=468 Accuracy=98.99: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 23.61it/s]\n</pre> <pre>\nTest set: Average loss: 0.0407, Accuracy: 9869/10000 (98.69%)\n\nEPOCH: 16\n</pre> <pre>Loss=0.0011728400131687522 Batch_id=468 Accuracy=99.15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 21.07it/s]\n</pre> <pre>\nTest set: Average loss: 0.0386, Accuracy: 9873/10000 (98.73%)\n\nEPOCH: 17\n</pre> <pre>Loss=0.005977586377412081 Batch_id=468 Accuracy=99.11: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.57it/s]\n</pre> <pre>\nTest set: Average loss: 0.0415, Accuracy: 9875/10000 (98.75%)\n\nEPOCH: 18\n</pre> <pre>Loss=0.007099102716892958 Batch_id=468 Accuracy=99.18: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.92it/s]\n</pre> <pre>\nTest set: Average loss: 0.0448, Accuracy: 9870/10000 (98.70%)\n\nEPOCH: 19\n</pre> <pre>Loss=0.008157476782798767 Batch_id=468 Accuracy=99.19: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.91it/s]\n</pre> <pre>\nTest set: Average loss: 0.0384, Accuracy: 9876/10000 (98.76%)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>t = [t_items.item() for t_items in train_losses]\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(2,2,figsize=(15,10))\naxs[0, 0].plot(t)\naxs[0, 0].set_title(\"Training Loss\")\naxs[1, 0].plot(train_acc)\naxs[1, 0].set_title(\"Training Accuracy\")\naxs[0, 1].plot(test_losses)\naxs[0, 1].set_title(\"Test Loss\")\naxs[1, 1].plot(test_acc)\naxs[1, 1].set_title(\"Test Accuracy\")\n</pre> t = [t_items.item() for t_items in train_losses] %matplotlib inline import matplotlib.pyplot as plt fig, axs = plt.subplots(2,2,figsize=(15,10)) axs[0, 0].plot(t) axs[0, 0].set_title(\"Training Loss\") axs[1, 0].plot(train_acc) axs[1, 0].set_title(\"Training Accuracy\") axs[0, 1].plot(test_losses) axs[0, 1].set_title(\"Test Loss\") axs[1, 1].plot(test_acc) axs[1, 1].set_title(\"Test Accuracy\") Out[\u00a0]: <pre>Text(0.5, 1.0, 'Test Accuracy')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/deep_learning/Code_3/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"notebooks/deep_learning/Code_3/#data-transformations","title":"Data Transformations\u00b6","text":"<p>We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.</p>"},{"location":"notebooks/deep_learning/Code_3/#dataset-and-creating-traintest-split","title":"Dataset and Creating Train/Test Split\u00b6","text":""},{"location":"notebooks/deep_learning/Code_3/#dataloader-arguments-testtrain-dataloaders","title":"Dataloader Arguments &amp; Test/Train Dataloaders\u00b6","text":""},{"location":"notebooks/deep_learning/Code_3/#data-statistics","title":"Data Statistics\u00b6","text":"<p>It is important to know your data very well. Let's check some of the statistics around our data and how it actually looks like</p>"},{"location":"notebooks/deep_learning/Code_3/#more","title":"MORE\u00b6","text":"<p>It is important that we view as many images as possible. This is required to get some idea on image augmentation later on</p>"},{"location":"notebooks/deep_learning/Code_3/#the-model","title":"The model\u00b6","text":"<p>Let's start with the model we first saw</p>"},{"location":"notebooks/deep_learning/Code_3/#model-params","title":"Model Params\u00b6","text":"<p>Can't emphasize on how important viewing Model Summary is. Unfortunately, there is no in-built model visualizer, so we have to take external help</p>"},{"location":"notebooks/deep_learning/Code_3/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>Looking at logs can be boring, so we'll introduce tqdm progressbar to get cooler logs.</p> <p>Let's write train and test functions</p>"},{"location":"notebooks/deep_learning/Code_3/#lets-train-and-test-our-model","title":"Let's Train and test our model\u00b6","text":""},{"location":"notebooks/deep_learning/Code_4/","title":"Code 4","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n</pre> from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Train Phase transformations\ntrain_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\n\n# Test Phase transformations\ntest_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n</pre> # Train Phase transformations train_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ])  # Test Phase transformations test_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,))                                        ])  In\u00a0[\u00a0]: Copied! <pre>train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n</pre> train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 105912966.24it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 101114936.41it/s]</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 24069506.83it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 2218014.76it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n</pre> SEED = 1  # CUDA? cuda = torch.cuda.is_available() print(\"CUDA Available?\", cuda)  # For reproducibility torch.manual_seed(SEED)  if cuda:     torch.cuda.manual_seed(SEED)  # dataloader arguments - something you'll fetch these from cmdprmt dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)  # train dataloader train_loader = torch.utils.data.DataLoader(train, **dataloader_args)  # test dataloader test_loader = torch.utils.data.DataLoader(test, **dataloader_args) <pre>CUDA Available? True\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</pre> In\u00a0[\u00a0]: Copied! <pre># We'd need to convert it into Numpy! Remember above we have converted it into tensors already\ntrain_data = train.train_data\ntrain_data = train.transform(train_data.numpy())\n\nprint('[Train]')\nprint(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\nprint(' - Tensor Shape:', train.train_data.size())\nprint(' - min:', torch.min(train_data))\nprint(' - max:', torch.max(train_data))\nprint(' - mean:', torch.mean(train_data))\nprint(' - std:', torch.std(train_data))\nprint(' - var:', torch.var(train_data))\n\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(labels.shape)\n\n# Let's visualize some of the images\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n</pre> # We'd need to convert it into Numpy! Remember above we have converted it into tensors already train_data = train.train_data train_data = train.transform(train_data.numpy())  print('[Train]') print(' - Numpy Shape:', train.train_data.cpu().numpy().shape) print(' - Tensor Shape:', train.train_data.size()) print(' - min:', torch.min(train_data)) print(' - max:', torch.max(train_data)) print(' - mean:', torch.mean(train_data)) print(' - std:', torch.std(train_data)) print(' - var:', torch.var(train_data))  dataiter = iter(train_loader) images, labels = dataiter.next()  print(images.shape) print(labels.shape)  # Let's visualize some of the images %matplotlib inline import matplotlib.pyplot as plt  plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')  <pre>/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n</pre> <pre>[Train]\n - Numpy Shape: (60000, 28, 28)\n - Tensor Shape: torch.Size([60000, 28, 28])\n - min: tensor(-0.4242)\n - max: tensor(2.8215)\n - mean: tensor(-0.0001)\n - std: tensor(1.0000)\n - var: tensor(1.0001)\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-5-f75c6547409a&gt; in &lt;cell line: 15&gt;()\n     13 \n     14 dataiter = iter(train_loader)\n---&gt; 15 images, labels = dataiter.next()\n     16 \n     17 print(images.shape)\n\nAttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'</pre> In\u00a0[\u00a0]: Copied! <pre>figure = plt.figure()\nnum_of_images = 60\nfor index in range(1, num_of_images + 1):\n    plt.subplot(6, 10, index)\n    plt.axis('off')\n    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n</pre> figure = plt.figure() num_of_images = 60 for index in range(1, num_of_images + 1):     plt.subplot(6, 10, index)     plt.axis('off')     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r') In\u00a0[\u00a0]: Copied! <pre>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Input Block\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 26\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 24\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # output_size = 22\n\n        # TRANSITION BLOCK 1\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 11\n\n        # CONVOLUTION BLOCK 2\n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 9\n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # output_size = 7\n\n        # OUTPUT BLOCK\n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 7\n        self.convblock8 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(7, 7), padding=0, bias=False),\n            # nn.BatchNorm2d(10), NEVER\n            # nn.ReLU() NEVER!\n        ) # output_size = 1\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        x = self.pool1(x)\n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        x = self.convblock6(x)\n        x = self.convblock7(x)\n        x = self.convblock8(x)\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n</pre> class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         # Input Block         self.convblock1 = nn.Sequential(             nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 26          # CONVOLUTION BLOCK 1         self.convblock2 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 24         self.convblock3 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(20),             nn.ReLU()         ) # output_size = 22          # TRANSITION BLOCK 1         self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11         self.convblock4 = nn.Sequential(             nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 11          # CONVOLUTION BLOCK 2         self.convblock5 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 9         self.convblock6 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(20),             nn.ReLU()         ) # output_size = 7          # OUTPUT BLOCK         self.convblock7 = nn.Sequential(             nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 7         self.convblock8 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(7, 7), padding=0, bias=False),             # nn.BatchNorm2d(10), NEVER             # nn.ReLU() NEVER!         ) # output_size = 1      def forward(self, x):         x = self.convblock1(x)         x = self.convblock2(x)         x = self.convblock3(x)         x = self.pool1(x)         x = self.convblock4(x)         x = self.convblock5(x)         x = self.convblock6(x)         x = self.convblock7(x)         x = self.convblock8(x)         x = x.view(-1, 10)         return F.log_softmax(x, dim=-1) In\u00a0[\u00a0]: Copied! <pre>!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))\n</pre> !pip install torchsummary from torchsummary import summary use_cuda = torch.cuda.is_available() device = torch.device(\"cuda\" if use_cuda else \"cpu\") print(device) model = Net().to(device) summary(model, input_size=(1, 28, 28)) <pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 10, 26, 26]              90\n       BatchNorm2d-2           [-1, 10, 26, 26]              20\n              ReLU-3           [-1, 10, 26, 26]               0\n            Conv2d-4           [-1, 10, 24, 24]             900\n       BatchNorm2d-5           [-1, 10, 24, 24]              20\n              ReLU-6           [-1, 10, 24, 24]               0\n            Conv2d-7           [-1, 20, 22, 22]           1,800\n       BatchNorm2d-8           [-1, 20, 22, 22]              40\n              ReLU-9           [-1, 20, 22, 22]               0\n        MaxPool2d-10           [-1, 20, 11, 11]               0\n           Conv2d-11           [-1, 10, 11, 11]             200\n      BatchNorm2d-12           [-1, 10, 11, 11]              20\n             ReLU-13           [-1, 10, 11, 11]               0\n           Conv2d-14             [-1, 10, 9, 9]             900\n      BatchNorm2d-15             [-1, 10, 9, 9]              20\n             ReLU-16             [-1, 10, 9, 9]               0\n           Conv2d-17             [-1, 20, 7, 7]           1,800\n      BatchNorm2d-18             [-1, 20, 7, 7]              40\n             ReLU-19             [-1, 20, 7, 7]               0\n           Conv2d-20             [-1, 10, 7, 7]             200\n      BatchNorm2d-21             [-1, 10, 7, 7]              20\n             ReLU-22             [-1, 10, 7, 7]               0\n           Conv2d-23             [-1, 10, 1, 1]           4,900\n================================================================\nTotal params: 10,970\nTrainable params: 10,970\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.61\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.65\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))\n</pre> from tqdm import tqdm  train_losses = [] test_losses = [] train_acc = [] test_acc = []  def train(model, device, train_loader, optimizer, epoch):   model.train()   pbar = tqdm(train_loader)   correct = 0   processed = 0   for batch_idx, (data, target) in enumerate(pbar):     # get samples     data, target = data.to(device), target.to(device)      # Init     optimizer.zero_grad()     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.      # Predict     y_pred = model(data)      # Calculate loss     loss = F.nll_loss(y_pred, target)     train_losses.append(loss)      # Backpropagation     loss.backward()     optimizer.step()      # Update pbar-tqdm      pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability     correct += pred.eq(target.view_as(pred)).sum().item()     processed += len(data)      pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')     train_acc.append(100*correct/processed)  def test(model, device, test_loader):     model.eval()     test_loss = 0     correct = 0     with torch.no_grad():         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()      test_loss /= len(test_loader.dataset)     test_losses.append(test_loss)      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(         test_loss, correct, len(test_loader.dataset),         100. * correct / len(test_loader.dataset)))      test_acc.append(100. * correct / len(test_loader.dataset)) In\u00a0[\u00a0]: Copied! <pre>model =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)\n</pre> model =  Net().to(device) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) EPOCHS = 20 for epoch in range(EPOCHS):     print(\"EPOCH:\", epoch)     train(model, device, train_loader, optimizer, epoch)     test(model, device, test_loader) <pre>EPOCH: 0\n</pre> <pre>Loss=0.07356555014848709 Batch_id=468 Accuracy=94.45: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.40it/s]\n</pre> <pre>\nTest set: Average loss: 0.0659, Accuracy: 9786/10000 (97.86%)\n\nEPOCH: 1\n</pre> <pre>Loss=0.036042749881744385 Batch_id=468 Accuracy=98.39: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 26.01it/s]\n</pre> <pre>\nTest set: Average loss: 0.0379, Accuracy: 9887/10000 (98.87%)\n\nEPOCH: 2\n</pre> <pre>Loss=0.06399010121822357 Batch_id=468 Accuracy=98.81: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.90it/s]\n</pre> <pre>\nTest set: Average loss: 0.0351, Accuracy: 9881/10000 (98.81%)\n\nEPOCH: 3\n</pre> <pre>Loss=0.02564128302037716 Batch_id=468 Accuracy=99.03: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.16it/s]\n</pre> <pre>\nTest set: Average loss: 0.0329, Accuracy: 9890/10000 (98.90%)\n\nEPOCH: 4\n</pre> <pre>Loss=0.007669726852327585 Batch_id=468 Accuracy=99.13: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.40it/s]\n</pre> <pre>\nTest set: Average loss: 0.0319, Accuracy: 9889/10000 (98.89%)\n\nEPOCH: 5\n</pre> <pre>Loss=0.019197234883904457 Batch_id=468 Accuracy=99.26: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.83it/s]\n</pre> <pre>\nTest set: Average loss: 0.0273, Accuracy: 9897/10000 (98.97%)\n\nEPOCH: 6\n</pre> <pre>Loss=0.022666513919830322 Batch_id=468 Accuracy=99.37: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.17it/s]\n</pre> <pre>\nTest set: Average loss: 0.0278, Accuracy: 9912/10000 (99.12%)\n\nEPOCH: 7\n</pre> <pre>Loss=0.029269637539982796 Batch_id=468 Accuracy=99.43: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.95it/s]\n</pre> <pre>\nTest set: Average loss: 0.0237, Accuracy: 9922/10000 (99.22%)\n\nEPOCH: 8\n</pre> <pre>Loss=0.004982430953532457 Batch_id=468 Accuracy=99.45: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.48it/s]\n</pre> <pre>\nTest set: Average loss: 0.0313, Accuracy: 9897/10000 (98.97%)\n\nEPOCH: 9\n</pre> <pre>Loss=0.001017248840071261 Batch_id=468 Accuracy=99.58: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.19it/s]\n</pre> <pre>\nTest set: Average loss: 0.0278, Accuracy: 9911/10000 (99.11%)\n\nEPOCH: 10\n</pre> <pre>Loss=0.011475987732410431 Batch_id=468 Accuracy=99.59: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.08it/s]\n</pre> <pre>\nTest set: Average loss: 0.0233, Accuracy: 9918/10000 (99.18%)\n\nEPOCH: 11\n</pre> <pre>Loss=0.09133023768663406 Batch_id=468 Accuracy=99.63: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.08it/s]\n</pre> <pre>\nTest set: Average loss: 0.0236, Accuracy: 9922/10000 (99.22%)\n\nEPOCH: 12\n</pre> <pre>Loss=0.016045520082116127 Batch_id=468 Accuracy=99.69: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.41it/s]\n</pre> <pre>\nTest set: Average loss: 0.0244, Accuracy: 9919/10000 (99.19%)\n\nEPOCH: 13\n</pre> <pre>Loss=0.03933852165937424 Batch_id=468 Accuracy=99.69: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.95it/s]\n</pre> <pre>\nTest set: Average loss: 0.0284, Accuracy: 9914/10000 (99.14%)\n\nEPOCH: 14\n</pre> <pre>Loss=0.0023803263902664185 Batch_id=468 Accuracy=99.78: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.26it/s]\n</pre> <pre>\nTest set: Average loss: 0.0234, Accuracy: 9921/10000 (99.21%)\n\nEPOCH: 15\n</pre> <pre>Loss=0.0059540593065321445 Batch_id=468 Accuracy=99.80: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.09it/s]\n</pre> <pre>\nTest set: Average loss: 0.0280, Accuracy: 9914/10000 (99.14%)\n\nEPOCH: 16\n</pre> <pre>Loss=0.0003684877010527998 Batch_id=468 Accuracy=99.86: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.89it/s]\n</pre> <pre>\nTest set: Average loss: 0.0265, Accuracy: 9919/10000 (99.19%)\n\nEPOCH: 17\n</pre> <pre>Loss=0.000611196446698159 Batch_id=468 Accuracy=99.85: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.41it/s]\n</pre> <pre>\nTest set: Average loss: 0.0247, Accuracy: 9917/10000 (99.17%)\n\nEPOCH: 18\n</pre> <pre>Loss=0.0010846791556105018 Batch_id=468 Accuracy=99.88: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.07it/s]\n</pre> <pre>\nTest set: Average loss: 0.0254, Accuracy: 9919/10000 (99.19%)\n\nEPOCH: 19\n</pre> <pre>Loss=0.0023512793704867363 Batch_id=468 Accuracy=99.92: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.18it/s]\n</pre> <pre>\nTest set: Average loss: 0.0257, Accuracy: 9917/10000 (99.17%)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>t = [t_items.item() for t_items in train_losses]\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(2,2,figsize=(15,10))\naxs[0, 0].plot(t)\naxs[0, 0].set_title(\"Training Loss\")\naxs[1, 0].plot(train_acc[4000:])\naxs[1, 0].set_title(\"Training Accuracy\")\naxs[0, 1].plot(test_losses)\naxs[0, 1].set_title(\"Test Loss\")\naxs[1, 1].plot(test_acc)\naxs[1, 1].set_title(\"Test Accuracy\")\n</pre> t = [t_items.item() for t_items in train_losses] %matplotlib inline import matplotlib.pyplot as plt fig, axs = plt.subplots(2,2,figsize=(15,10)) axs[0, 0].plot(t) axs[0, 0].set_title(\"Training Loss\") axs[1, 0].plot(train_acc[4000:]) axs[1, 0].set_title(\"Training Accuracy\") axs[0, 1].plot(test_losses) axs[0, 1].set_title(\"Test Loss\") axs[1, 1].plot(test_acc) axs[1, 1].set_title(\"Test Accuracy\") Out[\u00a0]: <pre>Text(0.5, 1.0, 'Test Accuracy')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/deep_learning/Code_4/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"notebooks/deep_learning/Code_4/#data-transformations","title":"Data Transformations\u00b6","text":"<p>We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.</p>"},{"location":"notebooks/deep_learning/Code_4/#dataset-and-creating-traintest-split","title":"Dataset and Creating Train/Test Split\u00b6","text":""},{"location":"notebooks/deep_learning/Code_4/#dataloader-arguments-testtrain-dataloaders","title":"Dataloader Arguments &amp; Test/Train Dataloaders\u00b6","text":""},{"location":"notebooks/deep_learning/Code_4/#data-statistics","title":"Data Statistics\u00b6","text":"<p>It is important to know your data very well. Let's check some of the statistics around our data and how it actually looks like</p>"},{"location":"notebooks/deep_learning/Code_4/#more","title":"MORE\u00b6","text":"<p>It is important that we view as many images as possible. This is required to get some idea on image augmentation later on</p>"},{"location":"notebooks/deep_learning/Code_4/#the-model","title":"The model\u00b6","text":"<p>Let's start with the model we first saw</p>"},{"location":"notebooks/deep_learning/Code_4/#model-params","title":"Model Params\u00b6","text":"<p>Can't emphasize on how important viewing Model Summary is. Unfortunately, there is no in-built model visualizer, so we have to take external help</p>"},{"location":"notebooks/deep_learning/Code_4/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>Looking at logs can be boring, so we'll introduce tqdm progressbar to get cooler logs.</p> <p>Let's write train and test functions</p>"},{"location":"notebooks/deep_learning/Code_4/#lets-train-and-test-our-model","title":"Let's Train and test our model\u00b6","text":""},{"location":"notebooks/deep_learning/Code_5/","title":"Code 5","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n</pre> from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Train Phase transformations\ntrain_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\n\n# Test Phase transformations\ntest_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n</pre> # Train Phase transformations train_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ])  # Test Phase transformations test_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,))                                        ])  In\u00a0[\u00a0]: Copied! <pre>train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n</pre> train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 234468450.90it/s]</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n</pre> <pre>\n</pre> <pre>\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 41160616.32it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 66543104.53it/s]</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 4482477.36it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n</pre> SEED = 1  # CUDA? cuda = torch.cuda.is_available() print(\"CUDA Available?\", cuda)  # For reproducibility torch.manual_seed(SEED)  if cuda:     torch.cuda.manual_seed(SEED)  # dataloader arguments - something you'll fetch these from cmdprmt dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)  # train dataloader train_loader = torch.utils.data.DataLoader(train, **dataloader_args)  # test dataloader test_loader = torch.utils.data.DataLoader(test, **dataloader_args) <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</pre> <pre>CUDA Available? True\n</pre> In\u00a0[\u00a0]: Copied! <pre># We'd need to convert it into Numpy! Remember above we have converted it into tensors already\ntrain_data = train.train_data\ntrain_data = train.transform(train_data.numpy())\n\nprint('[Train]')\nprint(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\nprint(' - Tensor Shape:', train.train_data.size())\nprint(' - min:', torch.min(train_data))\nprint(' - max:', torch.max(train_data))\nprint(' - mean:', torch.mean(train_data))\nprint(' - std:', torch.std(train_data))\nprint(' - var:', torch.var(train_data))\n\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(labels.shape)\n\n# Let's visualize some of the images\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n</pre> # We'd need to convert it into Numpy! Remember above we have converted it into tensors already train_data = train.train_data train_data = train.transform(train_data.numpy())  print('[Train]') print(' - Numpy Shape:', train.train_data.cpu().numpy().shape) print(' - Tensor Shape:', train.train_data.size()) print(' - min:', torch.min(train_data)) print(' - max:', torch.max(train_data)) print(' - mean:', torch.mean(train_data)) print(' - std:', torch.std(train_data)) print(' - var:', torch.var(train_data))  dataiter = iter(train_loader) images, labels = dataiter.next()  print(images.shape) print(labels.shape)  # Let's visualize some of the images %matplotlib inline import matplotlib.pyplot as plt  plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')  <pre>/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n</pre> <pre>[Train]\n - Numpy Shape: (60000, 28, 28)\n - Tensor Shape: torch.Size([60000, 28, 28])\n - min: tensor(-0.4242)\n - max: tensor(2.8215)\n - mean: tensor(-0.0001)\n - std: tensor(1.0000)\n - var: tensor(1.0001)\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-5-f75c6547409a&gt; in &lt;cell line: 15&gt;()\n     13 \n     14 dataiter = iter(train_loader)\n---&gt; 15 images, labels = dataiter.next()\n     16 \n     17 print(images.shape)\n\nAttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'</pre> In\u00a0[\u00a0]: Copied! <pre>figure = plt.figure()\nnum_of_images = 60\nfor index in range(1, num_of_images + 1):\n    plt.subplot(6, 10, index)\n    plt.axis('off')\n    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n</pre> figure = plt.figure() num_of_images = 60 for index in range(1, num_of_images + 1):     plt.subplot(6, 10, index)     plt.axis('off')     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r') In\u00a0[\u00a0]: Copied! <pre>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Input Block\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 26\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 24\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # output_size = 22\n\n        # TRANSITION BLOCK 1\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 11\n\n        # CONVOLUTION BLOCK 2\n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 9\n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # output_size = 7\n\n        # OUTPUT BLOCK\n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 7\n        self.convblock8 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(7, 7), padding=0, bias=False),\n            # nn.BatchNorm2d(10), NEVER\n            # nn.ReLU() NEVER!\n        ) # output_size = 1\n\n        self.dropout = nn.Dropout(0.25) # WRONG 0.05-0.1\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        x = self.dropout(x) #WRONG\n        x = self.pool1(x)\n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        x = self.convblock6(x) #WRONG\n        x = self.dropout(x)\n        x = self.convblock7(x)\n        x = self.convblock8(x)\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n</pre> class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         # Input Block         self.convblock1 = nn.Sequential(             nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 26          # CONVOLUTION BLOCK 1         self.convblock2 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 24         self.convblock3 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(20),             nn.ReLU()         ) # output_size = 22          # TRANSITION BLOCK 1         self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11         self.convblock4 = nn.Sequential(             nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 11          # CONVOLUTION BLOCK 2         self.convblock5 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 9         self.convblock6 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(20),             nn.ReLU()         ) # output_size = 7          # OUTPUT BLOCK         self.convblock7 = nn.Sequential(             nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 7         self.convblock8 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(7, 7), padding=0, bias=False),             # nn.BatchNorm2d(10), NEVER             # nn.ReLU() NEVER!         ) # output_size = 1          self.dropout = nn.Dropout(0.25) # WRONG 0.05-0.1      def forward(self, x):         x = self.convblock1(x)         x = self.convblock2(x)         x = self.convblock3(x)         x = self.dropout(x) #WRONG         x = self.pool1(x)         x = self.convblock4(x)         x = self.convblock5(x)         x = self.convblock6(x) #WRONG         x = self.dropout(x)         x = self.convblock7(x)         x = self.convblock8(x)         x = x.view(-1, 10)         return F.log_softmax(x, dim=-1) In\u00a0[\u00a0]: Copied! <pre>!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))\n</pre> !pip install torchsummary from torchsummary import summary use_cuda = torch.cuda.is_available() device = torch.device(\"cuda\" if use_cuda else \"cpu\") print(device) model = Net().to(device) summary(model, input_size=(1, 28, 28)) <pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 10, 26, 26]              90\n       BatchNorm2d-2           [-1, 10, 26, 26]              20\n              ReLU-3           [-1, 10, 26, 26]               0\n            Conv2d-4           [-1, 10, 24, 24]             900\n       BatchNorm2d-5           [-1, 10, 24, 24]              20\n              ReLU-6           [-1, 10, 24, 24]               0\n            Conv2d-7           [-1, 20, 22, 22]           1,800\n       BatchNorm2d-8           [-1, 20, 22, 22]              40\n              ReLU-9           [-1, 20, 22, 22]               0\n          Dropout-10           [-1, 20, 22, 22]               0\n        MaxPool2d-11           [-1, 20, 11, 11]               0\n           Conv2d-12           [-1, 10, 11, 11]             200\n      BatchNorm2d-13           [-1, 10, 11, 11]              20\n             ReLU-14           [-1, 10, 11, 11]               0\n           Conv2d-15             [-1, 10, 9, 9]             900\n      BatchNorm2d-16             [-1, 10, 9, 9]              20\n             ReLU-17             [-1, 10, 9, 9]               0\n           Conv2d-18             [-1, 20, 7, 7]           1,800\n      BatchNorm2d-19             [-1, 20, 7, 7]              40\n             ReLU-20             [-1, 20, 7, 7]               0\n          Dropout-21             [-1, 20, 7, 7]               0\n           Conv2d-22             [-1, 10, 7, 7]             200\n      BatchNorm2d-23             [-1, 10, 7, 7]              20\n             ReLU-24             [-1, 10, 7, 7]               0\n           Conv2d-25             [-1, 10, 1, 1]           4,900\n================================================================\nTotal params: 10,970\nTrainable params: 10,970\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.69\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.73\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))\n</pre> from tqdm import tqdm  train_losses = [] test_losses = [] train_acc = [] test_acc = []  def train(model, device, train_loader, optimizer, epoch):   model.train()   pbar = tqdm(train_loader)   correct = 0   processed = 0   for batch_idx, (data, target) in enumerate(pbar):     # get samples     data, target = data.to(device), target.to(device)      # Init     optimizer.zero_grad()     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.      # Predict     y_pred = model(data)      # Calculate loss     loss = F.nll_loss(y_pred, target)     train_losses.append(loss)      # Backpropagation     loss.backward()     optimizer.step()      # Update pbar-tqdm      pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability     correct += pred.eq(target.view_as(pred)).sum().item()     processed += len(data)      pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')     train_acc.append(100*correct/processed)  def test(model, device, test_loader):     model.eval()     test_loss = 0     correct = 0     with torch.no_grad():         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()      test_loss /= len(test_loader.dataset)     test_losses.append(test_loss)      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(         test_loss, correct, len(test_loader.dataset),         100. * correct / len(test_loader.dataset)))      test_acc.append(100. * correct / len(test_loader.dataset)) In\u00a0[\u00a0]: Copied! <pre>model =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nEPOCHS = 25\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)\n</pre> model =  Net().to(device) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) EPOCHS = 25 for epoch in range(EPOCHS):     print(\"EPOCH:\", epoch)     train(model, device, train_loader, optimizer, epoch)     test(model, device, test_loader) <pre>EPOCH: 0\n</pre> <pre>Loss=0.14537030458450317 Batch_id=468 Accuracy=93.06: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 21.30it/s]\n</pre> <pre>\nTest set: Average loss: 0.1119, Accuracy: 9654/10000 (96.54%)\n\nEPOCH: 1\n</pre> <pre>Loss=0.046182166785001755 Batch_id=468 Accuracy=97.98: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 20.52it/s]\n</pre> <pre>\nTest set: Average loss: 0.0478, Accuracy: 9863/10000 (98.63%)\n\nEPOCH: 2\n</pre> <pre>Loss=0.08074275404214859 Batch_id=468 Accuracy=98.37: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:16&lt;00:00, 27.63it/s]\n</pre> <pre>\nTest set: Average loss: 0.0454, Accuracy: 9871/10000 (98.71%)\n\nEPOCH: 3\n</pre> <pre>Loss=0.00934766698628664 Batch_id=468 Accuracy=98.59: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.82it/s]\n</pre> <pre>\nTest set: Average loss: 0.0366, Accuracy: 9881/10000 (98.81%)\n\nEPOCH: 4\n</pre> <pre>Loss=0.004754191264510155 Batch_id=468 Accuracy=98.72: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.07it/s]\n</pre> <pre>\nTest set: Average loss: 0.0474, Accuracy: 9847/10000 (98.47%)\n\nEPOCH: 5\n</pre> <pre>Loss=0.03414740785956383 Batch_id=468 Accuracy=98.89: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.32it/s]\n</pre> <pre>\nTest set: Average loss: 0.0354, Accuracy: 9886/10000 (98.86%)\n\nEPOCH: 6\n</pre> <pre>Loss=0.032188866287469864 Batch_id=468 Accuracy=98.98: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.90it/s]\n</pre> <pre>\nTest set: Average loss: 0.0449, Accuracy: 9858/10000 (98.58%)\n\nEPOCH: 7\n</pre> <pre>Loss=0.019482962787151337 Batch_id=468 Accuracy=99.00: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.12it/s]\n</pre> <pre>\nTest set: Average loss: 0.0359, Accuracy: 9877/10000 (98.77%)\n\nEPOCH: 8\n</pre> <pre>Loss=0.014714364893734455 Batch_id=468 Accuracy=99.07: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.96it/s]\n</pre> <pre>\nTest set: Average loss: 0.0377, Accuracy: 9875/10000 (98.75%)\n\nEPOCH: 9\n</pre> <pre>Loss=0.009072361513972282 Batch_id=468 Accuracy=99.17: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.23it/s]\n</pre> <pre>\nTest set: Average loss: 0.0386, Accuracy: 9877/10000 (98.77%)\n\nEPOCH: 10\n</pre> <pre>Loss=0.08451824635267258 Batch_id=468 Accuracy=99.13: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.69it/s]\n</pre> <pre>\nTest set: Average loss: 0.0405, Accuracy: 9865/10000 (98.65%)\n\nEPOCH: 11\n</pre> <pre>Loss=0.049892082810401917 Batch_id=468 Accuracy=99.12: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.60it/s]\n</pre> <pre>\nTest set: Average loss: 0.0313, Accuracy: 9902/10000 (99.02%)\n\nEPOCH: 12\n</pre> <pre>Loss=0.012026932090520859 Batch_id=468 Accuracy=99.25: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.38it/s]\n</pre> <pre>\nTest set: Average loss: 0.0311, Accuracy: 9900/10000 (99.00%)\n\nEPOCH: 13\n</pre> <pre>Loss=0.050940413028001785 Batch_id=468 Accuracy=99.28: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.52it/s]\n</pre> <pre>\nTest set: Average loss: 0.0283, Accuracy: 9908/10000 (99.08%)\n\nEPOCH: 14\n</pre> <pre>Loss=0.01183382049202919 Batch_id=468 Accuracy=99.20: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.69it/s]\n</pre> <pre>\nTest set: Average loss: 0.0297, Accuracy: 9907/10000 (99.07%)\n\nEPOCH: 15\n</pre> <pre>Loss=0.013279805891215801 Batch_id=468 Accuracy=99.28: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.85it/s]\n</pre> <pre>\nTest set: Average loss: 0.0294, Accuracy: 9892/10000 (98.92%)\n\nEPOCH: 16\n</pre> <pre>Loss=0.001536824624054134 Batch_id=468 Accuracy=99.36: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.38it/s]\n</pre> <pre>\nTest set: Average loss: 0.0300, Accuracy: 9896/10000 (98.96%)\n\nEPOCH: 17\n</pre> <pre>Loss=0.00842300895601511 Batch_id=468 Accuracy=99.30: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.83it/s]\n</pre> <pre>\nTest set: Average loss: 0.0306, Accuracy: 9901/10000 (99.01%)\n\nEPOCH: 18\n</pre> <pre>Loss=0.006372010800987482 Batch_id=468 Accuracy=99.36: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.34it/s]\n</pre> <pre>\nTest set: Average loss: 0.0292, Accuracy: 9908/10000 (99.08%)\n\nEPOCH: 19\n</pre> <pre>Loss=0.018397802487015724 Batch_id=468 Accuracy=99.38: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.72it/s]\n</pre> <pre>\nTest set: Average loss: 0.0279, Accuracy: 9913/10000 (99.13%)\n\nEPOCH: 20\n</pre> <pre>Loss=0.008465004153549671 Batch_id=468 Accuracy=99.38: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.56it/s]\n</pre> <pre>\nTest set: Average loss: 0.0273, Accuracy: 9908/10000 (99.08%)\n\nEPOCH: 21\n</pre> <pre>Loss=0.0021848510950803757 Batch_id=468 Accuracy=99.40: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.71it/s]\n</pre> <pre>\nTest set: Average loss: 0.0270, Accuracy: 9908/10000 (99.08%)\n\nEPOCH: 22\n</pre> <pre>Loss=0.03612786531448364 Batch_id=468 Accuracy=99.41: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.72it/s]\n</pre> <pre>\nTest set: Average loss: 0.0313, Accuracy: 9899/10000 (98.99%)\n\nEPOCH: 23\n</pre> <pre>Loss=0.027340782806277275 Batch_id=468 Accuracy=99.40: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.35it/s]\n</pre> <pre>\nTest set: Average loss: 0.0286, Accuracy: 9908/10000 (99.08%)\n\nEPOCH: 24\n</pre> <pre>Loss=0.036343153566122055 Batch_id=468 Accuracy=99.44: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.57it/s]\n</pre> <pre>\nTest set: Average loss: 0.0298, Accuracy: 9913/10000 (99.13%)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>t = [t_items.item() for t_items in train_losses]\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(2,2,figsize=(15,10))\naxs[0, 0].plot(t)\naxs[0, 0].set_title(\"Training Loss\")\naxs[1, 0].plot(train_acc[4000:])\naxs[1, 0].set_title(\"Training Accuracy\")\naxs[0, 1].plot(test_losses)\naxs[0, 1].set_title(\"Test Loss\")\naxs[1, 1].plot(test_acc)\naxs[1, 1].set_title(\"Test Accuracy\")\n</pre> t = [t_items.item() for t_items in train_losses] %matplotlib inline import matplotlib.pyplot as plt fig, axs = plt.subplots(2,2,figsize=(15,10)) axs[0, 0].plot(t) axs[0, 0].set_title(\"Training Loss\") axs[1, 0].plot(train_acc[4000:]) axs[1, 0].set_title(\"Training Accuracy\") axs[0, 1].plot(test_losses) axs[0, 1].set_title(\"Test Loss\") axs[1, 1].plot(test_acc) axs[1, 1].set_title(\"Test Accuracy\") Out[\u00a0]: <pre>Text(0.5, 1.0, 'Test Accuracy')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/deep_learning/Code_5/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"notebooks/deep_learning/Code_5/#data-transformations","title":"Data Transformations\u00b6","text":"<p>We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.</p>"},{"location":"notebooks/deep_learning/Code_5/#dataset-and-creating-traintest-split","title":"Dataset and Creating Train/Test Split\u00b6","text":""},{"location":"notebooks/deep_learning/Code_5/#dataloader-arguments-testtrain-dataloaders","title":"Dataloader Arguments &amp; Test/Train Dataloaders\u00b6","text":""},{"location":"notebooks/deep_learning/Code_5/#data-statistics","title":"Data Statistics\u00b6","text":"<p>It is important to know your data very well. Let's check some of the statistics around our data and how it actually looks like</p>"},{"location":"notebooks/deep_learning/Code_5/#more","title":"MORE\u00b6","text":"<p>It is important that we view as many images as possible. This is required to get some idea on image augmentation later on</p>"},{"location":"notebooks/deep_learning/Code_5/#the-model","title":"The model\u00b6","text":"<p>Let's start with the model we first saw</p>"},{"location":"notebooks/deep_learning/Code_5/#model-params","title":"Model Params\u00b6","text":"<p>Can't emphasize on how important viewing Model Summary is. Unfortunately, there is no in-built model visualizer, so we have to take external help</p>"},{"location":"notebooks/deep_learning/Code_5/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>Looking at logs can be boring, so we'll introduce tqdm progressbar to get cooler logs.</p> <p>Let's write train and test functions</p>"},{"location":"notebooks/deep_learning/Code_5/#lets-train-and-test-our-model","title":"Let's Train and test our model\u00b6","text":""},{"location":"notebooks/deep_learning/Code_6/","title":"Code 6","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n</pre> from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Train Phase transformations\ntrain_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\n\n# Test Phase transformations\ntest_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n</pre> # Train Phase transformations train_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ])  # Test Phase transformations test_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,))                                        ])  In\u00a0[\u00a0]: Copied! <pre>train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n</pre> train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 374626832.50it/s]</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n</pre> <pre>\n</pre> <pre>\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 25105843.28it/s]</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n</pre> <pre>\n</pre> <pre>Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 158385237.53it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 21623755.70it/s]</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n</pre> SEED = 1  # CUDA? cuda = torch.cuda.is_available() print(\"CUDA Available?\", cuda)  # For reproducibility torch.manual_seed(SEED)  if cuda:     torch.cuda.manual_seed(SEED)  # dataloader arguments - something you'll fetch these from cmdprmt dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)  # train dataloader train_loader = torch.utils.data.DataLoader(train, **dataloader_args)  # test dataloader test_loader = torch.utils.data.DataLoader(test, **dataloader_args) <pre>CUDA Available? True\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</pre> In\u00a0[\u00a0]: Copied! <pre># We'd need to convert it into Numpy! Remember above we have converted it into tensors already\ntrain_data = train.train_data\ntrain_data = train.transform(train_data.numpy())\n\nprint('[Train]')\nprint(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\nprint(' - Tensor Shape:', train.train_data.size())\nprint(' - min:', torch.min(train_data))\nprint(' - max:', torch.max(train_data))\nprint(' - mean:', torch.mean(train_data))\nprint(' - std:', torch.std(train_data))\nprint(' - var:', torch.var(train_data))\n\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(labels.shape)\n\n# Let's visualize some of the images\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n</pre> # We'd need to convert it into Numpy! Remember above we have converted it into tensors already train_data = train.train_data train_data = train.transform(train_data.numpy())  print('[Train]') print(' - Numpy Shape:', train.train_data.cpu().numpy().shape) print(' - Tensor Shape:', train.train_data.size()) print(' - min:', torch.min(train_data)) print(' - max:', torch.max(train_data)) print(' - mean:', torch.mean(train_data)) print(' - std:', torch.std(train_data)) print(' - var:', torch.var(train_data))  dataiter = iter(train_loader) images, labels = dataiter.next()  print(images.shape) print(labels.shape)  # Let's visualize some of the images %matplotlib inline import matplotlib.pyplot as plt  plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')  <pre>/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n</pre> <pre>[Train]\n - Numpy Shape: (60000, 28, 28)\n - Tensor Shape: torch.Size([60000, 28, 28])\n - min: tensor(-0.4242)\n - max: tensor(2.8215)\n - mean: tensor(-0.0001)\n - std: tensor(1.0000)\n - var: tensor(1.0001)\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-5-f75c6547409a&gt; in &lt;cell line: 15&gt;()\n     13 \n     14 dataiter = iter(train_loader)\n---&gt; 15 images, labels = dataiter.next()\n     16 \n     17 print(images.shape)\n\nAttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'</pre> In\u00a0[\u00a0]: Copied! <pre>figure = plt.figure()\nnum_of_images = 60\nfor index in range(1, num_of_images + 1):\n    plt.subplot(6, 10, index)\n    plt.axis('off')\n    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n</pre> figure = plt.figure() num_of_images = 60 for index in range(1, num_of_images + 1):     plt.subplot(6, 10, index)     plt.axis('off')     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r') In\u00a0[\u00a0]: Copied! <pre>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Input Block 28  &gt;&gt;&gt; 64\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 26 &gt;&gt;&gt; 62\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 24. &gt;&gt;&gt; 60\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # output_size = 22 &gt;&gt;&gt; 58\n\n        # TRANSITION BLOCK 1\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 11 &gt;&gt;&gt; 29\n\n        # CONVOLUTION BLOCK 2\n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 9 &gt;&gt;&gt; 27\n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # output_size = 7 &gt;&gt;&gt; 25\n\n        # OUTPUT BLOCK\n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 7 &gt;&gt;&gt; 25\n        self.gap = nn.Sequential(\n            nn.AvgPool2d(kernel_size=7) # 7&gt;&gt; 9... nn.AdaptiveAvgPool((1, 1))\n        ) # output_size = 1\n\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        x = self.dropout(x)\n        x = self.pool1(x)\n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        x = self.convblock6(x)\n        x = self.dropout(x)\n        x = self.convblock7(x)\n        x = self.gap(x)\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n</pre> class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         # Input Block 28  &gt;&gt;&gt; 64         self.convblock1 = nn.Sequential(             nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 26 &gt;&gt;&gt; 62          # CONVOLUTION BLOCK 1         self.convblock2 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 24. &gt;&gt;&gt; 60         self.convblock3 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(20),             nn.ReLU()         ) # output_size = 22 &gt;&gt;&gt; 58          # TRANSITION BLOCK 1         self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11         self.convblock4 = nn.Sequential(             nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 11 &gt;&gt;&gt; 29          # CONVOLUTION BLOCK 2         self.convblock5 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 9 &gt;&gt;&gt; 27         self.convblock6 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(20),             nn.ReLU()         ) # output_size = 7 &gt;&gt;&gt; 25          # OUTPUT BLOCK         self.convblock7 = nn.Sequential(             nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 7 &gt;&gt;&gt; 25         self.gap = nn.Sequential(             nn.AvgPool2d(kernel_size=7) # 7&gt;&gt; 9... nn.AdaptiveAvgPool((1, 1))         ) # output_size = 1          self.dropout = nn.Dropout(0.25)      def forward(self, x):         x = self.convblock1(x)         x = self.convblock2(x)         x = self.convblock3(x)         x = self.dropout(x)         x = self.pool1(x)         x = self.convblock4(x)         x = self.convblock5(x)         x = self.convblock6(x)         x = self.dropout(x)         x = self.convblock7(x)         x = self.gap(x)         x = x.view(-1, 10)         return F.log_softmax(x, dim=-1) In\u00a0[\u00a0]: Copied! <pre>!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))\n</pre> !pip install torchsummary from torchsummary import summary use_cuda = torch.cuda.is_available() device = torch.device(\"cuda\" if use_cuda else \"cpu\") print(device) model = Net().to(device) summary(model, input_size=(1, 28, 28)) <pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 10, 26, 26]              90\n       BatchNorm2d-2           [-1, 10, 26, 26]              20\n              ReLU-3           [-1, 10, 26, 26]               0\n            Conv2d-4           [-1, 10, 24, 24]             900\n       BatchNorm2d-5           [-1, 10, 24, 24]              20\n              ReLU-6           [-1, 10, 24, 24]               0\n            Conv2d-7           [-1, 20, 22, 22]           1,800\n       BatchNorm2d-8           [-1, 20, 22, 22]              40\n              ReLU-9           [-1, 20, 22, 22]               0\n          Dropout-10           [-1, 20, 22, 22]               0\n        MaxPool2d-11           [-1, 20, 11, 11]               0\n           Conv2d-12           [-1, 10, 11, 11]             200\n      BatchNorm2d-13           [-1, 10, 11, 11]              20\n             ReLU-14           [-1, 10, 11, 11]               0\n           Conv2d-15             [-1, 10, 9, 9]             900\n      BatchNorm2d-16             [-1, 10, 9, 9]              20\n             ReLU-17             [-1, 10, 9, 9]               0\n           Conv2d-18             [-1, 20, 7, 7]           1,800\n      BatchNorm2d-19             [-1, 20, 7, 7]              40\n             ReLU-20             [-1, 20, 7, 7]               0\n          Dropout-21             [-1, 20, 7, 7]               0\n           Conv2d-22             [-1, 10, 7, 7]             200\n      BatchNorm2d-23             [-1, 10, 7, 7]              20\n             ReLU-24             [-1, 10, 7, 7]               0\n        AvgPool2d-25             [-1, 10, 1, 1]               0\n================================================================\nTotal params: 6,070\nTrainable params: 6,070\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.69\nParams size (MB): 0.02\nEstimated Total Size (MB): 0.71\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))\n</pre> from tqdm import tqdm  train_losses = [] test_losses = [] train_acc = [] test_acc = []  def train(model, device, train_loader, optimizer, epoch):   model.train()   pbar = tqdm(train_loader)   correct = 0   processed = 0   for batch_idx, (data, target) in enumerate(pbar):     # get samples     data, target = data.to(device), target.to(device)      # Init     optimizer.zero_grad()     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.      # Predict     y_pred = model(data)      # Calculate loss     loss = F.nll_loss(y_pred, target)     train_losses.append(loss)      # Backpropagation     loss.backward()     optimizer.step()      # Update pbar-tqdm      pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability     correct += pred.eq(target.view_as(pred)).sum().item()     processed += len(data)      pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')     train_acc.append(100*correct/processed)  def test(model, device, test_loader):     model.eval()     test_loss = 0     correct = 0     with torch.no_grad():         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()      test_loss /= len(test_loader.dataset)     test_losses.append(test_loss)      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(         test_loss, correct, len(test_loader.dataset),         100. * correct / len(test_loader.dataset)))      test_acc.append(100. * correct / len(test_loader.dataset)) In\u00a0[\u00a0]: Copied! <pre>model =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)\n</pre> model =  Net().to(device) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) EPOCHS = 20 for epoch in range(EPOCHS):     print(\"EPOCH:\", epoch)     train(model, device, train_loader, optimizer, epoch)     test(model, device, test_loader) <pre>EPOCH: 0\n</pre> <pre>Loss=0.37875261902809143 Batch_id=468 Accuracy=79.86: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:24&lt;00:00, 19.44it/s]\n</pre> <pre>\nTest set: Average loss: 0.3644, Accuracy: 9489/10000 (94.89%)\n\nEPOCH: 1\n</pre> <pre>Loss=0.14279000461101532 Batch_id=468 Accuracy=96.06: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:27&lt;00:00, 16.85it/s]\n</pre> <pre>\nTest set: Average loss: 0.2392, Accuracy: 9493/10000 (94.93%)\n\nEPOCH: 2\n</pre> <pre>Loss=0.1344187706708908 Batch_id=468 Accuracy=97.08: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.87it/s]\n</pre> <pre>\nTest set: Average loss: 0.2819, Accuracy: 9260/10000 (92.60%)\n\nEPOCH: 3\n</pre> <pre>Loss=0.11288797110319138 Batch_id=468 Accuracy=97.43: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.84it/s]\n</pre> <pre>\nTest set: Average loss: 0.1778, Accuracy: 9576/10000 (95.76%)\n\nEPOCH: 4\n</pre> <pre>Loss=0.07701962441205978 Batch_id=468 Accuracy=97.75: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.20it/s]\n</pre> <pre>\nTest set: Average loss: 0.1388, Accuracy: 9664/10000 (96.64%)\n\nEPOCH: 5\n</pre> <pre>Loss=0.09646547585725784 Batch_id=468 Accuracy=97.95: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.19it/s]\n</pre> <pre>\nTest set: Average loss: 0.1397, Accuracy: 9654/10000 (96.54%)\n\nEPOCH: 6\n</pre> <pre>Loss=0.0795092061161995 Batch_id=468 Accuracy=98.17: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.46it/s]\n</pre> <pre>\nTest set: Average loss: 0.0941, Accuracy: 9777/10000 (97.77%)\n\nEPOCH: 7\n</pre> <pre>Loss=0.14718694984912872 Batch_id=468 Accuracy=98.11: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.27it/s]\n</pre> <pre>\nTest set: Average loss: 0.1186, Accuracy: 9709/10000 (97.09%)\n\nEPOCH: 8\n</pre> <pre>Loss=0.11523934453725815 Batch_id=468 Accuracy=98.28: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.95it/s]\n</pre> <pre>\nTest set: Average loss: 0.0788, Accuracy: 9816/10000 (98.16%)\n\nEPOCH: 9\n</pre> <pre>Loss=0.06614526361227036 Batch_id=468 Accuracy=98.33: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.75it/s]\n</pre> <pre>\nTest set: Average loss: 0.0849, Accuracy: 9801/10000 (98.01%)\n\nEPOCH: 10\n</pre> <pre>Loss=0.05591153725981712 Batch_id=468 Accuracy=98.45: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.56it/s]\n</pre> <pre>\nTest set: Average loss: 0.0904, Accuracy: 9791/10000 (97.91%)\n\nEPOCH: 11\n</pre> <pre>Loss=0.0845068022608757 Batch_id=468 Accuracy=98.43: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.18it/s]\n</pre> <pre>\nTest set: Average loss: 0.0856, Accuracy: 9788/10000 (97.88%)\n\nEPOCH: 12\n</pre> <pre>Loss=0.12031087279319763 Batch_id=468 Accuracy=98.50: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.49it/s]\n</pre> <pre>\nTest set: Average loss: 0.0890, Accuracy: 9787/10000 (97.87%)\n\nEPOCH: 13\n</pre> <pre>Loss=0.1395779252052307 Batch_id=468 Accuracy=98.48: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.41it/s]\n</pre> <pre>\nTest set: Average loss: 0.0759, Accuracy: 9809/10000 (98.09%)\n\nEPOCH: 14\n</pre> <pre>Loss=0.03229466453194618 Batch_id=468 Accuracy=98.64: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.94it/s]\n</pre> <pre>\nTest set: Average loss: 0.0627, Accuracy: 9845/10000 (98.45%)\n\nEPOCH: 15\n</pre> <pre>Loss=0.047740787267684937 Batch_id=468 Accuracy=98.58: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.56it/s]\n</pre> <pre>\nTest set: Average loss: 0.0578, Accuracy: 9851/10000 (98.51%)\n\nEPOCH: 16\n</pre> <pre>Loss=0.11154284328222275 Batch_id=468 Accuracy=98.61: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.68it/s]\n</pre> <pre>\nTest set: Average loss: 0.0673, Accuracy: 9832/10000 (98.32%)\n\nEPOCH: 17\n</pre> <pre>Loss=0.019923433661460876 Batch_id=468 Accuracy=98.73: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.88it/s]\n</pre> <pre>\nTest set: Average loss: 0.0565, Accuracy: 9862/10000 (98.62%)\n\nEPOCH: 18\n</pre> <pre>Loss=0.0757511705160141 Batch_id=468 Accuracy=98.69: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:24&lt;00:00, 19.03it/s]\n</pre> <pre>\nTest set: Average loss: 0.0683, Accuracy: 9826/10000 (98.26%)\n\nEPOCH: 19\n</pre> <pre>Loss=0.06021649017930031 Batch_id=468 Accuracy=98.74: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.70it/s]\n</pre> <pre>\nTest set: Average loss: 0.0540, Accuracy: 9858/10000 (98.58%)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>t = [t_items.item() for t_items in train_losses]\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(2,2,figsize=(15,10))\naxs[0, 0].plot(t)\naxs[0, 0].set_title(\"Training Loss\")\naxs[1, 0].plot(train_acc[4000:])\naxs[1, 0].set_title(\"Training Accuracy\")\naxs[0, 1].plot(test_losses)\naxs[0, 1].set_title(\"Test Loss\")\naxs[1, 1].plot(test_acc)\naxs[1, 1].set_title(\"Test Accuracy\")\n</pre> t = [t_items.item() for t_items in train_losses] %matplotlib inline import matplotlib.pyplot as plt fig, axs = plt.subplots(2,2,figsize=(15,10)) axs[0, 0].plot(t) axs[0, 0].set_title(\"Training Loss\") axs[1, 0].plot(train_acc[4000:]) axs[1, 0].set_title(\"Training Accuracy\") axs[0, 1].plot(test_losses) axs[0, 1].set_title(\"Test Loss\") axs[1, 1].plot(test_acc) axs[1, 1].set_title(\"Test Accuracy\") Out[\u00a0]: <pre>Text(0.5, 1.0, 'Test Accuracy')</pre>"},{"location":"notebooks/deep_learning/Code_6/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"notebooks/deep_learning/Code_6/#data-transformations","title":"Data Transformations\u00b6","text":"<p>We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.</p>"},{"location":"notebooks/deep_learning/Code_6/#dataset-and-creating-traintest-split","title":"Dataset and Creating Train/Test Split\u00b6","text":""},{"location":"notebooks/deep_learning/Code_6/#dataloader-arguments-testtrain-dataloaders","title":"Dataloader Arguments &amp; Test/Train Dataloaders\u00b6","text":""},{"location":"notebooks/deep_learning/Code_6/#data-statistics","title":"Data Statistics\u00b6","text":"<p>It is important to know your data very well. Let's check some of the statistics around our data and how it actually looks like</p>"},{"location":"notebooks/deep_learning/Code_6/#more","title":"MORE\u00b6","text":"<p>It is important that we view as many images as possible. This is required to get some idea on image augmentation later on</p>"},{"location":"notebooks/deep_learning/Code_6/#the-model","title":"The model\u00b6","text":"<p>Let's start with the model we first saw</p>"},{"location":"notebooks/deep_learning/Code_6/#model-params","title":"Model Params\u00b6","text":"<p>Can't emphasize on how important viewing Model Summary is. Unfortunately, there is no in-built model visualizer, so we have to take external help</p>"},{"location":"notebooks/deep_learning/Code_6/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>Looking at logs can be boring, so we'll introduce tqdm progressbar to get cooler logs.</p> <p>Let's write train and test functions</p>"},{"location":"notebooks/deep_learning/Code_6/#lets-train-and-test-our-model","title":"Let's Train and test our model\u00b6","text":""},{"location":"notebooks/deep_learning/Code_7/","title":"Code 7","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n</pre> from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Train Phase transformations\ntrain_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\n\n# Test Phase transformations\ntest_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n</pre> # Train Phase transformations train_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ])  # Test Phase transformations test_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,))                                        ])  In\u00a0[\u00a0]: Copied! <pre>train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n</pre> train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 103171424.78it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 90942713.08it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 25901147.88it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 18265128.25it/s]</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n</pre> SEED = 1  # CUDA? cuda = torch.cuda.is_available() print(\"CUDA Available?\", cuda)  # For reproducibility torch.manual_seed(SEED)  if cuda:     torch.cuda.manual_seed(SEED)  # dataloader arguments - something you'll fetch these from cmdprmt dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)  # train dataloader train_loader = torch.utils.data.DataLoader(train, **dataloader_args)  # test dataloader test_loader = torch.utils.data.DataLoader(test, **dataloader_args) <pre>CUDA Available? True\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</pre> In\u00a0[\u00a0]: Copied! <pre># We'd need to convert it into Numpy! Remember above we have converted it into tensors already\ntrain_data = train.train_data\ntrain_data = train.transform(train_data.numpy())\n\nprint('[Train]')\nprint(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\nprint(' - Tensor Shape:', train.train_data.size())\nprint(' - min:', torch.min(train_data))\nprint(' - max:', torch.max(train_data))\nprint(' - mean:', torch.mean(train_data))\nprint(' - std:', torch.std(train_data))\nprint(' - var:', torch.var(train_data))\n\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(labels.shape)\n\n# Let's visualize some of the images\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n</pre> # We'd need to convert it into Numpy! Remember above we have converted it into tensors already train_data = train.train_data train_data = train.transform(train_data.numpy())  print('[Train]') print(' - Numpy Shape:', train.train_data.cpu().numpy().shape) print(' - Tensor Shape:', train.train_data.size()) print(' - min:', torch.min(train_data)) print(' - max:', torch.max(train_data)) print(' - mean:', torch.mean(train_data)) print(' - std:', torch.std(train_data)) print(' - var:', torch.var(train_data))  dataiter = iter(train_loader) images, labels = dataiter.next()  print(images.shape) print(labels.shape)  # Let's visualize some of the images %matplotlib inline import matplotlib.pyplot as plt  plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')  <pre>/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n</pre> <pre>[Train]\n - Numpy Shape: (60000, 28, 28)\n - Tensor Shape: torch.Size([60000, 28, 28])\n - min: tensor(-0.4242)\n - max: tensor(2.8215)\n - mean: tensor(-0.0001)\n - std: tensor(1.0000)\n - var: tensor(1.0001)\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-5-f75c6547409a&gt; in &lt;cell line: 15&gt;()\n     13 \n     14 dataiter = iter(train_loader)\n---&gt; 15 images, labels = dataiter.next()\n     16 \n     17 print(images.shape)\n\nAttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'</pre> In\u00a0[\u00a0]: Copied! <pre>figure = plt.figure()\nnum_of_images = 60\nfor index in range(1, num_of_images + 1):\n    plt.subplot(6, 10, index)\n    plt.axis('off')\n    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n</pre> figure = plt.figure() num_of_images = 60 for index in range(1, num_of_images + 1):     plt.subplot(6, 10, index)     plt.axis('off')     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r') In\u00a0[\u00a0]: Copied! <pre>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Input Block\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 26\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 24\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # output_size = 22\n\n        # TRANSITION BLOCK 1\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 11\n\n        # CONVOLUTION BLOCK 2\n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(10),\n            nn.ReLU()\n        ) # output_size = 9\n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # output_size = 7\n\n        # OUTPUT BLOCK\n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU()\n        ) # output_size = 5\n        self.convblock8 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n        ) # output_size = 5\n        self.gap = nn.Sequential(\n            nn.AvgPool2d(kernel_size=5)\n        ) # output_size = 1\n\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        x = self.dropout(x)\n        x = self.pool1(x)\n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        x = self.convblock6(x)\n        x = self.dropout(x)\n        x = self.convblock7(x)\n        x = self.convblock8(x)\n        x = self.gap(x)\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n</pre> class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         # Input Block         self.convblock1 = nn.Sequential(             nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 26          # CONVOLUTION BLOCK 1         self.convblock2 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 24         self.convblock3 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(20),             nn.ReLU()         ) # output_size = 22          # TRANSITION BLOCK 1         self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11         self.convblock4 = nn.Sequential(             nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 11          # CONVOLUTION BLOCK 2         self.convblock5 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(10),             nn.ReLU()         ) # output_size = 9         self.convblock6 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(20),             nn.ReLU()         ) # output_size = 7          # OUTPUT BLOCK         self.convblock7 = nn.Sequential(             nn.Conv2d(in_channels=20, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),             nn.BatchNorm2d(32),             nn.ReLU()         ) # output_size = 5         self.convblock8 = nn.Sequential(             nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),         ) # output_size = 5         self.gap = nn.Sequential(             nn.AvgPool2d(kernel_size=5)         ) # output_size = 1          self.dropout = nn.Dropout(0.25)      def forward(self, x):         x = self.convblock1(x)         x = self.convblock2(x)         x = self.convblock3(x)         x = self.dropout(x)         x = self.pool1(x)         x = self.convblock4(x)         x = self.convblock5(x)         x = self.convblock6(x)         x = self.dropout(x)         x = self.convblock7(x)         x = self.convblock8(x)         x = self.gap(x)         x = x.view(-1, 10)         return F.log_softmax(x, dim=-1) In\u00a0[\u00a0]: Copied! <pre>!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))\n</pre> !pip install torchsummary from torchsummary import summary use_cuda = torch.cuda.is_available() device = torch.device(\"cuda\" if use_cuda else \"cpu\") print(device) model = Net().to(device) summary(model, input_size=(1, 28, 28)) <pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 10, 26, 26]              90\n       BatchNorm2d-2           [-1, 10, 26, 26]              20\n              ReLU-3           [-1, 10, 26, 26]               0\n            Conv2d-4           [-1, 10, 24, 24]             900\n       BatchNorm2d-5           [-1, 10, 24, 24]              20\n              ReLU-6           [-1, 10, 24, 24]               0\n            Conv2d-7           [-1, 20, 22, 22]           1,800\n       BatchNorm2d-8           [-1, 20, 22, 22]              40\n              ReLU-9           [-1, 20, 22, 22]               0\n          Dropout-10           [-1, 20, 22, 22]               0\n        MaxPool2d-11           [-1, 20, 11, 11]               0\n           Conv2d-12           [-1, 10, 11, 11]             200\n      BatchNorm2d-13           [-1, 10, 11, 11]              20\n             ReLU-14           [-1, 10, 11, 11]               0\n           Conv2d-15             [-1, 10, 9, 9]             900\n      BatchNorm2d-16             [-1, 10, 9, 9]              20\n             ReLU-17             [-1, 10, 9, 9]               0\n           Conv2d-18             [-1, 20, 7, 7]           1,800\n      BatchNorm2d-19             [-1, 20, 7, 7]              40\n             ReLU-20             [-1, 20, 7, 7]               0\n          Dropout-21             [-1, 20, 7, 7]               0\n           Conv2d-22             [-1, 32, 5, 5]           5,760\n      BatchNorm2d-23             [-1, 32, 5, 5]              64\n             ReLU-24             [-1, 32, 5, 5]               0\n           Conv2d-25             [-1, 10, 5, 5]             320\n        AvgPool2d-26             [-1, 10, 1, 1]               0\n================================================================\nTotal params: 11,994\nTrainable params: 11,994\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.70\nParams size (MB): 0.05\nEstimated Total Size (MB): 0.75\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))\n</pre> from tqdm import tqdm  train_losses = [] test_losses = [] train_acc = [] test_acc = []  def train(model, device, train_loader, optimizer, epoch):   model.train()   pbar = tqdm(train_loader)   correct = 0   processed = 0   for batch_idx, (data, target) in enumerate(pbar):     # get samples     data, target = data.to(device), target.to(device)      # Init     optimizer.zero_grad()     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.      # Predict     y_pred = model(data)      # Calculate loss     loss = F.nll_loss(y_pred, target)     train_losses.append(loss)      # Backpropagation     loss.backward()     optimizer.step()      # Update pbar-tqdm      pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability     correct += pred.eq(target.view_as(pred)).sum().item()     processed += len(data)      pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')     train_acc.append(100*correct/processed)  def test(model, device, test_loader):     model.eval()     test_loss = 0     correct = 0     with torch.no_grad():         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()      test_loss /= len(test_loader.dataset)     test_losses.append(test_loss)      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(         test_loss, correct, len(test_loader.dataset),         100. * correct / len(test_loader.dataset)))      test_acc.append(100. * correct / len(test_loader.dataset)) In\u00a0[\u00a0]: Copied! <pre>model =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)\n</pre> model =  Net().to(device) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) EPOCHS = 20 for epoch in range(EPOCHS):     print(\"EPOCH:\", epoch)     train(model, device, train_loader, optimizer, epoch)     test(model, device, test_loader) <pre>EPOCH: 0\n</pre> <pre>Loss=0.15517620742321014 Batch_id=468 Accuracy=82.38: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 23.98it/s]\n</pre> <pre>\nTest set: Average loss: 0.4737, Accuracy: 8500/10000 (85.00%)\n\nEPOCH: 1\n</pre> <pre>Loss=0.05157714709639549 Batch_id=468 Accuracy=97.52: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.26it/s]\n</pre> <pre>\nTest set: Average loss: 0.1424, Accuracy: 9604/10000 (96.04%)\n\nEPOCH: 2\n</pre> <pre>Loss=0.038243018090724945 Batch_id=468 Accuracy=98.23: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 26.06it/s]\n</pre> <pre>\nTest set: Average loss: 0.1993, Accuracy: 9387/10000 (93.87%)\n\nEPOCH: 3\n</pre> <pre>Loss=0.09625473618507385 Batch_id=468 Accuracy=98.48: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.55it/s]\n</pre> <pre>\nTest set: Average loss: 0.1380, Accuracy: 9575/10000 (95.75%)\n\nEPOCH: 4\n</pre> <pre>Loss=0.008819338865578175 Batch_id=468 Accuracy=98.64: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 23.82it/s]\n</pre> <pre>\nTest set: Average loss: 0.0567, Accuracy: 9836/10000 (98.36%)\n\nEPOCH: 5\n</pre> <pre>Loss=0.03814196586608887 Batch_id=468 Accuracy=98.77: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.97it/s]\n</pre> <pre>\nTest set: Average loss: 0.0854, Accuracy: 9726/10000 (97.26%)\n\nEPOCH: 6\n</pre> <pre>Loss=0.019087085500359535 Batch_id=468 Accuracy=98.86: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.25it/s]\n</pre> <pre>\nTest set: Average loss: 0.0494, Accuracy: 9842/10000 (98.42%)\n\nEPOCH: 7\n</pre> <pre>Loss=0.05842439457774162 Batch_id=468 Accuracy=98.96: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.18it/s]\n</pre> <pre>\nTest set: Average loss: 0.0838, Accuracy: 9739/10000 (97.39%)\n\nEPOCH: 8\n</pre> <pre>Loss=0.031596582382917404 Batch_id=468 Accuracy=98.98: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.29it/s]\n</pre> <pre>\nTest set: Average loss: 0.0505, Accuracy: 9850/10000 (98.50%)\n\nEPOCH: 9\n</pre> <pre>Loss=0.012726793996989727 Batch_id=468 Accuracy=99.03: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.67it/s]\n</pre> <pre>\nTest set: Average loss: 0.0680, Accuracy: 9783/10000 (97.83%)\n\nEPOCH: 10\n</pre> <pre>Loss=0.022121017798781395 Batch_id=468 Accuracy=99.05: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.74it/s]\n</pre> <pre>\nTest set: Average loss: 0.0779, Accuracy: 9751/10000 (97.51%)\n\nEPOCH: 11\n</pre> <pre>Loss=0.008670867420732975 Batch_id=468 Accuracy=99.12: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.06it/s]\n</pre> <pre>\nTest set: Average loss: 0.0607, Accuracy: 9814/10000 (98.14%)\n\nEPOCH: 12\n</pre> <pre>Loss=0.07616553455591202 Batch_id=468 Accuracy=99.17: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.41it/s]\n</pre> <pre>\nTest set: Average loss: 0.0430, Accuracy: 9868/10000 (98.68%)\n\nEPOCH: 13\n</pre> <pre>Loss=0.007742537651211023 Batch_id=468 Accuracy=99.20: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.40it/s]\n</pre> <pre>\nTest set: Average loss: 0.0726, Accuracy: 9777/10000 (97.77%)\n\nEPOCH: 14\n</pre> <pre>Loss=0.004553579725325108 Batch_id=468 Accuracy=99.21: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.67it/s]\n</pre> <pre>\nTest set: Average loss: 0.0434, Accuracy: 9870/10000 (98.70%)\n\nEPOCH: 15\n</pre> <pre>Loss=0.011823096312582493 Batch_id=468 Accuracy=99.27: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.81it/s]\n</pre> <pre>\nTest set: Average loss: 0.0466, Accuracy: 9850/10000 (98.50%)\n\nEPOCH: 16\n</pre> <pre>Loss=0.030195215716958046 Batch_id=468 Accuracy=99.27: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.13it/s]\n</pre> <pre>\nTest set: Average loss: 0.0339, Accuracy: 9894/10000 (98.94%)\n\nEPOCH: 17\n</pre> <pre>Loss=0.04987773671746254 Batch_id=468 Accuracy=99.27: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.94it/s]\n</pre> <pre>\nTest set: Average loss: 0.0478, Accuracy: 9849/10000 (98.49%)\n\nEPOCH: 18\n</pre> <pre>Loss=0.005478643346577883 Batch_id=468 Accuracy=99.29: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.48it/s]\n</pre> <pre>\nTest set: Average loss: 0.0366, Accuracy: 9881/10000 (98.81%)\n\nEPOCH: 19\n</pre> <pre>Loss=0.003676382126286626 Batch_id=468 Accuracy=99.34: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.76it/s]\n</pre> <pre>\nTest set: Average loss: 0.0408, Accuracy: 9883/10000 (98.83%)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(2,2,figsize=(15,10))\naxs[0, 0].plot(train_losses)\naxs[0, 0].set_title(\"Training Loss\")\naxs[1, 0].plot(train_acc[4000:])\naxs[1, 0].set_title(\"Training Accuracy\")\naxs[0, 1].plot(test_losses)\naxs[0, 1].set_title(\"Test Loss\")\naxs[1, 1].plot(test_acc)\naxs[1, 1].set_title(\"Test Accuracy\")\n</pre> fig, axs = plt.subplots(2,2,figsize=(15,10)) axs[0, 0].plot(train_losses) axs[0, 0].set_title(\"Training Loss\") axs[1, 0].plot(train_acc[4000:]) axs[1, 0].set_title(\"Training Accuracy\") axs[0, 1].plot(test_losses) axs[0, 1].set_title(\"Test Loss\") axs[1, 1].plot(test_acc) axs[1, 1].set_title(\"Test Accuracy\") Out[\u00a0]: <pre>Text(0.5, 1.0, 'Test Accuracy')</pre>"},{"location":"notebooks/deep_learning/Code_7/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"notebooks/deep_learning/Code_7/#data-transformations","title":"Data Transformations\u00b6","text":"<p>We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.</p>"},{"location":"notebooks/deep_learning/Code_7/#dataset-and-creating-traintest-split","title":"Dataset and Creating Train/Test Split\u00b6","text":""},{"location":"notebooks/deep_learning/Code_7/#dataloader-arguments-testtrain-dataloaders","title":"Dataloader Arguments &amp; Test/Train Dataloaders\u00b6","text":""},{"location":"notebooks/deep_learning/Code_7/#data-statistics","title":"Data Statistics\u00b6","text":"<p>It is important to know your data very well. Let's check some of the statistics around our data and how it actually looks like</p>"},{"location":"notebooks/deep_learning/Code_7/#more","title":"MORE\u00b6","text":"<p>It is important that we view as many images as possible. This is required to get some idea on image augmentation later on</p>"},{"location":"notebooks/deep_learning/Code_7/#the-model","title":"The model\u00b6","text":"<p>Let's start with the model we first saw</p>"},{"location":"notebooks/deep_learning/Code_7/#model-params","title":"Model Params\u00b6","text":"<p>Can't emphasize on how important viewing Model Summary is. Unfortunately, there is no in-built model visualizer, so we have to take external help</p>"},{"location":"notebooks/deep_learning/Code_7/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>Looking at logs can be boring, so we'll introduce tqdm progressbar to get cooler logs.</p> <p>Let's write train and test functions</p>"},{"location":"notebooks/deep_learning/Code_7/#lets-train-and-test-our-model","title":"Let's Train and test our model\u00b6","text":""},{"location":"notebooks/deep_learning/Code_8/","title":"Code 8","text":"In\u00a0[\u00a0]: Copied! <pre>! nvidia-smi\n</pre> ! nvidia-smi <pre>Sat Jan 14 06:37:09 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   59C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</pre> In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n</pre> from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Train Phase transformations\ntrain_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\n\n# Test Phase transformations\ntest_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n</pre> # Train Phase transformations train_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ])  # Test Phase transformations test_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,))                                        ])  In\u00a0[\u00a0]: Copied! <pre>train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n</pre> train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 301542035.61it/s]</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n</pre> <pre>\n</pre> <pre>\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 21822319.19it/s]</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 143029210.11it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 7697183.34it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n</pre> SEED = 1  # CUDA? cuda = torch.cuda.is_available() print(\"CUDA Available?\", cuda)  # For reproducibility torch.manual_seed(SEED)  if cuda:     torch.cuda.manual_seed(SEED)  # dataloader arguments - something you'll fetch these from cmdprmt dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)  # train dataloader train_loader = torch.utils.data.DataLoader(train, **dataloader_args)  # test dataloader test_loader = torch.utils.data.DataLoader(test, **dataloader_args) <pre>CUDA Available? True\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</pre> In\u00a0[\u00a0]: Copied! <pre># We'd need to convert it into Numpy! Remember above we have converted it into tensors already\ntrain_data = train.train_data\ntrain_data = train.transform(train_data.numpy())\n\nprint('[Train]')\nprint(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\nprint(' - Tensor Shape:', train.train_data.size())\nprint(' - min:', torch.min(train_data))\nprint(' - max:', torch.max(train_data))\nprint(' - mean:', torch.mean(train_data))\nprint(' - std:', torch.std(train_data))\nprint(' - var:', torch.var(train_data))\n\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\nprint(labels.shape)\n\n# Let's visualize some of the images\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n</pre> # We'd need to convert it into Numpy! Remember above we have converted it into tensors already train_data = train.train_data train_data = train.transform(train_data.numpy())  print('[Train]') print(' - Numpy Shape:', train.train_data.cpu().numpy().shape) print(' - Tensor Shape:', train.train_data.size()) print(' - min:', torch.min(train_data)) print(' - max:', torch.max(train_data)) print(' - mean:', torch.mean(train_data)) print(' - std:', torch.std(train_data)) print(' - var:', torch.var(train_data))  dataiter = iter(train_loader) images, labels = dataiter.next()  print(images.shape) print(labels.shape)  # Let's visualize some of the images %matplotlib inline import matplotlib.pyplot as plt  plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')  <pre>/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n</pre> <pre>[Train]\n - Numpy Shape: (60000, 28, 28)\n - Tensor Shape: torch.Size([60000, 28, 28])\n - min: tensor(-0.4242)\n - max: tensor(2.8215)\n - mean: tensor(-0.0001)\n - std: tensor(1.0000)\n - var: tensor(1.0001)\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-5-f75c6547409a&gt; in &lt;cell line: 15&gt;()\n     13 \n     14 dataiter = iter(train_loader)\n---&gt; 15 images, labels = dataiter.next()\n     16 \n     17 print(images.shape)\n\nAttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'</pre> In\u00a0[\u00a0]: Copied! <pre>figure = plt.figure()\nnum_of_images = 60\nfor index in range(1, num_of_images + 1):\n    plt.subplot(6, 10, index)\n    plt.axis('off')\n    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n</pre> figure = plt.figure() num_of_images = 60 for index in range(1, num_of_images + 1):     plt.subplot(6, 10, index)     plt.axis('off')     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r') In\u00a0[\u00a0]: Copied! <pre>dropout_value = 0.1\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Input Block\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 26\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Dropout(dropout_value)\n        ) # output_size = 24\n\n        # TRANSITION BLOCK 1\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n        ) # output_size = 24\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12\n\n        # CONVOLUTION BLOCK 2\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 10\n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 8\n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 6\n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 6\n\n        # OUTPUT BLOCK\n        self.gap = nn.Sequential(\n            nn.AvgPool2d(kernel_size=6)\n        ) # output_size = 1\n\n        self.convblock8 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            # nn.BatchNorm2d(10),\n            # nn.ReLU(),\n            # nn.Dropout(dropout_value)\n        )\n\n\n        self.dropout = nn.Dropout(dropout_value)\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        x = self.pool1(x)\n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        x = self.convblock6(x)\n        x = self.convblock7(x)\n        x = self.gap(x)\n        x = self.convblock8(x)\n\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n</pre> dropout_value = 0.1 class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         # Input Block         self.convblock1 = nn.Sequential(             nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 26          # CONVOLUTION BLOCK 1         self.convblock2 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(32),             nn.Dropout(dropout_value)         ) # output_size = 24          # TRANSITION BLOCK 1         self.convblock3 = nn.Sequential(             nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),         ) # output_size = 24         self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12          # CONVOLUTION BLOCK 2         self.convblock4 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 10         self.convblock5 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 8         self.convblock6 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 6         self.convblock7 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 6          # OUTPUT BLOCK         self.gap = nn.Sequential(             nn.AvgPool2d(kernel_size=6)         ) # output_size = 1          self.convblock8 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             # nn.BatchNorm2d(10),             # nn.ReLU(),             # nn.Dropout(dropout_value)         )           self.dropout = nn.Dropout(dropout_value)      def forward(self, x):         x = self.convblock1(x)         x = self.convblock2(x)         x = self.convblock3(x)         x = self.pool1(x)         x = self.convblock4(x)         x = self.convblock5(x)         x = self.convblock6(x)         x = self.convblock7(x)         x = self.gap(x)         x = self.convblock8(x)          x = x.view(-1, 10)         return F.log_softmax(x, dim=-1) In\u00a0[\u00a0]: Copied! <pre>!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))\n</pre> !pip install torchsummary from torchsummary import summary use_cuda = torch.cuda.is_available() device = torch.device(\"cuda\" if use_cuda else \"cpu\") print(device) model = Net().to(device) summary(model, input_size=(1, 28, 28)) <pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 26, 26]             144\n              ReLU-2           [-1, 16, 26, 26]               0\n       BatchNorm2d-3           [-1, 16, 26, 26]              32\n           Dropout-4           [-1, 16, 26, 26]               0\n            Conv2d-5           [-1, 32, 24, 24]           4,608\n              ReLU-6           [-1, 32, 24, 24]               0\n       BatchNorm2d-7           [-1, 32, 24, 24]              64\n           Dropout-8           [-1, 32, 24, 24]               0\n            Conv2d-9           [-1, 10, 24, 24]             320\n        MaxPool2d-10           [-1, 10, 12, 12]               0\n           Conv2d-11           [-1, 16, 10, 10]           1,440\n             ReLU-12           [-1, 16, 10, 10]               0\n      BatchNorm2d-13           [-1, 16, 10, 10]              32\n          Dropout-14           [-1, 16, 10, 10]               0\n           Conv2d-15             [-1, 16, 8, 8]           2,304\n             ReLU-16             [-1, 16, 8, 8]               0\n      BatchNorm2d-17             [-1, 16, 8, 8]              32\n          Dropout-18             [-1, 16, 8, 8]               0\n           Conv2d-19             [-1, 16, 6, 6]           2,304\n             ReLU-20             [-1, 16, 6, 6]               0\n      BatchNorm2d-21             [-1, 16, 6, 6]              32\n          Dropout-22             [-1, 16, 6, 6]               0\n           Conv2d-23             [-1, 16, 6, 6]           2,304\n             ReLU-24             [-1, 16, 6, 6]               0\n      BatchNorm2d-25             [-1, 16, 6, 6]              32\n          Dropout-26             [-1, 16, 6, 6]               0\n        AvgPool2d-27             [-1, 16, 1, 1]               0\n           Conv2d-28             [-1, 10, 1, 1]             160\n================================================================\nTotal params: 13,808\nTrainable params: 13,808\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 1.06\nParams size (MB): 0.05\nEstimated Total Size (MB): 1.12\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))\n</pre> from tqdm import tqdm  train_losses = [] test_losses = [] train_acc = [] test_acc = []  def train(model, device, train_loader, optimizer, epoch):   model.train()   pbar = tqdm(train_loader)   correct = 0   processed = 0   for batch_idx, (data, target) in enumerate(pbar):     # get samples     data, target = data.to(device), target.to(device)      # Init     optimizer.zero_grad()     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.      # Predict     y_pred = model(data)      # Calculate loss     loss = F.nll_loss(y_pred, target)     train_losses.append(loss)      # Backpropagation     loss.backward()     optimizer.step()      # Update pbar-tqdm      pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability     correct += pred.eq(target.view_as(pred)).sum().item()     processed += len(data)      pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')     train_acc.append(100*correct/processed)  def test(model, device, test_loader):     model.eval()     test_loss = 0     correct = 0     with torch.no_grad():         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()      test_loss /= len(test_loader.dataset)     test_losses.append(test_loss)      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(         test_loss, correct, len(test_loader.dataset),         100. * correct / len(test_loader.dataset)))      test_acc.append(100. * correct / len(test_loader.dataset)) In\u00a0[\u00a0]: Copied! <pre>model =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)\n</pre> model =  Net().to(device) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  EPOCHS = 20 for epoch in range(EPOCHS):     print(\"EPOCH:\", epoch)     train(model, device, train_loader, optimizer, epoch)     test(model, device, test_loader) <pre>EPOCH: 0\n</pre> <pre>Loss=0.0424773283302784 Batch_id=468 Accuracy=87.44: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.84it/s]\n</pre> <pre>\nTest set: Average loss: 0.0650, Accuracy: 9805/10000 (98.05%)\n\nEPOCH: 1\n</pre> <pre>Loss=0.037217721343040466 Batch_id=468 Accuracy=97.76: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 27.26it/s]\n</pre> <pre>\nTest set: Average loss: 0.0736, Accuracy: 9768/10000 (97.68%)\n\nEPOCH: 2\n</pre> <pre>Loss=0.014409967698156834 Batch_id=468 Accuracy=98.30: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.20it/s]\n</pre> <pre>\nTest set: Average loss: 0.0346, Accuracy: 9899/10000 (98.99%)\n\nEPOCH: 3\n</pre> <pre>Loss=0.05342693254351616 Batch_id=468 Accuracy=98.60: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.21it/s]\n</pre> <pre>\nTest set: Average loss: 0.0287, Accuracy: 9904/10000 (99.04%)\n\nEPOCH: 4\n</pre> <pre>Loss=0.04268275201320648 Batch_id=468 Accuracy=98.71: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.88it/s]\n</pre> <pre>\nTest set: Average loss: 0.0278, Accuracy: 9909/10000 (99.09%)\n\nEPOCH: 5\n</pre> <pre>Loss=0.05142010375857353 Batch_id=468 Accuracy=98.86: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.73it/s]\n</pre> <pre>\nTest set: Average loss: 0.0324, Accuracy: 9889/10000 (98.89%)\n\nEPOCH: 6\n</pre> <pre>Loss=0.08974120020866394 Batch_id=468 Accuracy=98.90: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.37it/s]\n</pre> <pre>\nTest set: Average loss: 0.0270, Accuracy: 9902/10000 (99.02%)\n\nEPOCH: 7\n</pre> <pre>Loss=0.08733672648668289 Batch_id=468 Accuracy=98.91: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.81it/s]\n</pre> <pre>\nTest set: Average loss: 0.0242, Accuracy: 9926/10000 (99.26%)\n\nEPOCH: 8\n</pre> <pre>Loss=0.0760163739323616 Batch_id=468 Accuracy=99.05: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.32it/s]\n</pre> <pre>\nTest set: Average loss: 0.0251, Accuracy: 9915/10000 (99.15%)\n\nEPOCH: 9\n</pre> <pre>Loss=0.009890333749353886 Batch_id=468 Accuracy=99.05: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.68it/s]\n</pre> <pre>\nTest set: Average loss: 0.0196, Accuracy: 9935/10000 (99.35%)\n\nEPOCH: 10\n</pre> <pre>Loss=0.009909511543810368 Batch_id=468 Accuracy=99.06: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.37it/s]\n</pre> <pre>\nTest set: Average loss: 0.0210, Accuracy: 9933/10000 (99.33%)\n\nEPOCH: 11\n</pre> <pre>Loss=0.019411901012063026 Batch_id=468 Accuracy=99.12: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.76it/s]\n</pre> <pre>\nTest set: Average loss: 0.0264, Accuracy: 9912/10000 (99.12%)\n\nEPOCH: 12\n</pre> <pre>Loss=0.05437644198536873 Batch_id=468 Accuracy=99.17: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:19&lt;00:00, 24.67it/s]\n</pre> <pre>\nTest set: Average loss: 0.0218, Accuracy: 9925/10000 (99.25%)\n\nEPOCH: 13\n</pre> <pre>Loss=0.01864788867533207 Batch_id=468 Accuracy=99.25: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.72it/s]\n</pre> <pre>\nTest set: Average loss: 0.0188, Accuracy: 9940/10000 (99.40%)\n\nEPOCH: 14\n</pre> <pre>Loss=0.031508877873420715 Batch_id=468 Accuracy=99.19: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 25.06it/s]\n</pre> <pre>\nTest set: Average loss: 0.0244, Accuracy: 9923/10000 (99.23%)\n\nEPOCH: 15\n</pre> <pre>Loss=0.04409903660416603 Batch_id=468 Accuracy=99.25: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.69it/s]\n</pre> <pre>\nTest set: Average loss: 0.0202, Accuracy: 9936/10000 (99.36%)\n\nEPOCH: 16\n</pre> <pre>Loss=0.005426719319075346 Batch_id=468 Accuracy=99.26: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.84it/s]\n</pre> <pre>\nTest set: Average loss: 0.0203, Accuracy: 9933/10000 (99.33%)\n\nEPOCH: 17\n</pre> <pre>Loss=0.09359464049339294 Batch_id=468 Accuracy=99.29: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.68it/s]\n</pre> <pre>\nTest set: Average loss: 0.0195, Accuracy: 9935/10000 (99.35%)\n\nEPOCH: 18\n</pre> <pre>Loss=0.005491908174008131 Batch_id=468 Accuracy=99.31: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:18&lt;00:00, 24.86it/s]\n</pre> <pre>\nTest set: Average loss: 0.0210, Accuracy: 9935/10000 (99.35%)\n\nEPOCH: 19\n</pre> <pre>Loss=0.005351042374968529 Batch_id=468 Accuracy=99.30: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:17&lt;00:00, 26.81it/s]\n</pre> <pre>\nTest set: Average loss: 0.0270, Accuracy: 9914/10000 (99.14%)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(2,2,figsize=(15,10))\naxs[0, 0].plot(train_losses)\naxs[0, 0].set_title(\"Training Loss\")\naxs[1, 0].plot(train_acc[4000:])\naxs[1, 0].set_title(\"Training Accuracy\")\naxs[0, 1].plot(test_losses)\naxs[0, 1].set_title(\"Test Loss\")\naxs[1, 1].plot(test_acc)\naxs[1, 1].set_title(\"Test Accuracy\")\n</pre> fig, axs = plt.subplots(2,2,figsize=(15,10)) axs[0, 0].plot(train_losses) axs[0, 0].set_title(\"Training Loss\") axs[1, 0].plot(train_acc[4000:]) axs[1, 0].set_title(\"Training Accuracy\") axs[0, 1].plot(test_losses) axs[0, 1].set_title(\"Test Loss\") axs[1, 1].plot(test_acc) axs[1, 1].set_title(\"Test Accuracy\") Out[\u00a0]: <pre>Text(0.5, 1.0, 'Test Accuracy')</pre>"},{"location":"notebooks/deep_learning/Code_8/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"notebooks/deep_learning/Code_8/#data-transformations","title":"Data Transformations\u00b6","text":"<p>We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.</p>"},{"location":"notebooks/deep_learning/Code_8/#dataset-and-creating-traintest-split","title":"Dataset and Creating Train/Test Split\u00b6","text":""},{"location":"notebooks/deep_learning/Code_8/#dataloader-arguments-testtrain-dataloaders","title":"Dataloader Arguments &amp; Test/Train Dataloaders\u00b6","text":""},{"location":"notebooks/deep_learning/Code_8/#data-statistics","title":"Data Statistics\u00b6","text":"<p>It is important to know your data very well. Let's check some of the statistics around our data and how it actually looks like</p>"},{"location":"notebooks/deep_learning/Code_8/#more","title":"MORE\u00b6","text":"<p>It is important that we view as many images as possible. This is required to get some idea on image augmentation later on</p>"},{"location":"notebooks/deep_learning/Code_8/#the-model","title":"The model\u00b6","text":"<p>Let's start with the model we first saw</p>"},{"location":"notebooks/deep_learning/Code_8/#model-params","title":"Model Params\u00b6","text":"<p>Can't emphasize on how important viewing Model Summary is. Unfortunately, there is no in-built model visualizer, so we have to take external help</p>"},{"location":"notebooks/deep_learning/Code_8/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>Looking at logs can be boring, so we'll introduce tqdm progressbar to get cooler logs.</p> <p>Let's write train and test functions</p>"},{"location":"notebooks/deep_learning/Code_8/#lets-train-and-test-our-model","title":"Let's Train and test our model\u00b6","text":""},{"location":"notebooks/deep_learning/Code_9/","title":"Code 9","text":"In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n</pre> from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Train Phase transformations\ntrain_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\n\n# Test Phase transformations\ntest_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n</pre> # Train Phase transformations train_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.RandomRotation((-7.0, 7.0), fill=(1,)),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ])  # Test Phase transformations test_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,))                                        ])  In\u00a0[\u00a0]: Copied! <pre>train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n</pre> train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 321873151.59it/s]</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n</pre> <pre>\n</pre> <pre>\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 121135693.82it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 130299214.29it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 18230171.07it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n</pre> SEED = 1  # CUDA? cuda = torch.cuda.is_available() print(\"CUDA Available?\", cuda)  # For reproducibility torch.manual_seed(SEED)  if cuda:     torch.cuda.manual_seed(SEED)  # dataloader arguments - something you'll fetch these from cmdprmt dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)  # train dataloader train_loader = torch.utils.data.DataLoader(train, **dataloader_args)  # test dataloader test_loader = torch.utils.data.DataLoader(test, **dataloader_args) <pre>CUDA Available? True\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</pre> In\u00a0[\u00a0]: Copied! <pre>import torch.nn.functional as F\ndropout_value = 0.1\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Input Block\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 26\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Dropout(dropout_value)\n        ) # output_size = 24\n\n        # TRANSITION BLOCK 1\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n        ) # output_size = 24\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12\n\n        # CONVOLUTION BLOCK 2\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 10\n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 8\n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 6\n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 6\n\n        # OUTPUT BLOCK\n        self.gap = nn.Sequential(\n            nn.AvgPool2d(kernel_size=6)\n        ) # output_size = 1\n\n        self.convblock8 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            # nn.BatchNorm2d(10),\n            # nn.ReLU(),\n            # nn.Dropout(dropout_value)\n        )\n\n\n        self.dropout = nn.Dropout(dropout_value)\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        x = self.pool1(x)\n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        x = self.convblock6(x)\n        x = self.convblock7(x)\n        x = self.gap(x)\n        x = self.convblock8(x)\n\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n</pre> import torch.nn.functional as F dropout_value = 0.1 class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         # Input Block         self.convblock1 = nn.Sequential(             nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 26          # CONVOLUTION BLOCK 1         self.convblock2 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(32),             nn.Dropout(dropout_value)         ) # output_size = 24          # TRANSITION BLOCK 1         self.convblock3 = nn.Sequential(             nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),         ) # output_size = 24         self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12          # CONVOLUTION BLOCK 2         self.convblock4 = nn.Sequential(             nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 10         self.convblock5 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 8         self.convblock6 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 6         self.convblock7 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),             nn.ReLU(),             nn.BatchNorm2d(16),             nn.Dropout(dropout_value)         ) # output_size = 6          # OUTPUT BLOCK         self.gap = nn.Sequential(             nn.AvgPool2d(kernel_size=6)         ) # output_size = 1          self.convblock8 = nn.Sequential(             nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),             # nn.BatchNorm2d(10),             # nn.ReLU(),             # nn.Dropout(dropout_value)         )           self.dropout = nn.Dropout(dropout_value)      def forward(self, x):         x = self.convblock1(x)         x = self.convblock2(x)         x = self.convblock3(x)         x = self.pool1(x)         x = self.convblock4(x)         x = self.convblock5(x)         x = self.convblock6(x)         x = self.convblock7(x)         x = self.gap(x)         x = self.convblock8(x)          x = x.view(-1, 10)         return F.log_softmax(x, dim=-1) In\u00a0[\u00a0]: Copied! <pre>!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))\n</pre> !pip install torchsummary from torchsummary import summary use_cuda = torch.cuda.is_available() device = torch.device(\"cuda\" if use_cuda else \"cpu\") print(device) model = Net().to(device) summary(model, input_size=(1, 28, 28)) <pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 26, 26]             144\n              ReLU-2           [-1, 16, 26, 26]               0\n       BatchNorm2d-3           [-1, 16, 26, 26]              32\n           Dropout-4           [-1, 16, 26, 26]               0\n            Conv2d-5           [-1, 32, 24, 24]           4,608\n              ReLU-6           [-1, 32, 24, 24]               0\n       BatchNorm2d-7           [-1, 32, 24, 24]              64\n           Dropout-8           [-1, 32, 24, 24]               0\n            Conv2d-9           [-1, 10, 24, 24]             320\n        MaxPool2d-10           [-1, 10, 12, 12]               0\n           Conv2d-11           [-1, 16, 10, 10]           1,440\n             ReLU-12           [-1, 16, 10, 10]               0\n      BatchNorm2d-13           [-1, 16, 10, 10]              32\n          Dropout-14           [-1, 16, 10, 10]               0\n           Conv2d-15             [-1, 16, 8, 8]           2,304\n             ReLU-16             [-1, 16, 8, 8]               0\n      BatchNorm2d-17             [-1, 16, 8, 8]              32\n          Dropout-18             [-1, 16, 8, 8]               0\n           Conv2d-19             [-1, 16, 6, 6]           2,304\n             ReLU-20             [-1, 16, 6, 6]               0\n      BatchNorm2d-21             [-1, 16, 6, 6]              32\n          Dropout-22             [-1, 16, 6, 6]               0\n           Conv2d-23             [-1, 16, 6, 6]           2,304\n             ReLU-24             [-1, 16, 6, 6]               0\n      BatchNorm2d-25             [-1, 16, 6, 6]              32\n          Dropout-26             [-1, 16, 6, 6]               0\n        AvgPool2d-27             [-1, 16, 1, 1]               0\n           Conv2d-28             [-1, 10, 1, 1]             160\n================================================================\nTotal params: 13,808\nTrainable params: 13,808\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 1.06\nParams size (MB): 0.05\nEstimated Total Size (MB): 1.12\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))\n</pre> from tqdm import tqdm  train_losses = [] test_losses = [] train_acc = [] test_acc = []  def train(model, device, train_loader, optimizer, epoch):   model.train()   pbar = tqdm(train_loader)   correct = 0   processed = 0   for batch_idx, (data, target) in enumerate(pbar):     # get samples     data, target = data.to(device), target.to(device)      # Init     optimizer.zero_grad()     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.      # Predict     y_pred = model(data)      # Calculate loss     loss = F.nll_loss(y_pred, target)     train_losses.append(loss)      # Backpropagation     loss.backward()     optimizer.step()      # Update pbar-tqdm      pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability     correct += pred.eq(target.view_as(pred)).sum().item()     processed += len(data)      pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')     train_acc.append(100*correct/processed)  def test(model, device, test_loader):     model.eval()     test_loss = 0     correct = 0     with torch.no_grad():         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()      test_loss /= len(test_loader.dataset)     test_losses.append(test_loss)      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(         test_loss, correct, len(test_loader.dataset),         100. * correct / len(test_loader.dataset)))      test_acc.append(100. * correct / len(test_loader.dataset)) In\u00a0[\u00a0]: Copied! <pre>from torch.optim.lr_scheduler import StepLR\n\nmodel =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n# scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n\n\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    # scheduler.step()\n    test(model, device, test_loader)\n</pre> from torch.optim.lr_scheduler import StepLR  model =  Net().to(device) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # scheduler = StepLR(optimizer, step_size=6, gamma=0.1)   EPOCHS = 20 for epoch in range(EPOCHS):     print(\"EPOCH:\", epoch)     train(model, device, train_loader, optimizer, epoch)     # scheduler.step()     test(model, device, test_loader) <pre>EPOCH: 0\n</pre> <pre>Loss=0.15558578073978424 Batch_id=468 Accuracy=86.76: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:25&lt;00:00, 18.47it/s]\n</pre> <pre>\nTest set: Average loss: 0.0621, Accuracy: 9815/10000 (98.15%)\n\nEPOCH: 1\n</pre> <pre>Loss=0.0665636882185936 Batch_id=468 Accuracy=97.66: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 22.07it/s]\n</pre> <pre>\nTest set: Average loss: 0.0409, Accuracy: 9871/10000 (98.71%)\n\nEPOCH: 2\n</pre> <pre>Loss=0.043326977640390396 Batch_id=468 Accuracy=98.07: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 22.18it/s]\n</pre> <pre>\nTest set: Average loss: 0.0328, Accuracy: 9901/10000 (99.01%)\n\nEPOCH: 3\n</pre> <pre>Loss=0.08539190888404846 Batch_id=468 Accuracy=98.34: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.14it/s]\n</pre> <pre>\nTest set: Average loss: 0.0277, Accuracy: 9909/10000 (99.09%)\n\nEPOCH: 4\n</pre> <pre>Loss=0.03832345828413963 Batch_id=468 Accuracy=98.53: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.87it/s]\n</pre> <pre>\nTest set: Average loss: 0.0314, Accuracy: 9890/10000 (98.90%)\n\nEPOCH: 5\n</pre> <pre>Loss=0.05199088528752327 Batch_id=468 Accuracy=98.67: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 20.78it/s]\n</pre> <pre>\nTest set: Average loss: 0.0255, Accuracy: 9918/10000 (99.18%)\n\nEPOCH: 6\n</pre> <pre>Loss=0.0759967565536499 Batch_id=468 Accuracy=98.69: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.89it/s]\n</pre> <pre>\nTest set: Average loss: 0.0222, Accuracy: 9926/10000 (99.26%)\n\nEPOCH: 7\n</pre> <pre>Loss=0.014048202894628048 Batch_id=468 Accuracy=98.78: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.98it/s]\n</pre> <pre>\nTest set: Average loss: 0.0215, Accuracy: 9929/10000 (99.29%)\n\nEPOCH: 8\n</pre> <pre>Loss=0.02991652488708496 Batch_id=468 Accuracy=98.77: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.74it/s]\n</pre> <pre>\nTest set: Average loss: 0.0210, Accuracy: 9930/10000 (99.30%)\n\nEPOCH: 9\n</pre> <pre>Loss=0.0512668676674366 Batch_id=468 Accuracy=98.85: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.33it/s]\n</pre> <pre>\nTest set: Average loss: 0.0200, Accuracy: 9933/10000 (99.33%)\n\nEPOCH: 10\n</pre> <pre>Loss=0.036390434950590134 Batch_id=468 Accuracy=98.91: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.98it/s]\n</pre> <pre>\nTest set: Average loss: 0.0219, Accuracy: 9928/10000 (99.28%)\n\nEPOCH: 11\n</pre> <pre>Loss=0.02335640788078308 Batch_id=468 Accuracy=98.89: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:24&lt;00:00, 18.88it/s]\n</pre> <pre>\nTest set: Average loss: 0.0194, Accuracy: 9937/10000 (99.37%)\n\nEPOCH: 12\n</pre> <pre>Loss=0.00636637769639492 Batch_id=468 Accuracy=98.98: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.78it/s]\n</pre> <pre>\nTest set: Average loss: 0.0212, Accuracy: 9927/10000 (99.27%)\n\nEPOCH: 13\n</pre> <pre>Loss=0.021595226600766182 Batch_id=468 Accuracy=98.95: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 20.87it/s]\n</pre> <pre>\nTest set: Average loss: 0.0238, Accuracy: 9922/10000 (99.22%)\n\nEPOCH: 14\n</pre> <pre>Loss=0.02908242493867874 Batch_id=468 Accuracy=99.05: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.32it/s]\n</pre> <pre>\nTest set: Average loss: 0.0199, Accuracy: 9934/10000 (99.34%)\n\nEPOCH: 15\n</pre> <pre>Loss=0.05177703872323036 Batch_id=468 Accuracy=99.02: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.64it/s]\n</pre> <pre>\nTest set: Average loss: 0.0216, Accuracy: 9929/10000 (99.29%)\n\nEPOCH: 16\n</pre> <pre>Loss=0.03772637993097305 Batch_id=468 Accuracy=99.03: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.98it/s]\n</pre> <pre>\nTest set: Average loss: 0.0181, Accuracy: 9943/10000 (99.43%)\n\nEPOCH: 17\n</pre> <pre>Loss=0.00879430677741766 Batch_id=468 Accuracy=99.07: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.98it/s]\n</pre> <pre>\nTest set: Average loss: 0.0195, Accuracy: 9932/10000 (99.32%)\n\nEPOCH: 18\n</pre> <pre>Loss=0.029844345524907112 Batch_id=468 Accuracy=99.13: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:22&lt;00:00, 21.29it/s]\n</pre> <pre>\nTest set: Average loss: 0.0176, Accuracy: 9947/10000 (99.47%)\n\nEPOCH: 19\n</pre> <pre>Loss=0.0018948359647765756 Batch_id=468 Accuracy=99.11: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 22.15it/s]\n</pre> <pre>\nTest set: Average loss: 0.0169, Accuracy: 9943/10000 (99.43%)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/deep_learning/Code_9/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"notebooks/deep_learning/Code_9/#data-transformations","title":"Data Transformations\u00b6","text":"<p>We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.</p>"},{"location":"notebooks/deep_learning/Code_9/#dataset-and-creating-traintest-split","title":"Dataset and Creating Train/Test Split\u00b6","text":""},{"location":"notebooks/deep_learning/Code_9/#dataloader-arguments-testtrain-dataloaders","title":"Dataloader Arguments &amp; Test/Train Dataloaders\u00b6","text":""},{"location":"notebooks/deep_learning/Code_9/#the-model","title":"The model\u00b6","text":"<p>Let's start with the model we first saw</p>"},{"location":"notebooks/deep_learning/Code_9/#model-params","title":"Model Params\u00b6","text":"<p>Can't emphasize on how important viewing Model Summary is. Unfortunately, there is no in-built model visualizer, so we have to take external help</p>"},{"location":"notebooks/deep_learning/Code_9/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>All right, so we have 24M params, and that's too many, we know that. But the purpose of this notebook is to set things right for our future experiments.</p> <p>Looking at logs can be boring, so we'll introduce tqdm progressbar to get cooler logs.</p> <p>Let's write train and test functions</p>"},{"location":"notebooks/deep_learning/PyTorch_101/","title":"PyTorch 101","text":"In\u00a0[1]: Copied! <pre># Pytorch's tensors are similar to Numpy's ndarrays\n!pip install torch\n</pre> # Pytorch's tensors are similar to Numpy's ndarrays !pip install torch <pre>Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions&gt;=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy&gt;=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx&gt;=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec&gt;=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy&gt;=1.13.3-&gt;torch) (1.3.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-&gt;torch) (3.0.3)\n</pre> In\u00a0[2]: Copied! <pre>!ls\n</pre> !ls <pre>sample_data\n</pre> In\u00a0[3]: Copied! <pre>import numpy as np\n\nV0 = np.array(1.3)\nV1 = np.array([1., 2., 3.])\nV2 = np.array([[1., 2.], [4., 5.]])\nprint(f'{V0}, {V1}, {V2}')\n</pre> import numpy as np  V0 = np.array(1.3) V1 = np.array([1., 2., 3.]) V2 = np.array([[1., 2.], [4., 5.]]) print(f'{V0}, {V1}, {V2}') <pre>1.3, [1. 2. 3.], [[1. 2.]\n [4. 5.]]\n</pre> In\u00a0[4]: Copied! <pre>numpy_array = np.array([1, 2, 3])\n</pre> numpy_array = np.array([1, 2, 3]) In\u00a0[5]: Copied! <pre>import torch\n\nt1 = torch.Tensor(numpy_array)\n# Constructor - same as torch.FloatTensor\n# uses a default float32 tensor, this bechanvor can be changed\n# all other tensors inherit from this main tensor class\n\nt2 = torch.tensor(numpy_array)\n# Factory Function\n# going to produce a new tensor of the same type\n# does not share underlying memory with numpy!\n# always copies the data\n# torch.tensor(data, dtype=None, device=None, requires_grad=False/True)\n# highly recommended to use!\n\nt3 = torch.as_tensor(numpy_array)\n# Factory Function\n# produce a new tensor with the same data type\n# share the underlying memory with numpy, changing one, will change another!\n# CAN ACCEPT ANY PYTHON DATA STRUCTURE INCLUDING NUMPY ARRAYS\n# always tries to avoid a copy of the data!\n# we have to manually call requires_grad() function on the final tensor. It does not suppor\n#     requires_grad by default\n# recommended to use!\n\nt4 = torch.from_numpy(numpy_array)\n# Factory Function\n# produce a new tensor with the same data type\n# share the underlying memory with numpy, changing one, will change another!\n# CAN ACCEPT ONLY NUMPY ARRAYS\n</pre> import torch  t1 = torch.Tensor(numpy_array) # Constructor - same as torch.FloatTensor # uses a default float32 tensor, this bechanvor can be changed # all other tensors inherit from this main tensor class  t2 = torch.tensor(numpy_array) # Factory Function # going to produce a new tensor of the same type # does not share underlying memory with numpy! # always copies the data # torch.tensor(data, dtype=None, device=None, requires_grad=False/True) # highly recommended to use!  t3 = torch.as_tensor(numpy_array) # Factory Function # produce a new tensor with the same data type # share the underlying memory with numpy, changing one, will change another! # CAN ACCEPT ANY PYTHON DATA STRUCTURE INCLUDING NUMPY ARRAYS # always tries to avoid a copy of the data! # we have to manually call requires_grad() function on the final tensor. It does not suppor #     requires_grad by default # recommended to use!  t4 = torch.from_numpy(numpy_array) # Factory Function # produce a new tensor with the same data type # share the underlying memory with numpy, changing one, will change another! # CAN ACCEPT ONLY NUMPY ARRAYS In\u00a0[6]: Copied! <pre>numpy_array.dtype, t1.dtype, t2.dtype, t3.dtype, t4.dtype\n</pre> numpy_array.dtype, t1.dtype, t2.dtype, t3.dtype, t4.dtype Out[6]: <pre>(dtype('int64'), torch.float32, torch.int64, torch.int64, torch.int64)</pre> In\u00a0[7]: Copied! <pre>numpy_array *= 4\n# numpy_array = numpy_array * 4\nnumpy_array\n</pre> numpy_array *= 4 # numpy_array = numpy_array * 4 numpy_array Out[7]: <pre>array([ 4,  8, 12])</pre> In\u00a0[8]: Copied! <pre>t1, t2, t3, t4\n</pre> t1, t2, t3, t4 Out[8]: <pre>(tensor([1., 2., 3.]),\n tensor([1, 2, 3]),\n tensor([ 4,  8, 12]),\n tensor([ 4,  8, 12]))</pre> In\u00a0[9]: Copied! <pre>t1 *= 2\nnumpy_array, t1, t2, t3, t4\n</pre> t1 *= 2 numpy_array, t1, t2, t3, t4 Out[9]: <pre>(array([ 4,  8, 12]),\n tensor([2., 4., 6.]),\n tensor([1, 2, 3]),\n tensor([ 4,  8, 12]),\n tensor([ 4,  8, 12]))</pre> In\u00a0[10]: Copied! <pre>t2 *= 2\nnumpy_array, t1, t2, t3, t4\n</pre> t2 *= 2 numpy_array, t1, t2, t3, t4 Out[10]: <pre>(array([ 4,  8, 12]),\n tensor([2., 4., 6.]),\n tensor([2, 4, 6]),\n tensor([ 4,  8, 12]),\n tensor([ 4,  8, 12]))</pre> In\u00a0[11]: Copied! <pre>t3 *= 2\nnumpy_array, t1, t2, t3, t4\n</pre> t3 *= 2 numpy_array, t1, t2, t3, t4 Out[11]: <pre>(array([ 8, 16, 24]),\n tensor([2., 4., 6.]),\n tensor([2, 4, 6]),\n tensor([ 8, 16, 24]),\n tensor([ 8, 16, 24]))</pre> In\u00a0[12]: Copied! <pre>t4 *= 2\nnumpy_array, t1, t2, t3, t4\n</pre> t4 *= 2 numpy_array, t1, t2, t3, t4 Out[12]: <pre>(array([16, 32, 48]),\n tensor([2., 4., 6.]),\n tensor([2, 4, 6]),\n tensor([16, 32, 48]),\n tensor([16, 32, 48]))</pre> In\u00a0[13]: Copied! <pre>t5 = t4.cuda()\n\nt5.device, t4.device\n</pre> t5 = t4.cuda()  t5.device, t4.device Out[13]: <pre>(device(type='cuda', index=0), device(type='cpu'))</pre> In\u00a0[14]: Copied! <pre>t6 = t4 + t3\nt6\n</pre> t6 = t4 + t3 t6 Out[14]: <pre>tensor([32, 64, 96])</pre> In\u00a0[15]: Copied! <pre>t6 = t4 + t5\nt6\n</pre> t6 = t4 + t5 t6 <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/tmp/ipython-input-2838280936.py in &lt;cell line: 0&gt;()\n----&gt; 1 t6 = t4 + t5\n      2 t6\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</pre> In\u00a0[16]: Copied! <pre>torch.eye(2)\n</pre> torch.eye(2) Out[16]: <pre>tensor([[1., 0.],\n        [0., 1.]])</pre> In\u00a0[17]: Copied! <pre>torch.zeros(2, 2)\n</pre> torch.zeros(2, 2) Out[17]: <pre>tensor([[0., 0.],\n        [0., 0.]])</pre> In\u00a0[18]: Copied! <pre>torch.ones(2, 3)\n</pre> torch.ones(2, 3) Out[18]: <pre>tensor([[1., 1., 1.],\n        [1., 1., 1.]])</pre> In\u00a0[19]: Copied! <pre>torch.rand(2, 4)\n</pre> torch.rand(2, 4) Out[19]: <pre>tensor([[0.8332, 0.8275, 0.3166, 0.2415],\n        [0.3012, 0.1273, 0.4978, 0.8948]])</pre> In\u00a0[20]: Copied! <pre>data = torch.tensor([[1, 2, 3], [4, 5, 6]])\ndata[1, 0], data[0, 0:2], data[:2, :2]\n</pre> data = torch.tensor([[1, 2, 3], [4, 5, 6]]) data[1, 0], data[0, 0:2], data[:2, :2] Out[20]: <pre>(tensor(4),\n tensor([1, 2]),\n tensor([[1, 2],\n         [4, 5]]))</pre> In\u00a0[21]: Copied! <pre>t = torch.tensor(data = [1, 2, 3], dtype=torch.float32, device='cpu', requires_grad=False)\nt\n</pre> t = torch.tensor(data = [1, 2, 3], dtype=torch.float32, device='cpu', requires_grad=False) t Out[21]: <pre>tensor([1., 2., 3.])</pre> In\u00a0[22]: Copied! <pre>t = torch.as_tensor(data = [1, 2, 3], dtype=torch.float32, device='cpu', requires_grad=False)\nt\n</pre> t = torch.as_tensor(data = [1, 2, 3], dtype=torch.float32, device='cpu', requires_grad=False) t <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/tmp/ipython-input-2082098798.py in &lt;cell line: 0&gt;()\n----&gt; 1 t = torch.as_tensor(data = [1, 2, 3], dtype=torch.float32, device='cpu', requires_grad=False)\n      2 t\n\nTypeError: as_tensor() got an unexpected keyword argument 'requires_grad'</pre> In\u00a0[23]: Copied! <pre>t = torch.as_tensor(data = [1, 2, 3], dtype=torch.float32, device='cpu')\nt\n</pre> t = torch.as_tensor(data = [1, 2, 3], dtype=torch.float32, device='cpu') t Out[23]: <pre>tensor([1., 2., 3.])</pre> In\u00a0[24]: Copied! <pre>t.requires_grad_()\n</pre> t.requires_grad_() Out[24]: <pre>tensor([1., 2., 3.], requires_grad=True)</pre> In\u00a0[25]: Copied! <pre>from torch.autograd import grad\n\nx1 = torch.tensor(2, requires_grad=True, dtype=torch.float16)\nx2 = torch.tensor(3, requires_grad=True, dtype=torch.float16)\nx3 = torch.tensor(1, requires_grad=True, dtype=torch.float16)\nx4 = torch.tensor(4, requires_grad=True, dtype=torch.float16)\n\nx1, x2, x3, x4\n</pre> from torch.autograd import grad  x1 = torch.tensor(2, requires_grad=True, dtype=torch.float16) x2 = torch.tensor(3, requires_grad=True, dtype=torch.float16) x3 = torch.tensor(1, requires_grad=True, dtype=torch.float16) x4 = torch.tensor(4, requires_grad=True, dtype=torch.float16)  x1, x2, x3, x4 Out[25]: <pre>(tensor(2., dtype=torch.float16, requires_grad=True),\n tensor(3., dtype=torch.float16, requires_grad=True),\n tensor(1., dtype=torch.float16, requires_grad=True),\n tensor(4., dtype=torch.float16, requires_grad=True))</pre> In\u00a0[26]: Copied! <pre>z1 = x1 * x2\nz2 = x3 * x4\n\nf = z1 + z2\n# f = x1 * x2 + x3 * x4\n# f = 2 * 3 + 1 * 4\n# df_dx1 = x1 * 3 + 1 * 4\n# df_dx4 = 2 * 3 + 1 * x4\n\ndf_dx = grad(outputs = f, inputs = [x1, x2, x3, x4])\nprint(f'gradient of x1 = {df_dx[0]}')\nprint(f'gradient of x2 = {df_dx[1]}')\nprint(f'gradient of x3 = {df_dx[2]}')\nprint(f'gradient of x4 = {df_dx[3]}')\n</pre> z1 = x1 * x2 z2 = x3 * x4  f = z1 + z2 # f = x1 * x2 + x3 * x4 # f = 2 * 3 + 1 * 4 # df_dx1 = x1 * 3 + 1 * 4 # df_dx4 = 2 * 3 + 1 * x4  df_dx = grad(outputs = f, inputs = [x1, x2, x3, x4]) print(f'gradient of x1 = {df_dx[0]}') print(f'gradient of x2 = {df_dx[1]}') print(f'gradient of x3 = {df_dx[2]}') print(f'gradient of x4 = {df_dx[3]}') <pre>gradient of x1 = 3.0\ngradient of x2 = 2.0\ngradient of x3 = 4.0\ngradient of x4 = 1.0\n</pre> In\u00a0[27]: Copied! <pre>import torch.nn.functional as F\nimport torch.optim as optim\n\nopt = optim.SGD(params = [x1, x2, x3, x4], lr=0.001)\nopt.zero_grad()\n</pre> import torch.nn.functional as F import torch.optim as optim  opt = optim.SGD(params = [x1, x2, x3, x4], lr=0.001) opt.zero_grad() In\u00a0[28]: Copied! <pre>z1 = x1 * x2\nz2 = x3 * x4\n\nf = z1 + z2\n\nf.backward()\n\nprint(f'gradient of x1 = {x1.grad}')\nprint(f'gradient of x2 = {x2.grad}')\nprint(f'gradient of x3 = {x3.grad}')\nprint(f'gradient of x4 = {x4.grad}')\n</pre> z1 = x1 * x2 z2 = x3 * x4  f = z1 + z2  f.backward()  print(f'gradient of x1 = {x1.grad}') print(f'gradient of x2 = {x2.grad}') print(f'gradient of x3 = {x3.grad}') print(f'gradient of x4 = {x4.grad}') <pre>gradient of x1 = 3.0\ngradient of x2 = 2.0\ngradient of x3 = 4.0\ngradient of x4 = 1.0\n</pre> In\u00a0[29]: Copied! <pre>t = torch.tensor([\n    [0, 0, 0, 0],\n    [1, 2, 3, 4],\n    [2, 2, 2, 2]\n], dtype=torch.float32)\n</pre> t = torch.tensor([     [0, 0, 0, 0],     [1, 2, 3, 4],     [2, 2, 2, 2] ], dtype=torch.float32) In\u00a0[30]: Copied! <pre>t.shape, t.size(), len(t.shape)\n</pre> t.shape, t.size(), len(t.shape) Out[30]: <pre>(torch.Size([3, 4]), torch.Size([3, 4]), 2)</pre> In\u00a0[31]: Copied! <pre>torch.tensor(t.shape).prod()\n</pre> torch.tensor(t.shape).prod() Out[31]: <pre>tensor(12)</pre> In\u00a0[32]: Copied! <pre>t.numel()\n</pre> t.numel() Out[32]: <pre>12</pre> In\u00a0[33]: Copied! <pre>t.reshape(1, 12)\n</pre> t.reshape(1, 12) Out[33]: <pre>tensor([[0., 0., 0., 0., 1., 2., 3., 4., 2., 2., 2., 2.]])</pre> In\u00a0[34]: Copied! <pre>t.reshape(2, 6)\n</pre> t.reshape(2, 6) Out[34]: <pre>tensor([[0., 0., 0., 0., 1., 2.],\n        [3., 4., 2., 2., 2., 2.]])</pre> In\u00a0[35]: Copied! <pre>t.reshape(2, -1)\n</pre> t.reshape(2, -1) Out[35]: <pre>tensor([[0., 0., 0., 0., 1., 2.],\n        [3., 4., 2., 2., 2., 2.]])</pre> In\u00a0[36]: Copied! <pre>t.reshape(4, 3)\n</pre> t.reshape(4, 3) Out[36]: <pre>tensor([[0., 0., 0.],\n        [0., 1., 2.],\n        [3., 4., 2.],\n        [2., 2., 2.]])</pre> In\u00a0[37]: Copied! <pre>t.reshape(2, 2, 3)\n</pre> t.reshape(2, 2, 3) Out[37]: <pre>tensor([[[0., 0., 0.],\n         [0., 1., 2.]],\n\n        [[3., 4., 2.],\n         [2., 2., 2.]]])</pre> In\u00a0[38]: Copied! <pre>print(t.reshape(1, 12))\nprint(t.reshape(1, 12).shape)\n</pre> print(t.reshape(1, 12)) print(t.reshape(1, 12).shape) <pre>tensor([[0., 0., 0., 0., 1., 2., 3., 4., 2., 2., 2., 2.]])\ntorch.Size([1, 12])\n</pre> <p>Squeezing a Tensor</p> <p>Removes all the dimensions that have a length of 1</p> <p>Unsqueezing a Tensor Adds a dimension that has a length of 1.</p> In\u00a0[39]: Copied! <pre>print(t.reshape(1, 12))\nprint(t.reshape(1, 12).shape)\n</pre> print(t.reshape(1, 12)) print(t.reshape(1, 12).shape) <pre>tensor([[0., 0., 0., 0., 1., 2., 3., 4., 2., 2., 2., 2.]])\ntorch.Size([1, 12])\n</pre> In\u00a0[40]: Copied! <pre>print(t.reshape(1, 12).squeeze())\nprint(t.reshape(1, 12).squeeze().shape)\n</pre> print(t.reshape(1, 12).squeeze()) print(t.reshape(1, 12).squeeze().shape) <pre>tensor([0., 0., 0., 0., 1., 2., 3., 4., 2., 2., 2., 2.])\ntorch.Size([12])\n</pre> In\u00a0[41]: Copied! <pre>print(t.reshape(1, 12).squeeze().unsqueeze(dim = 0))\nprint(t.reshape(1, 12).squeeze().unsqueeze(dim = 0).shape)\n</pre> print(t.reshape(1, 12).squeeze().unsqueeze(dim = 0)) print(t.reshape(1, 12).squeeze().unsqueeze(dim = 0).shape) <pre>tensor([[0., 0., 0., 0., 1., 2., 3., 4., 2., 2., 2., 2.]])\ntorch.Size([1, 12])\n</pre> <p>Use cases?</p> <p>Neural networks are always trained in a batch of samples. This is troubling because when we want to test 1 image, we do not have an array, we only have 1 image. Well we unsqueeze it to fake a batch.</p> <p>We use this function that is very common, called Flatten. This essentially create a new tensor that is only 1D. This is done to connect our data to next Fully Connected Layers. We use squeeze function for this.</p> <p>Let's implement such a function.</p> In\u00a0[42]: Copied! <pre>def flatter(t):\n  t = t.reshape(1, -1)\n  t = t.squeeze()\n  return t\n\nt.shape\n</pre> def flatter(t):   t = t.reshape(1, -1)   t = t.squeeze()   return t  t.shape  Out[42]: <pre>torch.Size([3, 4])</pre> In\u00a0[43]: Copied! <pre>flatter(t)\n</pre> flatter(t) Out[43]: <pre>tensor([0., 0., 0., 0., 1., 2., 3., 4., 2., 2., 2., 2.])</pre> In\u00a0[44]: Copied! <pre>t1 = torch.tensor([\n    [1, 2],\n    [3, 4]\n])\n\nt2 = torch.tensor([\n    [5, 6],\n    [7, 8]\n])\n\ntorch.cat((t1, t2), dim=0)\n</pre> t1 = torch.tensor([     [1, 2],     [3, 4] ])  t2 = torch.tensor([     [5, 6],     [7, 8] ])  torch.cat((t1, t2), dim=0) Out[44]: <pre>tensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])</pre> In\u00a0[45]: Copied! <pre>torch.cat((t1, t2), dim=1)\n</pre> torch.cat((t1, t2), dim=1) Out[45]: <pre>tensor([[1, 2, 5, 6],\n        [3, 4, 7, 8]])</pre> In\u00a0[46]: Copied! <pre>t.reshape(2, -1)\n</pre> t.reshape(2, -1) Out[46]: <pre>tensor([[0., 0., 0., 0., 1., 2.],\n        [3., 4., 2., 2., 2., 2.]])</pre> In\u00a0[47]: Copied! <pre>img1 = torch.tensor([\n  [1, 1, 1, 1],\n  [1, 1, 1, 1],\n  [1, 1, 1, 1],\n  [1, 1, 1, 1]\n])\n\nimg2 = torch.tensor([\n  [2, 2, 2, 2],\n  [2, 2, 2, 2],\n  [2, 2, 2, 2],\n  [2, 2, 2, 2]\n])\n\nimg3 = torch.tensor([\n  [3, 3, 3, 3],\n  [3, 3, 3, 3],\n  [3, 3, 3, 3],\n  [3, 3, 3, 3]\n])\n</pre> img1 = torch.tensor([   [1, 1, 1, 1],   [1, 1, 1, 1],   [1, 1, 1, 1],   [1, 1, 1, 1] ])  img2 = torch.tensor([   [2, 2, 2, 2],   [2, 2, 2, 2],   [2, 2, 2, 2],   [2, 2, 2, 2] ])  img3 = torch.tensor([   [3, 3, 3, 3],   [3, 3, 3, 3],   [3, 3, 3, 3],   [3, 3, 3, 3] ]) <p>3 examples of 4x4 data</p> In\u00a0[48]: Copied! <pre>batch = torch.stack((img1, img2, img3))\nbatch.shape\n</pre> batch = torch.stack((img1, img2, img3)) batch.shape Out[48]: <pre>torch.Size([3, 4, 4])</pre> In\u00a0[49]: Copied! <pre>batch = batch.reshape(3, 1, 4, 4)\nbatch\n</pre> batch = batch.reshape(3, 1, 4, 4) batch Out[49]: <pre>tensor([[[[1, 1, 1, 1],\n          [1, 1, 1, 1],\n          [1, 1, 1, 1],\n          [1, 1, 1, 1]]],\n\n\n        [[[2, 2, 2, 2],\n          [2, 2, 2, 2],\n          [2, 2, 2, 2],\n          [2, 2, 2, 2]]],\n\n\n        [[[3, 3, 3, 3],\n          [3, 3, 3, 3],\n          [3, 3, 3, 3],\n          [3, 3, 3, 3]]]])</pre> In\u00a0[50]: Copied! <pre># let's get our first image\n\nbatch[0]\n</pre> # let's get our first image  batch[0] Out[50]: <pre>tensor([[[1, 1, 1, 1],\n         [1, 1, 1, 1],\n         [1, 1, 1, 1],\n         [1, 1, 1, 1]]])</pre> In\u00a0[51]: Copied! <pre># let's get the first channel of our first image\n\nbatch[0][0]\n</pre> # let's get the first channel of our first image  batch[0][0] Out[51]: <pre>tensor([[1, 1, 1, 1],\n        [1, 1, 1, 1],\n        [1, 1, 1, 1],\n        [1, 1, 1, 1]])</pre> In\u00a0[52]: Copied! <pre># let's get the first row of our first channel of our first image\n\nbatch[0][0][0]\n</pre> # let's get the first row of our first channel of our first image  batch[0][0][0] Out[52]: <pre>tensor([1, 1, 1, 1])</pre> In\u00a0[53]: Copied! <pre># let's get the first pixel of the first row of our first channel of our first image\n\nbatch[0][0][0][0]\n</pre> # let's get the first pixel of the first row of our first channel of our first image  batch[0][0][0][0] Out[53]: <pre>tensor(1)</pre> In\u00a0[54]: Copied! <pre># Let's see how we flatten our image inside the batch\n\nbatch.reshape(1, -1)[0]\n</pre> # Let's see how we flatten our image inside the batch  batch.reshape(1, -1)[0]  Out[54]: <pre>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])</pre> In\u00a0[55]: Copied! <pre>batch.reshape(1, 4, -1)[0]\n</pre> batch.reshape(1, 4, -1)[0] Out[55]: <pre>tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2],\n        [2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3],\n        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])</pre> In\u00a0[56]: Copied! <pre>batch.reshape(-1)\n</pre> batch.reshape(-1) Out[56]: <pre>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])</pre> In\u00a0[57]: Copied! <pre>batch.reshape(batch.numel())\n</pre> batch.reshape(batch.numel()) Out[57]: <pre>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])</pre> In\u00a0[58]: Copied! <pre>batch.flatten()\n</pre> batch.flatten() Out[58]: <pre>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])</pre> In\u00a0[59]: Copied! <pre>batch.reshape(3, 1, -1)\n</pre> batch.reshape(3, 1, -1) Out[59]: <pre>tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n\n        [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],\n\n        [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]])</pre> In\u00a0[60]: Copied! <pre>batch.reshape(3, -1)\n</pre> batch.reshape(3, -1) Out[60]: <pre>tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])</pre> In\u00a0[61]: Copied! <pre>batch.flatten(start_dim = 1)\n</pre> batch.flatten(start_dim = 1) Out[61]: <pre>tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])</pre> In\u00a0[62]: Copied! <pre>t1 = torch.tensor([\n    [1, 2],\n    [3, 4]\n], dtype=torch.float32)\n\nt2 = torch.tensor([\n    [5, 6],\n    [7, 8]\n], dtype=torch.float32)\n</pre> t1 = torch.tensor([     [1, 2],     [3, 4] ], dtype=torch.float32)  t2 = torch.tensor([     [5, 6],     [7, 8] ], dtype=torch.float32) In\u00a0[63]: Copied! <pre>t1 + t2\n</pre> t1 + t2 Out[63]: <pre>tensor([[ 6.,  8.],\n        [10., 12.]])</pre> In\u00a0[64]: Copied! <pre>t1 + 2\n</pre> t1 + 2 Out[64]: <pre>tensor([[3., 4.],\n        [5., 6.]])</pre> In\u00a0[65]: Copied! <pre>t1 - 2\n</pre> t1 - 2 Out[65]: <pre>tensor([[-1.,  0.],\n        [ 1.,  2.]])</pre> In\u00a0[66]: Copied! <pre>print(t1.add(2))\n\nprint(t1.sub(2))\n\nprint(t1.mul(2))\n\nprint(t1.div(2))\n</pre> print(t1.add(2))  print(t1.sub(2))  print(t1.mul(2))  print(t1.div(2)) <pre>tensor([[3., 4.],\n        [5., 6.]])\ntensor([[-1.,  0.],\n        [ 1.,  2.]])\ntensor([[2., 4.],\n        [6., 8.]])\ntensor([[0.5000, 1.0000],\n        [1.5000, 2.0000]])\n</pre> In\u00a0[67]: Copied! <pre>t = torch.tensor([\n    [0, 5, 7],\n    [6, 0, 7],\n    [0, 8, 0]\n], dtype=torch.float32)\n</pre> t = torch.tensor([     [0, 5, 7],     [6, 0, 7],     [0, 8, 0] ], dtype=torch.float32) In\u00a0[68]: Copied! <pre>t.eq(0)\n</pre> t.eq(0) Out[68]: <pre>tensor([[ True, False, False],\n        [False,  True, False],\n        [ True, False,  True]])</pre> In\u00a0[69]: Copied! <pre>t.ge(0)\n</pre> t.ge(0) Out[69]: <pre>tensor([[True, True, True],\n        [True, True, True],\n        [True, True, True]])</pre> In\u00a0[70]: Copied! <pre>t.le(7)\n</pre> t.le(7) Out[70]: <pre>tensor([[ True,  True,  True],\n        [ True,  True,  True],\n        [ True, False,  True]])</pre> In\u00a0[71]: Copied! <pre>t.abs()\n</pre> t.abs() Out[71]: <pre>tensor([[0., 5., 7.],\n        [6., 0., 7.],\n        [0., 8., 0.]])</pre> In\u00a0[72]: Copied! <pre>t.sqrt()\n</pre> t.sqrt() Out[72]: <pre>tensor([[0.0000, 2.2361, 2.6458],\n        [2.4495, 0.0000, 2.6458],\n        [0.0000, 2.8284, 0.0000]])</pre> In\u00a0[73]: Copied! <pre>t = torch.tensor([\n    [0, 5, 7],\n    [6, 0, 7],\n    [0, 8, 0]\n], dtype=torch.float32)\n</pre> t = torch.tensor([     [0, 5, 7],     [6, 0, 7],     [0, 8, 0] ], dtype=torch.float32) In\u00a0[74]: Copied! <pre>t.sum()\n</pre> t.sum() Out[74]: <pre>tensor(33.)</pre> In\u00a0[75]: Copied! <pre>t.prod()\n</pre> t.prod() Out[75]: <pre>tensor(0.)</pre> In\u00a0[76]: Copied! <pre>t.mean()\n</pre> t.mean() Out[76]: <pre>tensor(3.6667)</pre> In\u00a0[77]: Copied! <pre>t.std()\n</pre> t.std() Out[77]: <pre>tensor(3.5707)</pre> In\u00a0[78]: Copied! <pre>t = torch.tensor([\n  [1, 1, 1, 1],\n  [2, 2, 2, 2],\n  [3, 3, 3, 3],\n  [4, 4, 4, 4]\n])\n</pre> t = torch.tensor([   [1, 1, 1, 1],   [2, 2, 2, 2],   [3, 3, 3, 3],   [4, 4, 4, 4] ]) In\u00a0[79]: Copied! <pre>t.sum(dim=0)\n</pre> t.sum(dim=0) Out[79]: <pre>tensor([10, 10, 10, 10])</pre> In\u00a0[80]: Copied! <pre>t.sum(dim=1)\n</pre> t.sum(dim=1) Out[80]: <pre>tensor([ 4,  8, 12, 16])</pre> In\u00a0[81]: Copied! <pre>t.argmax(), t.max()\n</pre> t.argmax(), t.max() Out[81]: <pre>(tensor(12), tensor(4))</pre> In\u00a0[82]: Copied! <pre>t.argmin(), t.min()\n</pre> t.argmin(), t.min() Out[82]: <pre>(tensor(0), tensor(1))</pre> In\u00a0[83]: Copied! <pre>t.flatten()\n</pre> t.flatten() Out[83]: <pre>tensor([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4])</pre> In\u00a0[84]: Copied! <pre>import torch\nimport torchvision # provide access to datasets, models, transforms, utils, etc\nimport torchvision.transforms as transforms\n</pre> import torch import torchvision # provide access to datasets, models, transforms, utils, etc import torchvision.transforms as transforms In\u00a0[85]: Copied! <pre>from torch.utils.data import Dataset\n\nclass Animals(Dataset):\n  def __init__(self, csv_file):\n    self.data = pd.read_csv(csv_file)\n\n  def __getitem__(self, index):\n    r = self.data.iloc[index]\n    label, image = r\n    return label, image\n\n  def __len__(self):\n    return len(self.data)\n</pre>  from torch.utils.data import Dataset  class Animals(Dataset):   def __init__(self, csv_file):     self.data = pd.read_csv(csv_file)    def __getitem__(self, index):     r = self.data.iloc[index]     label, image = r     return label, image    def __len__(self):     return len(self.data) In\u00a0[86]: Copied! <pre># now let's work with FashionMnist\n\ntrain_set = torchvision.datasets.FashionMNIST(\n    root='./data'\n    ,train=True\n    ,download=True\n    ,transform=transforms.Compose([\n        transforms.ToTensor()\n    ])\n)\n</pre> # now let's work with FashionMnist  train_set = torchvision.datasets.FashionMNIST(     root='./data'     ,train=True     ,download=True     ,transform=transforms.Compose([         transforms.ToTensor()     ]) )   <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.4M/26.4M [00:01&lt;00:00, 13.4MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.5k/29.5k [00:00&lt;00:00, 212kB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.42M/4.42M [00:01&lt;00:00, 3.94MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.15k/5.15k [00:00&lt;00:00, 29.3MB/s]\n</pre> In\u00a0[87]: Copied! <pre>x = iter(range(10))\nnext(x)\nnext(x)\n</pre> x = iter(range(10)) next(x) next(x) Out[87]: <pre>1</pre> In\u00a0[88]: Copied! <pre>next(x)\n</pre> next(x) Out[88]: <pre>2</pre> In\u00a0[89]: Copied! <pre>next(iter(train_set))[1]\n</pre> next(iter(train_set))[1] Out[89]: <pre>9</pre> In\u00a0[90]: Copied! <pre>i = 0\n\nfor j in train_set:\n  print(j[1])\n  i+= 1\n  if i &gt; 5:\n    break\n</pre> i = 0  for j in train_set:   print(j[1])   i+= 1   if i &gt; 5:     break <pre>9\n0\n0\n3\n0\n2\n</pre> In\u00a0[91]: Copied! <pre>train_loader = torch.utils.data.DataLoader(train_set\n    ,batch_size=32\n    ,shuffle=True\n)\n</pre> train_loader = torch.utils.data.DataLoader(train_set     ,batch_size=32     ,shuffle=True ) In\u00a0[92]: Copied! <pre>next(iter(train_loader))[1]\n</pre> next(iter(train_loader))[1] Out[92]: <pre>tensor([9, 2, 0, 6, 8, 3, 9, 0, 3, 4, 2, 0, 7, 5, 0, 2, 2, 9, 3, 9, 8, 4, 5, 5,\n        0, 4, 3, 6, 9, 2, 8, 7])</pre> In\u00a0[93]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\ntorch.set_printoptions(linewidth=120)\n</pre> import numpy as np import matplotlib.pyplot as plt  torch.set_printoptions(linewidth=120) In\u00a0[94]: Copied! <pre>len(train_set)\n</pre> len(train_set) Out[94]: <pre>60000</pre> In\u00a0[95]: Copied! <pre>train_set.train_labels\n</pre> train_set.train_labels <pre>/usr/local/lib/python3.12/dist-packages/torchvision/datasets/mnist.py:66: UserWarning: train_labels has been renamed targets\n  warnings.warn(\"train_labels has been renamed targets\")\n</pre> Out[95]: <pre>tensor([9, 0, 0,  ..., 3, 0, 5])</pre> In\u00a0[96]: Copied! <pre>train_set.train_labels.bincount() #frequency of each label, we have balanced class here\n</pre> train_set.train_labels.bincount() #frequency of each label, we have balanced class here Out[96]: <pre>tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])</pre> In\u00a0[97]: Copied! <pre>sample = next(iter(train_set))\n\nlen(sample)\n</pre> sample = next(iter(train_set))  len(sample) Out[97]: <pre>2</pre> In\u00a0[98]: Copied! <pre>image, label = sample\n</pre> image, label = sample In\u00a0[99]: Copied! <pre>plt.imshow(image.squeeze(), cmap='gray')\nprint('label:', label)\n</pre> plt.imshow(image.squeeze(), cmap='gray') print('label:', label) <pre>label: 9\n</pre> In\u00a0[100]: Copied! <pre>batch = next(iter(train_loader))\n\nlen(batch), type(batch)\n</pre> batch = next(iter(train_loader))  len(batch), type(batch) Out[100]: <pre>(2, list)</pre> In\u00a0[101]: Copied! <pre>images, labels = batch\n</pre> images, labels = batch In\u00a0[102]: Copied! <pre>images.shape, labels.shape\n</pre> images.shape, labels.shape Out[102]: <pre>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</pre> In\u00a0[103]: Copied! <pre>grid = torchvision.utils.make_grid(images, nrow=10)\nplt.figure(figsize=(15,15))\nplt.imshow(np.transpose(grid, (1,2,0)))\nprint('labels:', labels)\n</pre> grid = torchvision.utils.make_grid(images, nrow=10) plt.figure(figsize=(15,15)) plt.imshow(np.transpose(grid, (1,2,0))) print('labels:', labels) <pre>labels: tensor([5, 3, 2, 9, 4, 2, 1, 0, 8, 7, 3, 0, 9, 3, 1, 7, 2, 1, 4, 3, 9, 0, 6, 9, 2, 9, 3, 5, 3, 9, 7, 8])\n</pre> In\u00a0[104]: Copied! <pre>import torch.nn as nn\n\nclass Network(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.layer = None\n\n  def forward(self, t):\n    t = self.layer(t)\n    return t\n</pre> import torch.nn as nn  class Network(nn.Module):   def __init__(self):     super().__init__()     self.layer = None    def forward(self, t):     t = self.layer(t)     return t  In\u00a0[105]: Copied! <pre>import torch\nimport torchvision # provide access to datasets, models, transforms, utils, etc\nimport torchvision.transforms as transforms\n</pre> import torch import torchvision # provide access to datasets, models, transforms, utils, etc import torchvision.transforms as transforms In\u00a0[106]: Copied! <pre>from torch.utils.data import Dataset\n\n# Dataset is there to be able to interact with DataLoader\n\nclass MyDataset(Dataset):\n  def __init__(self):\n    self.data = (\n        \"This was an amazing product\",\n        \"This was the shittiest product possible\",\n        \"Amazing product, fast delivery\",\n        \"Had to sell my kidney to buy this, and now my life has changed\",\n        \"Good one!\",\n        \"Bad One!\"\n    )\n\n  def __getitem__(self, index):\n    return self.data[index]\n\n  def __len__(self):\n    return len(self.data)\n\nmyData = MyDataset()\n\nfor m in myData:\n  print(m)\n</pre> from torch.utils.data import Dataset  # Dataset is there to be able to interact with DataLoader  class MyDataset(Dataset):   def __init__(self):     self.data = (         \"This was an amazing product\",         \"This was the shittiest product possible\",         \"Amazing product, fast delivery\",         \"Had to sell my kidney to buy this, and now my life has changed\",         \"Good one!\",         \"Bad One!\"     )    def __getitem__(self, index):     return self.data[index]    def __len__(self):     return len(self.data)  myData = MyDataset()  for m in myData:   print(m)   <pre>This was an amazing product\nThis was the shittiest product possible\nAmazing product, fast delivery\nHad to sell my kidney to buy this, and now my life has changed\nGood one!\nBad One!\n</pre> In\u00a0[107]: Copied! <pre>train_loader = torch.utils.data.DataLoader(myData, batch_size = 2, shuffle=True)\n</pre> train_loader = torch.utils.data.DataLoader(myData, batch_size = 2, shuffle=True) In\u00a0[108]: Copied! <pre>for tr in train_loader:\n  print(tr)\n</pre> for tr in train_loader:   print(tr) <pre>['This was an amazing product', 'Good one!']\n['Amazing product, fast delivery', 'Bad One!']\n['This was the shittiest product possible', 'Had to sell my kidney to buy this, and now my life has changed']\n</pre> In\u00a0[109]: Copied! <pre>train_set = torchvision.datasets.FashionMNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform = transforms.Compose([\n          transforms.ToTensor()\n    ])\n)\n</pre> train_set = torchvision.datasets.FashionMNIST(     root='./data',     train=True,     download=True,     transform = transforms.Compose([           transforms.ToTensor()     ]) ) In\u00a0[110]: Copied! <pre>for f in train_set:\n  print(f)\n  break\n</pre> for f in train_set:   print(f)   break <pre>(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n          0.0000, 0.0000, 0.0510, 0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n          0.0039, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118,\n          0.0000, 0.1412, 0.5333, 0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118, 0.0157, 0.0000,\n          0.0000, 0.0118],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235,\n          0.0000, 0.4000, 0.8000, 0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471,\n          0.0392, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.6078, 0.9255, 0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902, 0.3020, 0.5098,\n          0.2824, 0.0588],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n          0.2706, 0.8118, 0.8745, 0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725, 0.5529, 0.3451,\n          0.6745, 0.2588],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0000,\n          0.7843, 0.9098, 0.9098, 0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980, 0.4824, 0.7686,\n          0.8980, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.7176, 0.8824, 0.8471, 0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667, 0.8745, 0.9608,\n          0.6784, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.7569, 0.8941, 0.8549, 0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745, 0.8627, 0.9529,\n          0.7922, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118, 0.0000, 0.0471,\n          0.8588, 0.8627, 0.8314, 0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314, 0.8863, 0.7725,\n          0.8196, 0.2039],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.3882,\n          0.9569, 0.8706, 0.8627, 0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627, 0.9608, 0.4667,\n          0.6549, 0.2196],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.2157,\n          0.9255, 0.8941, 0.9020, 0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510, 0.8510, 0.8196,\n          0.3608, 0.0000],\n         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9294,\n          0.8863, 0.8510, 0.8745, 0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431, 0.8549, 1.0000,\n          0.3020, 0.0000],\n         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2431, 0.5686, 0.8000, 0.8941,\n          0.8118, 0.8353, 0.8667, 0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431, 0.8784, 0.9569,\n          0.6235, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196, 0.7412, 0.8941, 0.8627, 0.8706, 0.8510,\n          0.8863, 0.7843, 0.8039, 0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725, 0.9137, 0.9333,\n          0.8431, 0.0000],\n         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157, 0.8000, 0.8392, 0.8157, 0.8196, 0.7843,\n          0.6235, 0.9608, 0.7569, 0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275, 0.8627, 0.9098,\n          0.9647, 0.0000],\n         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392, 0.8039, 0.8039, 0.8039, 0.8627, 0.9412,\n          0.3137, 0.5882, 1.0000, 0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196, 0.8706, 0.8941,\n          0.8824, 0.0000],\n         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176, 0.9765, 0.8627, 0.7608, 0.8431, 0.8510,\n          0.9451, 0.2549, 0.2863, 0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745, 0.8745, 0.8784,\n          0.8980, 0.1137],\n         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824, 0.8471, 0.7255, 0.7725, 0.8078, 0.7765,\n          0.8353, 0.9412, 0.7647, 0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706, 0.8627, 0.8667,\n          0.9020, 0.2627],\n         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451, 0.7608, 0.7529, 0.7922, 0.8392, 0.8588,\n          0.8667, 0.8627, 0.9255, 0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745, 0.7098, 0.8039,\n          0.8078, 0.4510],\n         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686, 0.8000, 0.8235, 0.8353, 0.8118, 0.8275,\n          0.8235, 0.7843, 0.7686, 0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118, 0.6549, 0.6941,\n          0.8235, 0.3608],\n         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745, 0.6863, 0.7098, 0.7255, 0.7373, 0.7412,\n          0.7373, 0.7569, 0.7765, 0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608, 0.7529, 0.8471,\n          0.6667, 0.0000],\n         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294, 0.9373, 0.9490, 0.9647, 0.9529, 0.9569,\n          0.8667, 0.8627, 0.7569, 0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588, 0.3882, 0.2275,\n          0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.2392, 0.1725, 0.2824, 0.1608, 0.1373,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000]]]), 9)\n</pre> In\u00a0[111]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\ntorch.set_printoptions(linewidth=120)\n\nlen(train_set)\n</pre> import numpy as np import matplotlib.pyplot as plt  torch.set_printoptions(linewidth=120)  len(train_set) Out[111]: <pre>60000</pre> In\u00a0[112]: Copied! <pre>train_set.train_labels\n</pre> train_set.train_labels Out[112]: <pre>tensor([9, 0, 0,  ..., 3, 0, 5])</pre> In\u00a0[113]: Copied! <pre>dir(train_set)\n</pre> dir(train_set) Out[113]: <pre>['__add__',\n '__annotations__',\n '__class__',\n '__class_getitem__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__len__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__orig_bases__',\n '__parameters__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_check_exists',\n '_check_legacy_exist',\n '_format_transform_repr',\n '_load_data',\n '_load_legacy_data',\n '_repr_indent',\n 'class_to_idx',\n 'classes',\n 'data',\n 'download',\n 'extra_repr',\n 'mirrors',\n 'processed_folder',\n 'raw_folder',\n 'resources',\n 'root',\n 'target_transform',\n 'targets',\n 'test_data',\n 'test_file',\n 'test_labels',\n 'train',\n 'train_data',\n 'train_labels',\n 'training_file',\n 'transform',\n 'transforms']</pre> In\u00a0[114]: Copied! <pre>train_set.train_labels.bincount()\n</pre> train_set.train_labels.bincount() Out[114]: <pre>tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])</pre> In\u00a0[115]: Copied! <pre>sample = next(iter(train_set))\nsample\n</pre> sample = next(iter(train_set)) sample Out[115]: <pre>(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n           0.0000, 0.0000, 0.0510, 0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n           0.0039, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118,\n           0.0000, 0.1412, 0.5333, 0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118, 0.0157, 0.0000,\n           0.0000, 0.0118],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235,\n           0.0000, 0.4000, 0.8000, 0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471,\n           0.0392, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.6078, 0.9255, 0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902, 0.3020, 0.5098,\n           0.2824, 0.0588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n           0.2706, 0.8118, 0.8745, 0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725, 0.5529, 0.3451,\n           0.6745, 0.2588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0000,\n           0.7843, 0.9098, 0.9098, 0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980, 0.4824, 0.7686,\n           0.8980, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.7176, 0.8824, 0.8471, 0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667, 0.8745, 0.9608,\n           0.6784, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.7569, 0.8941, 0.8549, 0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745, 0.8627, 0.9529,\n           0.7922, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118, 0.0000, 0.0471,\n           0.8588, 0.8627, 0.8314, 0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314, 0.8863, 0.7725,\n           0.8196, 0.2039],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.3882,\n           0.9569, 0.8706, 0.8627, 0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627, 0.9608, 0.4667,\n           0.6549, 0.2196],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.2157,\n           0.9255, 0.8941, 0.9020, 0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510, 0.8510, 0.8196,\n           0.3608, 0.0000],\n          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9294,\n           0.8863, 0.8510, 0.8745, 0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431, 0.8549, 1.0000,\n           0.3020, 0.0000],\n          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2431, 0.5686, 0.8000, 0.8941,\n           0.8118, 0.8353, 0.8667, 0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431, 0.8784, 0.9569,\n           0.6235, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196, 0.7412, 0.8941, 0.8627, 0.8706, 0.8510,\n           0.8863, 0.7843, 0.8039, 0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725, 0.9137, 0.9333,\n           0.8431, 0.0000],\n          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157, 0.8000, 0.8392, 0.8157, 0.8196, 0.7843,\n           0.6235, 0.9608, 0.7569, 0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275, 0.8627, 0.9098,\n           0.9647, 0.0000],\n          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392, 0.8039, 0.8039, 0.8039, 0.8627, 0.9412,\n           0.3137, 0.5882, 1.0000, 0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196, 0.8706, 0.8941,\n           0.8824, 0.0000],\n          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176, 0.9765, 0.8627, 0.7608, 0.8431, 0.8510,\n           0.9451, 0.2549, 0.2863, 0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745, 0.8745, 0.8784,\n           0.8980, 0.1137],\n          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824, 0.8471, 0.7255, 0.7725, 0.8078, 0.7765,\n           0.8353, 0.9412, 0.7647, 0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706, 0.8627, 0.8667,\n           0.9020, 0.2627],\n          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451, 0.7608, 0.7529, 0.7922, 0.8392, 0.8588,\n           0.8667, 0.8627, 0.9255, 0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745, 0.7098, 0.8039,\n           0.8078, 0.4510],\n          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686, 0.8000, 0.8235, 0.8353, 0.8118, 0.8275,\n           0.8235, 0.7843, 0.7686, 0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118, 0.6549, 0.6941,\n           0.8235, 0.3608],\n          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745, 0.6863, 0.7098, 0.7255, 0.7373, 0.7412,\n           0.7373, 0.7569, 0.7765, 0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608, 0.7529, 0.8471,\n           0.6667, 0.0000],\n          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294, 0.9373, 0.9490, 0.9647, 0.9529, 0.9569,\n           0.8667, 0.8627, 0.7569, 0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588, 0.3882, 0.2275,\n           0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.2392, 0.1725, 0.2824, 0.1608, 0.1373,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000]]]),\n 9)</pre> In\u00a0[116]: Copied! <pre>image, label = sample\nimage.size()\n</pre> image, label = sample image.size() Out[116]: <pre>torch.Size([1, 28, 28])</pre> In\u00a0[117]: Copied! <pre>plt.imshow(image.squeeze(), cmap='gray')\nprint('label:', label)\n</pre> plt.imshow(image.squeeze(), cmap='gray') print('label:', label) <pre>label: 9\n</pre> In\u00a0[118]: Copied! <pre>train_loader = torch.utils.data.DataLoader(\n    train_set,\n    batch_size = 32,\n    shuffle=True\n)\n</pre> train_loader = torch.utils.data.DataLoader(     train_set,     batch_size = 32,     shuffle=True ) In\u00a0[119]: Copied! <pre>for tl in train_loader:\n  print(len(tl[0]))\n  print(tl[1])\n  break\n</pre> for tl in train_loader:   print(len(tl[0]))   print(tl[1])   break <pre>32\ntensor([3, 2, 7, 0, 5, 5, 9, 0, 0, 5, 9, 1, 9, 9, 1, 3, 0, 2, 5, 3, 0, 6, 4, 9, 4, 1, 0, 6, 7, 3, 3, 5])\n</pre> In\u00a0[120]: Copied! <pre>batch = next(iter(train_loader))\nbatch\n</pre> batch = next(iter(train_loader)) batch Out[120]: <pre>[tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n \n \n         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n \n \n         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n \n \n         ...,\n \n \n         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n \n \n         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n \n \n         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]),\n tensor([0, 9, 5, 1, 7, 2, 5, 4, 2, 9, 4, 4, 1, 7, 4, 8, 4, 3, 6, 7, 0, 2, 4, 5, 6, 0, 6, 7, 2, 4, 4, 5])]</pre> In\u00a0[121]: Copied! <pre>images, labels = batch\n\nimages.shape, labels.shape\n</pre> images, labels = batch  images.shape, labels.shape Out[121]: <pre>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</pre> In\u00a0[122]: Copied! <pre>grid = torchvision.utils.make_grid(images, nrow=10)\nplt.figure(figsize=(15,15))\nplt.imshow(np.transpose(grid, (1,2,0)))\nprint('labels:', labels)\n</pre> grid = torchvision.utils.make_grid(images, nrow=10) plt.figure(figsize=(15,15)) plt.imshow(np.transpose(grid, (1,2,0))) print('labels:', labels) <pre>labels: tensor([0, 9, 5, 1, 7, 2, 5, 4, 2, 9, 4, 4, 1, 7, 4, 8, 4, 3, 6, 7, 0, 2, 4, 5, 6, 0, 6, 7, 2, 4, 4, 5])\n</pre> In\u00a0[123]: Copied! <pre>import torch.nn as nn\n\nclass Network(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.layer = None\n\n  def forward(self, t):\n    t = self.layer(t)\n    return t\n</pre> import torch.nn as nn  class Network(nn.Module):   def __init__(self):     super().__init__()     self.layer = None    def forward(self, t):     t = self.layer(t)     return t In\u00a0[124]: Copied! <pre>class Network(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n    # input 28 # output 24 # receptive_field = 5\n    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n    # input 24 # output 20 # receptive_field = 9\n    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n    # input 12x20x20, output 120\n    # input 10*512\n    self.fc1 = nn.Linear(in_features=12*20*20, out_features=120)\n    self.fc2 = nn.Linear(in_features=120, out_features=60)\n    self.out = nn.Linear(in_features=60, out_features=10)\n\n  def forward(self, t):\n    return t\n</pre> class Network(nn.Module):   def __init__(self):     super().__init__()      # input 28 # output 24 # receptive_field = 5     self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)     # input 24 # output 20 # receptive_field = 9     self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)     # input 12x20x20, output 120     # input 10*512     self.fc1 = nn.Linear(in_features=12*20*20, out_features=120)     self.fc2 = nn.Linear(in_features=120, out_features=60)     self.out = nn.Linear(in_features=60, out_features=10)    def forward(self, t):     return t  In\u00a0[125]: Copied! <pre>network = Network()\n\nprint(network)\n</pre> network = Network()  print(network) <pre>Network(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=4800, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=10, bias=True)\n)\n</pre> In\u00a0[126]: Copied! <pre>network.fc2\n</pre> network.fc2 Out[126]: <pre>Linear(in_features=120, out_features=60, bias=True)</pre> In\u00a0[127]: Copied! <pre>dir(network.fc2)\n</pre> dir(network.fc2) Out[127]: <pre>['T_destination',\n '__annotations__',\n '__call__',\n '__class__',\n '__constants__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattr__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_apply',\n '_backward_hooks',\n '_backward_pre_hooks',\n '_buffers',\n '_call_impl',\n '_compiled_call_impl',\n '_forward_hooks',\n '_forward_hooks_always_called',\n '_forward_hooks_with_kwargs',\n '_forward_pre_hooks',\n '_forward_pre_hooks_with_kwargs',\n '_get_backward_hooks',\n '_get_backward_pre_hooks',\n '_get_name',\n '_is_full_backward_hook',\n '_load_from_state_dict',\n '_load_state_dict_post_hooks',\n '_load_state_dict_pre_hooks',\n '_maybe_warn_non_full_backward_hook',\n '_modules',\n '_named_members',\n '_non_persistent_buffers_set',\n '_parameters',\n '_register_load_state_dict_pre_hook',\n '_register_state_dict_hook',\n '_replicate_for_data_parallel',\n '_save_to_state_dict',\n '_slow_forward',\n '_state_dict_hooks',\n '_state_dict_pre_hooks',\n '_version',\n '_wrapped_call_impl',\n 'add_module',\n 'apply',\n 'bfloat16',\n 'bias',\n 'buffers',\n 'call_super_init',\n 'children',\n 'compile',\n 'cpu',\n 'cuda',\n 'double',\n 'dump_patches',\n 'eval',\n 'extra_repr',\n 'float',\n 'forward',\n 'get_buffer',\n 'get_extra_state',\n 'get_parameter',\n 'get_submodule',\n 'half',\n 'in_features',\n 'ipu',\n 'load_state_dict',\n 'modules',\n 'mtia',\n 'named_buffers',\n 'named_children',\n 'named_modules',\n 'named_parameters',\n 'out_features',\n 'parameters',\n 'register_backward_hook',\n 'register_buffer',\n 'register_forward_hook',\n 'register_forward_pre_hook',\n 'register_full_backward_hook',\n 'register_full_backward_pre_hook',\n 'register_load_state_dict_post_hook',\n 'register_load_state_dict_pre_hook',\n 'register_module',\n 'register_parameter',\n 'register_state_dict_post_hook',\n 'register_state_dict_pre_hook',\n 'requires_grad_',\n 'reset_parameters',\n 'set_extra_state',\n 'set_submodule',\n 'share_memory',\n 'state_dict',\n 'to',\n 'to_empty',\n 'train',\n 'training',\n 'type',\n 'weight',\n 'xpu',\n 'zero_grad']</pre> In\u00a0[128]: Copied! <pre>network.fc2.weight\n</pre> network.fc2.weight Out[128]: <pre>Parameter containing:\ntensor([[-0.0024, -0.0301, -0.0447,  ...,  0.0061,  0.0608,  0.0019],\n        [-0.0758, -0.0316,  0.0891,  ...,  0.0329,  0.0138,  0.0295],\n        [ 0.0255, -0.0554, -0.0385,  ..., -0.0383, -0.0085, -0.0092],\n        ...,\n        [-0.0726,  0.0701, -0.0281,  ...,  0.0302,  0.0340,  0.0205],\n        [-0.0516, -0.0552,  0.0227,  ...,  0.0829, -0.0786,  0.0421],\n        [ 0.0532,  0.0505, -0.0278,  ..., -0.0590, -0.0640,  0.0709]], requires_grad=True)</pre> In\u00a0[129]: Copied! <pre>network.fc2.weight.shape\n</pre> network.fc2.weight.shape Out[129]: <pre>torch.Size([60, 120])</pre> In\u00a0[130]: Copied! <pre>class Network(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n    self.fc1 = nn.Linear(in_features=12*20*20, out_features=120)\n    self.fc2 = nn.Linear(in_features=120, out_features=60)\n    self.out = nn.Linear(in_features=60, out_features=10)\n\n  def forward(self, t):\n    # TODO implement this\n    return t\n\nnetwork = Network()\n\nfor name, param in network.named_parameters():\n  print(name, '\\t\\t', param.shape)\n</pre> class Network(nn.Module):   def __init__(self):     super().__init__()     self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)     self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)     self.fc1 = nn.Linear(in_features=12*20*20, out_features=120)     self.fc2 = nn.Linear(in_features=120, out_features=60)     self.out = nn.Linear(in_features=60, out_features=10)    def forward(self, t):     # TODO implement this     return t  network = Network()  for name, param in network.named_parameters():   print(name, '\\t\\t', param.shape) <pre>conv1.weight \t\t torch.Size([6, 1, 5, 5])\nconv1.bias \t\t torch.Size([6])\nconv2.weight \t\t torch.Size([12, 6, 5, 5])\nconv2.bias \t\t torch.Size([12])\nfc1.weight \t\t torch.Size([120, 4800])\nfc1.bias \t\t torch.Size([120])\nfc2.weight \t\t torch.Size([60, 120])\nfc2.bias \t\t torch.Size([60])\nout.weight \t\t torch.Size([10, 60])\nout.bias \t\t torch.Size([10])\n</pre> In\u00a0[131]: Copied! <pre>import torch.nn.functional as F\n</pre> import torch.nn.functional as F In\u00a0[132]: Copied! <pre>class Network(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n    self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n    self.fc2 = nn.Linear(in_features=120, out_features=60)\n    self.out = nn.Linear(in_features=60, out_features=10)\n\n  def forward(self, t):\n    # input layer\n    x = t\n\n    # conv1 layer\n    x = self.conv1(x)\n    x = F.relu(x)\n    x = F.max_pool2d(x, kernel_size=2, stride=2) # 28 | 24 | 12\n\n    # conv2 layer\n    x = self.conv2(x)\n    x = F.relu(x)\n    x = F.max_pool2d(x, kernel_size=2, stride=2) # 12 | 8 | 4 &gt;&gt; 12x4x4\n\n    # reshapre\n    x = x.reshape(-1, 12 * 4 * 4)\n\n    # fc1 layer\n    x = self.fc1(x)\n    x = F.relu(x)\n\n    # fc2 layer\n    x = self.fc2(x)\n    x = F.relu(x)\n\n    # output layer\n    x = self.out(x)\n    # x = F.softmax(x, dim=1)\n    return x\n</pre>  class Network(nn.Module):   def __init__(self):     super().__init__()     self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)     self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)     self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)     self.fc2 = nn.Linear(in_features=120, out_features=60)     self.out = nn.Linear(in_features=60, out_features=10)    def forward(self, t):     # input layer     x = t      # conv1 layer     x = self.conv1(x)     x = F.relu(x)     x = F.max_pool2d(x, kernel_size=2, stride=2) # 28 | 24 | 12      # conv2 layer     x = self.conv2(x)     x = F.relu(x)     x = F.max_pool2d(x, kernel_size=2, stride=2) # 12 | 8 | 4 &gt;&gt; 12x4x4      # reshapre     x = x.reshape(-1, 12 * 4 * 4)      # fc1 layer     x = self.fc1(x)     x = F.relu(x)      # fc2 layer     x = self.fc2(x)     x = F.relu(x)      # output layer     x = self.out(x)     # x = F.softmax(x, dim=1)     return x In\u00a0[133]: Copied! <pre>torch.set_grad_enabled(False)\n</pre> torch.set_grad_enabled(False) Out[133]: <pre>torch.autograd.grad_mode.set_grad_enabled(mode=False)</pre> In\u00a0[134]: Copied! <pre>sample = next(iter(train_set))\nimage, label = sample\nimage.shape, image.unsqueeze(0).shape\n</pre> sample = next(iter(train_set)) image, label = sample image.shape, image.unsqueeze(0).shape  Out[134]: <pre>(torch.Size([1, 28, 28]), torch.Size([1, 1, 28, 28]))</pre> In\u00a0[135]: Copied! <pre>network = Network()\n</pre> network = Network() In\u00a0[136]: Copied! <pre>pred = network(image)\n</pre> pred = network(image) In\u00a0[137]: Copied! <pre>pred = network(image.unsqueeze(0))\n</pre> pred = network(image.unsqueeze(0)) In\u00a0[138]: Copied! <pre>pred, pred.shape, label\n</pre> pred, pred.shape, label Out[138]: <pre>(tensor([[-0.1297,  0.1155, -0.1121, -0.0228, -0.0479,  0.1392, -0.0423,  0.1030,  0.0500, -0.0057]]),\n torch.Size([1, 10]),\n 9)</pre> In\u00a0[139]: Copied! <pre>pred.argmax(dim=1)\n</pre> pred.argmax(dim=1) Out[139]: <pre>tensor([5])</pre> In\u00a0[140]: Copied! <pre>F.softmax(pred, dim=1)\n</pre> F.softmax(pred, dim=1) Out[140]: <pre>tensor([[0.0871, 0.1113, 0.0886, 0.0969, 0.0945, 0.1139, 0.0950, 0.1099, 0.1042, 0.0986]])</pre> In\u00a0[141]: Copied! <pre>F.softmax(pred, dim=1).sum()\n</pre> F.softmax(pred, dim=1).sum() Out[141]: <pre>tensor(1.0000)</pre> In\u00a0[142]: Copied! <pre>data_loader = torch.utils.data.DataLoader(\n    train_set,\n    batch_size=10\n)\n</pre> data_loader = torch.utils.data.DataLoader(     train_set,     batch_size=10 ) In\u00a0[143]: Copied! <pre>batch = next(iter(data_loader))\nlen(batch[0])\n</pre> batch = next(iter(data_loader)) len(batch[0]) Out[143]: <pre>10</pre> In\u00a0[144]: Copied! <pre>images, labels = batch\n\npreds = network(images)\npreds.shape\n</pre> images, labels = batch  preds = network(images) preds.shape Out[144]: <pre>torch.Size([10, 10])</pre> In\u00a0[145]: Copied! <pre>preds\n</pre> preds Out[145]: <pre>tensor([[-0.1297,  0.1155, -0.1121, -0.0228, -0.0479,  0.1392, -0.0423,  0.1030,  0.0500, -0.0057],\n        [-0.1293,  0.1135, -0.1323, -0.0256, -0.0560,  0.1409, -0.0231,  0.0923,  0.0568,  0.0019],\n        [-0.1390,  0.1138, -0.1026, -0.0225, -0.0505,  0.1444, -0.0190,  0.0840,  0.0458, -0.0057],\n        [-0.1371,  0.1163, -0.1132, -0.0249, -0.0517,  0.1496, -0.0249,  0.0857,  0.0499,  0.0024],\n        [-0.1417,  0.1132, -0.1152, -0.0185, -0.0493,  0.1377, -0.0271,  0.0966,  0.0590, -0.0010],\n        [-0.1343,  0.1178, -0.1190, -0.0302, -0.0510,  0.1429, -0.0318,  0.0925,  0.0534, -0.0034],\n        [-0.1364,  0.1232, -0.1118, -0.0322, -0.0455,  0.1371, -0.0413,  0.0926,  0.0373, -0.0039],\n        [-0.1343,  0.1130, -0.1202, -0.0235, -0.0538,  0.1393, -0.0298,  0.0971,  0.0584, -0.0008],\n        [-0.1340,  0.1120, -0.1006, -0.0214, -0.0488,  0.1418, -0.0308,  0.0983,  0.0442, -0.0093],\n        [-0.1301,  0.1099, -0.1090, -0.0211, -0.0505,  0.1407, -0.0470,  0.1018,  0.0450, -0.0088]])</pre> In\u00a0[146]: Copied! <pre>preds.argmax(dim=1)\n</pre> preds.argmax(dim=1) Out[146]: <pre>tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5])</pre> In\u00a0[147]: Copied! <pre>preds.argmax(dim=1).eq(labels)\n</pre> preds.argmax(dim=1).eq(labels) Out[147]: <pre>tensor([False, False, False, False, False, False, False, False,  True,  True])</pre> In\u00a0[148]: Copied! <pre>def get_num_correct(preds, labels):\n  return preds.argmax(dim=1).eq(labels).sum().item()\n</pre> def get_num_correct(preds, labels):   return preds.argmax(dim=1).eq(labels).sum().item() In\u00a0[149]: Copied! <pre>get_num_correct(preds, labels)\n</pre> get_num_correct(preds, labels) Out[149]: <pre>2</pre> In\u00a0[150]: Copied! <pre>import torch.optim as optim\n\ntorch.set_grad_enabled(True)\n</pre> import torch.optim as optim  torch.set_grad_enabled(True) Out[150]: <pre>torch.autograd.grad_mode.set_grad_enabled(mode=True)</pre> In\u00a0[151]: Copied! <pre>preds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item()\n</pre> preds = network(images) loss = F.cross_entropy(preds, labels) loss.item() Out[151]: <pre>2.3412976264953613</pre> In\u00a0[152]: Copied! <pre>print(network.conv1.weight.grad)\n</pre> print(network.conv1.weight.grad) <pre>None\n</pre> In\u00a0[153]: Copied! <pre>loss.backward()\n</pre> loss.backward() In\u00a0[154]: Copied! <pre>print(network.conv1.weight.grad)\n</pre> print(network.conv1.weight.grad) <pre>tensor([[[[-5.1717e-04,  1.3957e-05, -7.7923e-04, -2.7695e-04, -6.8297e-05],\n          [-9.3553e-04, -7.3059e-04, -4.0039e-04,  3.3794e-04, -6.5965e-04],\n          [-1.0657e-03, -1.6296e-04,  7.9522e-05,  1.9946e-04, -8.7838e-04],\n          [ 2.6466e-05, -3.4090e-04, -7.8631e-04, -1.8676e-04,  5.3126e-05],\n          [-1.9878e-04, -9.8055e-04, -3.3847e-04,  4.5497e-04,  9.8605e-05]]],\n\n\n        [[[-8.0822e-04,  9.1748e-05,  7.5057e-04,  1.4063e-03,  5.5890e-04],\n          [ 5.0223e-04,  9.8965e-04,  6.8474e-07, -8.2789e-04,  1.2745e-03],\n          [ 1.2638e-04, -3.8300e-05, -6.8525e-04,  8.3018e-04,  1.4896e-03],\n          [ 1.1954e-03,  1.9945e-04,  7.8463e-05,  2.5624e-03,  2.2907e-03],\n          [-7.3374e-04, -8.3763e-04,  1.5492e-04,  2.2261e-03,  1.0759e-03]]],\n\n\n        [[[ 3.2059e-03,  2.0534e-03, -1.1834e-03, -3.2651e-03, -2.3879e-03],\n          [ 3.8577e-03,  2.2973e-03,  4.1454e-04, -9.6174e-04, -5.2301e-04],\n          [ 4.4892e-03,  4.1366e-03,  2.0622e-03, -7.3847e-04, -1.1253e-04],\n          [ 4.3328e-03,  4.7003e-03,  2.6987e-03, -4.4640e-04,  3.2431e-04],\n          [ 4.1801e-03,  5.1289e-03,  1.2794e-03,  3.5095e-04, -4.5822e-04]]],\n\n\n        [[[-2.7648e-06, -7.3162e-05, -4.5165e-05,  4.5164e-04,  5.2210e-04],\n          [ 0.0000e+00,  1.8269e-06,  4.1933e-04,  4.5737e-04,  9.1759e-06],\n          [-5.2106e-08,  3.3891e-04,  2.9576e-04,  7.7655e-06,  5.3805e-05],\n          [ 3.2477e-04,  2.6032e-04, -5.9319e-06, -3.8151e-06,  5.1509e-05],\n          [ 5.0492e-04, -1.1429e-05,  0.0000e+00,  1.1927e-06,  9.2046e-05]]],\n\n\n        [[[-1.3746e-05,  9.7549e-05,  2.4252e-04,  1.2119e-03,  7.0328e-04],\n          [ 6.2048e-05,  1.6621e-04,  8.7157e-04,  1.5080e-03,  5.6065e-04],\n          [ 8.8135e-05,  5.8921e-04,  1.0466e-03,  1.5948e-03,  4.1897e-04],\n          [ 2.7651e-04,  6.7448e-04,  7.9527e-04,  8.1507e-04,  3.7357e-04],\n          [ 7.2552e-04,  7.6179e-04,  5.7954e-04,  1.8578e-03,  3.1224e-04]]],\n\n\n        [[[ 7.4986e-03,  6.0435e-03,  4.0292e-03,  2.2515e-03,  3.3096e-03],\n          [ 6.8959e-03,  4.7121e-03,  3.1211e-03,  4.8291e-03,  4.3495e-03],\n          [ 7.0215e-03,  5.7440e-03,  5.5285e-03,  7.9680e-03,  7.3406e-03],\n          [ 6.9059e-03,  7.2099e-03,  7.9754e-03,  9.0368e-03,  7.3487e-03],\n          [ 6.7520e-03,  7.1282e-03,  9.0853e-03,  1.0633e-02,  8.2597e-03]]]])\n</pre> In\u00a0[155]: Copied! <pre>print(network.conv1.weight.grad.shape)\n</pre> print(network.conv1.weight.grad.shape) <pre>torch.Size([6, 1, 5, 5])\n</pre> In\u00a0[156]: Copied! <pre>optimizer = optim.Adam(network.parameters(), lr = 0.01)\n</pre> optimizer = optim.Adam(network.parameters(), lr = 0.01) In\u00a0[157]: Copied! <pre>loss.item(), get_num_correct(preds, labels)\n</pre> loss.item(), get_num_correct(preds, labels) Out[157]: <pre>(2.3412976264953613, 2)</pre> In\u00a0[158]: Copied! <pre>optimizer.step()\n</pre> optimizer.step() In\u00a0[159]: Copied! <pre>preds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item(), get_num_correct(preds, labels)\n</pre> preds = network(images) loss = F.cross_entropy(preds, labels) loss.item(), get_num_correct(preds, labels) Out[159]: <pre>(2.282127857208252, 2)</pre> In\u00a0[160]: Copied! <pre>loss.backward()\noptimizer.step()\npreds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item(), get_num_correct(preds, labels)\n</pre> loss.backward() optimizer.step() preds = network(images) loss = F.cross_entropy(preds, labels) loss.item(), get_num_correct(preds, labels) Out[160]: <pre>(2.2083067893981934, 2)</pre> In\u00a0[161]: Copied! <pre>loss.backward()\noptimizer.step()\npreds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item(), get_num_correct(preds, labels)\n</pre> loss.backward() optimizer.step() preds = network(images) loss = F.cross_entropy(preds, labels) loss.item(), get_num_correct(preds, labels) Out[161]: <pre>(2.00101900100708, 4)</pre> In\u00a0[162]: Copied! <pre>loss.backward()\noptimizer.step()\npreds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item(), get_num_correct(preds, labels)\n</pre> loss.backward() optimizer.step() preds = network(images) loss = F.cross_entropy(preds, labels) loss.item(), get_num_correct(preds, labels) Out[162]: <pre>(1.8126885890960693, 3)</pre> In\u00a0[163]: Copied! <pre>loss.backward()\noptimizer.zero_grad()\noptimizer.step()\npreds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item(), get_num_correct(preds, labels)\n</pre> loss.backward() optimizer.zero_grad() optimizer.step() preds = network(images) loss = F.cross_entropy(preds, labels) loss.item(), get_num_correct(preds, labels) Out[163]: <pre>(1.8126885890960693, 3)</pre> In\u00a0[164]: Copied! <pre>loss.backward()\noptimizer.zero_grad()\noptimizer.step()\npreds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item(), get_num_correct(preds, labels)\n</pre> loss.backward() optimizer.zero_grad() optimizer.step() preds = network(images) loss = F.cross_entropy(preds, labels) loss.item(), get_num_correct(preds, labels) Out[164]: <pre>(1.8126885890960693, 3)</pre> In\u00a0[165]: Copied! <pre>loss.backward()\noptimizer.zero_grad()\noptimizer.step()\npreds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item(), get_num_correct(preds, labels)\n</pre> loss.backward() optimizer.zero_grad() optimizer.step() preds = network(images) loss = F.cross_entropy(preds, labels) loss.item(), get_num_correct(preds, labels) Out[165]: <pre>(1.8126885890960693, 3)</pre> In\u00a0[166]: Copied! <pre>loss.backward()\noptimizer.zero_grad()\noptimizer.step()\npreds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item(), get_num_correct(preds, labels)\n</pre> loss.backward() optimizer.zero_grad() optimizer.step() preds = network(images) loss = F.cross_entropy(preds, labels) loss.item(), get_num_correct(preds, labels) Out[166]: <pre>(1.8126885890960693, 3)</pre> In\u00a0[167]: Copied! <pre>loss.backward()\noptimizer.zero_grad()\noptimizer.step()\npreds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item(), get_num_correct(preds, labels)\n</pre> loss.backward() optimizer.zero_grad() optimizer.step() preds = network(images) loss = F.cross_entropy(preds, labels) loss.item(), get_num_correct(preds, labels) Out[167]: <pre>(1.8126885890960693, 3)</pre> In\u00a0[168]: Copied! <pre>loss.backward()\noptimizer.zero_grad()\noptimizer.step()\npreds = network(images)\nloss = F.cross_entropy(preds, labels)\nloss.item(), get_num_correct(preds, labels)\n</pre> loss.backward() optimizer.zero_grad() optimizer.step() preds = network(images) loss = F.cross_entropy(preds, labels) loss.item(), get_num_correct(preds, labels) Out[168]: <pre>(1.8126885890960693, 3)</pre> In\u00a0[169]: Copied! <pre>network = Network()\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\noptimizer = optim.Adam(network.parameters(), lr=0.01)\n\nbatch = next(iter(train_loader)) # Get Batch\nimages, labels = batch\n\npreds = network(images) # Pass Batch\nloss = F.cross_entropy(preds, labels) # Calculate Loss\n\nloss.backward() # Calculate Gradients\noptimizer.step() # Update Weights\n\nprint('loss1:', loss.item())\npreds = network(images)\nloss = F.cross_entropy(preds, labels)\nprint('loss2:', loss.item())\n</pre> network = Network()  train_loader = torch.utils.data.DataLoader(train_set, batch_size=100) optimizer = optim.Adam(network.parameters(), lr=0.01)  batch = next(iter(train_loader)) # Get Batch images, labels = batch  preds = network(images) # Pass Batch loss = F.cross_entropy(preds, labels) # Calculate Loss  loss.backward() # Calculate Gradients optimizer.step() # Update Weights  print('loss1:', loss.item()) preds = network(images) loss = F.cross_entropy(preds, labels) print('loss2:', loss.item()) <pre>loss1: 2.3089022636413574\nloss2: 2.271914005279541\n</pre> In\u00a0[170]: Copied! <pre>torch.set_grad_enabled(False)\ntorch.set_grad_enabled(True)\n</pre> torch.set_grad_enabled(False) torch.set_grad_enabled(True) Out[170]: <pre>torch.autograd.grad_mode.set_grad_enabled(mode=True)</pre> In\u00a0[171]: Copied! <pre>network = Network()\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\noptimizer = optim.Adam(network.parameters(), lr=0.01)\n\ntotal_loss = 0\ntotal_correct = 0\n\nfor batch in train_loader: # Get Batch\n    images, labels = batch\n\n    preds = network(images) # Pass Batch\n    loss = F.cross_entropy(preds, labels) # Calculate Loss\n\n    optimizer.zero_grad()\n    loss.backward() # Calculate Gradients\n    optimizer.step() # Update Weights\n\n    total_loss += loss.item()\n    total_correct += get_num_correct(preds, labels)\n\nprint(\n    \"epoch:\", 0,\n    \"total_correct:\", total_correct,\n    \"loss:\", total_loss\n)\n</pre> network = Network()  train_loader = torch.utils.data.DataLoader(train_set, batch_size=100) optimizer = optim.Adam(network.parameters(), lr=0.01)  total_loss = 0 total_correct = 0  for batch in train_loader: # Get Batch     images, labels = batch      preds = network(images) # Pass Batch     loss = F.cross_entropy(preds, labels) # Calculate Loss      optimizer.zero_grad()     loss.backward() # Calculate Gradients     optimizer.step() # Update Weights      total_loss += loss.item()     total_correct += get_num_correct(preds, labels)  print(     \"epoch:\", 0,     \"total_correct:\", total_correct,     \"loss:\", total_loss ) <pre>epoch: 0 total_correct: 47656 loss: 327.9988403618336\n</pre> In\u00a0[172]: Copied! <pre>train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\noptimizer = optim.Adam(network.parameters(), lr=0.01)\n\nfor epoch in range(10):\n\n    total_loss = 0\n    total_correct = 0\n\n    for batch in train_loader: # Get Batch\n        images, labels = batch\n\n        preds = network(images) # Pass Batch\n        loss = F.cross_entropy(preds, labels) # Calculate Loss\n\n        optimizer.zero_grad()\n        loss.backward() # Calculate Gradients\n        optimizer.step() # Update Weights\n\n        total_loss += loss.item()\n        total_correct += get_num_correct(preds, labels)\n\n    print(\n        \"epoch\", epoch,\n        \"total_correct:\", total_correct,\n        \"loss:\", total_loss\n    )\n</pre> train_loader = torch.utils.data.DataLoader(train_set, batch_size=100) optimizer = optim.Adam(network.parameters(), lr=0.01)  for epoch in range(10):      total_loss = 0     total_correct = 0      for batch in train_loader: # Get Batch         images, labels = batch          preds = network(images) # Pass Batch         loss = F.cross_entropy(preds, labels) # Calculate Loss          optimizer.zero_grad()         loss.backward() # Calculate Gradients         optimizer.step() # Update Weights          total_loss += loss.item()         total_correct += get_num_correct(preds, labels)      print(         \"epoch\", epoch,         \"total_correct:\", total_correct,         \"loss:\", total_loss     ) <pre>epoch 0 total_correct: 51414 loss: 230.6321178972721\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n/tmp/ipython-input-4042147717.py in &lt;cell line: 0&gt;()\n      7     total_correct = 0\n      8 \n----&gt; 9     for batch in train_loader: # Get Batch\n     10         images, labels = batch\n     11 \n\n/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py in __next__(self)\n    730                 # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    731                 self._reset()  # type: ignore[call-arg]\n--&gt; 732             data = self._next_data()\n    733             self._num_yielded += 1\n    734             if (\n\n/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py in _next_data(self)\n    786     def _next_data(self):\n    787         index = self._next_index()  # may raise StopIteration\n--&gt; 788         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    789         if self._pin_memory:\n    790             data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n\n/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)\n     50                 data = self.dataset.__getitems__(possibly_batched_index)\n     51             else:\n---&gt; 52                 data = [self.dataset[idx] for idx in possibly_batched_index]\n     53         else:\n     54             data = self.dataset[possibly_batched_index]\n\n/usr/local/lib/python3.12/dist-packages/torchvision/datasets/mnist.py in __getitem__(self, index)\n    144 \n    145         if self.transform is not None:\n--&gt; 146             img = self.transform(img)\n    147 \n    148         if self.target_transform is not None:\n\n/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py in __call__(self, img)\n     93     def __call__(self, img):\n     94         for t in self.transforms:\n---&gt; 95             img = t(img)\n     96         return img\n     97 \n\n/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py in __call__(self, pic)\n    135             Tensor: Converted image.\n    136         \"\"\"\n--&gt; 137         return F.to_tensor(pic)\n    138 \n    139     def __repr__(self) -&gt; str:\n\n/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py in to_tensor(pic)\n    166     # handle PIL Image\n    167     mode_to_nptype = {\"I\": np.int32, \"I;16\" if sys.byteorder == \"little\" else \"I;16B\": np.int16, \"F\": np.float32}\n--&gt; 168     img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))\n    169 \n    170     if pic.mode == \"1\":\n\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py in __array_interface__(self)\n    733             new[\"data\"] = self.tobytes(\"raw\", \"L\")\n    734         else:\n--&gt; 735             new[\"data\"] = self.tobytes()\n    736         new[\"shape\"], new[\"typestr\"] = _conv_type_shape(self)\n    737         return new\n\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py in tobytes(self, encoder_name, *args)\n    800 \n    801         # unpack data\n--&gt; 802         e = _getencoder(self.mode, encoder_name, encoder_args)\n    803         e.setimage(self.im)\n    804 \n\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py in _getencoder(mode, encoder_name, args, extra)\n    455     try:\n    456         # get encoder\n--&gt; 457         encoder = getattr(core, f\"{encoder_name}_encoder\")\n    458     except AttributeError as e:\n    459         msg = f\"encoder {encoder_name} not available\"\n\nKeyboardInterrupt: </pre> <p>Conv2 &gt;&gt; 12 * 4 * 4 Conv2.reshape &gt;&gt; 192 random_number input &gt;&gt; 10 fc_rn (10, 20) &gt;&gt; 20 concat_data = concatenation (192, 20) combined_data &gt;&gt; 212 fc2(212) &gt;&gt; 100 &lt;&lt; fc2_output Approach 1 fc3(100) &gt;&gt; 10 + 19 OR 10 + 5 &lt;&lt; out fc3(100) &gt;&gt; 19 OR 15 &lt;&lt; out Approach 2 fc3(fc2_output) &gt;&gt; 10 &lt;&lt; MNIST VALUE fc4(fc2_output) &gt;&gt; 19/5 &lt;&lt; SUM VALUE return MNIST_VALUE, SUM_VALUE</p> <p>return out</p> <p>loss_for_mnist out[:10] loss_for_sum out[10:]</p> In\u00a0[\u00a0]: Copied! <pre>from google.colab import drive\ndrive.mount('/content/gdrive')\n</pre> from google.colab import drive drive.mount('/content/gdrive') In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image\nImage(open('/content/gdrive/MyDrive/ERA V2/PytorchGraphs.gif', 'rb').read())\n</pre> from IPython.display import Image Image(open('/content/gdrive/MyDrive/ERA V2/PytorchGraphs.gif', 'rb').read()) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/deep_learning/PyTorch_101/#pytorch","title":"Pytorch\u00b6","text":"<p>An open source machine learning framework that accelerates the path from research prototyping to production deployment.</p> <p>It is a deep learning framework and a scientific computing package.</p> <p>It has several components that you'd like to be aware of (from future perspective):</p> <ul> <li>PyTorch is production ready. With torchScript, PyTorch provides ease-of-use and flexiblility in eager mode, while seamlessly  transitioning to graph mode for speed, optimization, and functionality in C++ runtime environments.</li> <li>TorchServe is an easy to use tool for deploying PyTorch models at scale. It is cloud and environment agnostic and supports features such as multi-model serving, logging, metrics and the creation of RESTful endpoints for application integration.</li> <li>Optimized performance in both research and production by taking advantage of native support for asynchronous execution of collective operations and peer-to-peer communication that is accessible from Python and C++.</li> <li>PyTorch supports an end-to-end workflow from Python to deployment on iOS and Android. It extends the PyTorch API to cover common preprocessing and integration tasks needed for incorporating ML in mobile applications.</li> <li>An active community of researchers and developers have built a rich ecosystem of tools and libraries for extending PyTorch and supporting development in areas from computer vision to reinforcement learning.</li> <li>Export models in the standard ONNX (Open Neural Network Exchange) format for direct access to ONNX-compatible platforms, runtimes, visualizers, and more.</li> <li>The C++ frontend is a pure C++ interface to PyTorch that follows the design and architecture of the established Python frontend. It is intended to enable research in high performance, low latency and bare metal C++ applications.</li> <li>PyTorch is well supported on major cloud platforms, providing frictionless development and easy scaling through prebuilt images, large scale training on GPUs, ability to run models in a production scale environment, and more.</li> </ul> PyTorch TensorFlow Dynamic Computation Graphs Had to integrate Keras to add Dynamic Graphs, else static graphs Have explicit GPU and CPU controls 2.0 has made this slighyl easy compared to earlier More Pythonic in nature Steep learning curve Good Documentation Best possible documentation External tool for visualization and logging TensorBoard Lighter to work with Large deployment dependencies Debugging is difficult Better debugging capabilities <p></p> <p></p>"},{"location":"notebooks/deep_learning/PyTorch_101/#scientific-computing","title":"Scientific Computing\u00b6","text":"<p>The scientific computing aspect of PyTorch is primarily a result of PyTorch's tensor Library and associated tensor operations</p> <p>A tensor is an n-dimensional array.</p> <p></p>"},{"location":"notebooks/deep_learning/PyTorch_101/#numpy","title":"Numpy\u00b6","text":"<p>One of the most popular scientific computing package for working with Tensors is nunmpy. </p> <p>Numpy is the go-to package for nD arrays. PyTorch's tensor library mirrors numpy nD array capabilities very closely and in addition is highly interoprable with numpy. In PyTorch GPU support for tensors is inbuilt, and it is vey easy to move arrays from numpy (CPU) to GPU.</p>"},{"location":"notebooks/deep_learning/PyTorch_101/#pytorch-philosophy","title":"PyTorch Philosophy\u00b6","text":"<ul> <li>Stay out of the way</li> <li>Cater to the impatient</li> <li>Promote linear code-flow</li> <li>Full interop with the Python ecosystem</li> <li>Be as fasst as anything else</li> </ul>"},{"location":"notebooks/deep_learning/PyTorch_101/#debugging","title":"Debugging\u00b6","text":"<ul> <li>PyTorch is Python Extension</li> <li>You can use any Python Debugger</li> <li>Even print function works on PyTorch objects</li> </ul>"},{"location":"notebooks/deep_learning/PyTorch_101/#why-it-got-so-popular","title":"Why it got so popular?\u00b6","text":"<p>To solve deep neural networks, we need to calculate derivates, and to do this computationally, deep learning frameworks use what are called computational graphs. </p> <p>PyTorch introduced Dynamic Computational Graphs, a first in the industry. It means that the graph is generated on the fly as the operations occur in contrast to static graphs that are fully determined before actual action occur.</p> <p>Experimentation, advanced models and quick tests need Dynamic Computation Graphs as a back-end feature.</p> <p>The debate between PyTorch and TensorFlow for deep learning applications has evolved considerably, with both frameworks making significant strides in features, usability, and performance. TensorFlow's introduction of dynamic computation graphs (through TensorFlow 2.0 and the eager execution mode) has indeed narrowed the gap that initially set PyTorch apart, primarily its intuitive approach and ease of use due to dynamic graph computation. However, there are still several areas where users might prefer PyTorch over TensorFlow, depending on their specific needs and preferences:</p> <ol> <li><p>Ease of Learning and Use: PyTorch is often praised for its straightforward and pythonic syntax, making it easier to learn, especially for beginners in deep learning. Its dynamic computation graph (define-by-run approach) allows for more intuitive debugging and a simpler way to understand the model flow.</p> </li> <li><p>Research and Flexibility: PyTorch continues to be very popular in the research community due to its flexibility and simplicity, which are crucial for experimental setups. The dynamic nature of its computation graphs makes it easier to change the model on the fly and is often preferred for prototyping and exploratory research projects.</p> </li> <li><p>Better Integration with Python: PyTorch is designed to integrate closely with Python, leveraging its features and libraries with minimal friction. This seamless integration makes it a favorite for developers coming from a Python background, looking to incorporate deep learning models into their Python projects.</p> </li> <li><p>Community and Ecosystem: While TensorFlow has a large and established user base, PyTorch has been rapidly growing its community, particularly among researchers and academia. This vibrant community contributes to a wide range of tutorials, forums, and third-party extensions, which can be particularly valuable for cutting-edge research and newer methodologies.</p> </li> <li><p>Dynamic vs. Static Computation Graphs: Although TensorFlow now supports dynamic computation graphs, PyTorch's implementation is inherently dynamic, designed from the ground up to be flexible and intuitive. This can lead to easier experimentation and modifications without the need to rebuild the graph from scratch, as was traditionally the case with TensorFlow's static graphs before the introduction of eager execution.</p> </li> <li><p>Model Development and Deployment: TensorFlow might have an edge in deployment capabilities, especially with TensorFlow Serving and TensorFlow Lite for mobile devices. However, PyTorch has made strides in this area with TorchScript and the PyTorch Mobile, narrowing the deployment gap and offering competitive options for bringing models from research to production.</p> </li> </ol>"},{"location":"notebooks/deep_learning/PyTorch_101/#lets-do-some-coding-now","title":"Let's do some coding now.\u00b6","text":""},{"location":"notebooks/deep_learning/PyTorch_101/#pytorchs-tensors-are-similar-to-numpys-ndarrays","title":"Pytorch's tensors are similar to Numpy's ndarrays\u00b6","text":""},{"location":"notebooks/deep_learning/PyTorch_101/#creating-a-tensor-from-a-numpy","title":"Creating a Tensor from a Numpy\u00b6","text":""},{"location":"notebooks/deep_learning/PyTorch_101/#sharing-memory-for-performance-copy-vs-share","title":"Sharing memory for performance: copy vs share\u00b6","text":"Share Data Copy Data torch.as_tensor() torch.tensor() torch.from_numpy() torch.Tensor() <p>Zero Memory Copy \u21cb Very Efficient</p>"},{"location":"notebooks/deep_learning/PyTorch_101/#something-to-keep-in-mind","title":"Something to keep in mind\u00b6","text":"<ul> <li>Since numpy.array objects are allocated on CPU, the <code>as_tensor()</code> function must copy the data from the CPU to the GPU when a GPU is being used</li> <li>The memory sharing of <code>as_tensor()</code> doesn't work with built-in Python data structure like list</li> <li>The <code>as_tensor()</code> call requires developer knowledge of the sharing feature. This is necessary so we don't make an unwanted change in the underlying data without realising it</li> <li>The <code>as_tensor()</code> performance improvement will be greater when there are a lot of back and forth operations between <code>numpy.array</code> objects and tensor objects.</li> </ul>"},{"location":"notebooks/deep_learning/PyTorch_101/#torchtensor-attributes","title":"Torch.tensor attributes\u00b6","text":"Attribute Data Type Description data array_like list, tuple, NumPy ndarray, scalar dtype torch.dtype The tensor's data type requires_grad bool Should autograd record operation device torch.device Allocated on CPU or CUDA (GPU) <p>torch.tensor(data, dtype=None, device=None, requires_grad=False) \u2192 Tensor</p>"},{"location":"notebooks/deep_learning/PyTorch_101/#autograd","title":"Autograd\u00b6","text":"<ul> <li>Automatic differentiation for all operations on Tensors</li> <li>The backward graph is automatically defined by the forward graph!</li> </ul>"},{"location":"notebooks/deep_learning/PyTorch_101/#tensor-operation-types","title":"Tensor Operation Types\u00b6","text":"<ol> <li>Reshaping Operations</li> <li>Element-wise Operations</li> <li>Reduction Operations</li> <li>Access Operations</li> </ol>"},{"location":"notebooks/deep_learning/PyTorch_101/#why-knowing-these-is-important","title":"Why knowing these is important?\u00b6","text":"<p>When we reshape a tensor, the reshaping operation must account for all of numel (or total number of elements). We if we have 12 elements, we can only build 1x12, 12x1, 6x2, 2x6, 3x4, 4x3 reshaped tensors</p>"},{"location":"notebooks/deep_learning/PyTorch_101/#pytorch-main","title":"PyTorch Main\u00b6","text":""},{"location":"notebooks/deep_learning/PyTorch_101/#working-with-the-dataset","title":"Working with the dataset\u00b6","text":"<p>We will look at convolutional/image examples as they allow us to get a good intuition on axises and are more complex to work on just 1D data</p>"},{"location":"notebooks/deep_learning/PyTorch_101/#a-common-nn-pipeline-looks-like-this","title":"A common nn pipeline looks like this:\u00b6","text":"<ol> <li>Prepare the data</li> <li>Build the model</li> <li>Train the model</li> <li>Analyze the model</li> </ol>"},{"location":"notebooks/deep_learning/PyTorch_101/#pytorch-main","title":"PyTorch Main\u00b6","text":""},{"location":"notebooks/deep_learning/PyTorch_101/#working-with-the-dataset","title":"Working with the dataset\u00b6","text":"<p>We will look at convolutional/image examples as they allow us to get a good intuition on axises and are more complex to work on just 1D data</p>"},{"location":"notebooks/deep_learning/PyTorch_101/#a-common-nn-pipeline-looks-like-this","title":"A common nn pipeline looks like this:\u00b6","text":"<ol> <li>Prepare the data</li> <li>Build the model</li> <li>Train the model</li> <li>Analyze the model</li> </ol>"},{"location":"notebooks/deep_learning/PyTorch_101/#network","title":"Network\u00b6","text":""},{"location":"notebooks/deep_learning/Python_101/","title":"Python 101","text":"In\u00a0[\u00a0]: Copied! <pre>print(\"Good Morning\")\n</pre> print(\"Good Morning\") <pre>Good Morning\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"Good \\\nMorning\")\n</pre> print(\"Good \\ Morning\") <pre>Good Morning\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = [1, \"red\", \"r\", 3 + 4j, [3, 4, 'a']]\n</pre> a = [1, \"red\", \"r\", 3 + 4j, [3, 4, 'a']] In\u00a0[\u00a0]: Copied! <pre>a = [1,\n     \"red\",\n     \"r\",\n     3 + 4j,\n     [3, 4, 'a']\n     ]\n</pre> a = [1,      \"red\",      \"r\",      3 + 4j,      [3, 4, 'a']      ] In\u00a0[\u00a0]: Copied! <pre>a\n</pre> a Out[\u00a0]: <pre>[1, 'red', 'r', (3+4j), [3, 4, 'a']]</pre> In\u00a0[\u00a0]: Copied! <pre>a = \"This is a very long string that I am writing about the cup\nI might get soon which might be full of amazing coffee, and it might be hot as well\"\n</pre> a = \"This is a very long string that I am writing about the cup I might get soon which might be full of amazing coffee, and it might be hot as well\" <pre>\n  File \"&lt;ipython-input-8-7ab4f901ca8e&gt;\", line 1\n    a = \"This is a very long string that I am writing about the cup\n        ^\nSyntaxError: unterminated string literal (detected at line 1)\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = [1, # this is serial number ,\n     \"red\", # this is the color\n     \"r\", # this is initial of the code\n     3 + 4j,\n     [3, 4, 'a']\n     ]\n</pre> a = [1, # this is serial number ,      \"red\", # this is the color      \"r\", # this is initial of the code      3 + 4j,      [3, 4, 'a']      ] In\u00a0[\u00a0]: Copied! <pre>l = [1, \"rohan\", 3.4, 4 + 5j] # a LIST\nt = (1, \"rohan\", 3.4, 4 + 5j) # a tuple\ns = {1, \"rohan\", 3.4, 4 + 5j} # a SET\nd = {\"key1\": 1, 3: \"rohan\", 4: 3.4, \"key2\": 4 + 5j} # DICTOINARY\n</pre> l = [1, \"rohan\", 3.4, 4 + 5j] # a LIST t = (1, \"rohan\", 3.4, 4 + 5j) # a tuple s = {1, \"rohan\", 3.4, 4 + 5j} # a SET d = {\"key1\": 1, 3: \"rohan\", 4: 3.4, \"key2\": 4 + 5j} # DICTOINARY In\u00a0[\u00a0]: Copied! <pre>l, t, s, d\n</pre> l, t, s, d Out[\u00a0]: <pre>([1, 'rohan', 3.4, (4+5j)],\n (1, 'rohan', 3.4, (4+5j)),\n {(4+5j), 1, 3.4, 'rohan'},\n {'key1': 1, 3: 'rohan', 4: 3.4, 'key2': (4+5j)})</pre> In\u00a0[\u00a0]: Copied! <pre>l[0], t[0]\n</pre> l[0], t[0] Out[\u00a0]: <pre>(1, 1)</pre> In\u00a0[\u00a0]: Copied! <pre>l[0] = 10\nl\n</pre> l[0] = 10 l Out[\u00a0]: <pre>[10, 'rohan', 3.4, (4+5j)]</pre> In\u00a0[\u00a0]: Copied! <pre>t[0] = 10\nt\n</pre> t[0] = 10 t <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-21-0ec6968f8d77&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 t[0] = 10\n      2 t\n\nTypeError: 'tuple' object does not support item assignment</pre> In\u00a0[\u00a0]: Copied! <pre>t[0], l[0], s[0]\n</pre> t[0], l[0], s[0] <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-24-76c3a1755101&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 t[0], l[0], s[0]\n\nTypeError: 'set' object is not subscriptable</pre> In\u00a0[\u00a0]: Copied! <pre>s = {1, 1, 1, 1, 1}\n</pre> s = {1, 1, 1, 1, 1} In\u00a0[\u00a0]: Copied! <pre>s\n</pre> s Out[\u00a0]: <pre>{1}</pre> In\u00a0[\u00a0]: Copied! <pre>s = {1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, \"red\", \"red\", \"blue\"}\ns\n</pre> s = {1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, \"red\", \"red\", \"blue\"} s Out[\u00a0]: <pre>{1, 2, 3, 4, 'blue', 'red'}</pre> In\u00a0[\u00a0]: Copied! <pre>d = {\"key1\": 1, 3: \"rohan\", 4: 3.4, \"key2\": 4 + 5j} # DICTOINARY\nd[\"key1\"], d[3]\n</pre> d = {\"key1\": 1, 3: \"rohan\", 4: 3.4, \"key2\": 4 + 5j} # DICTOINARY d[\"key1\"], d[3] Out[\u00a0]: <pre>(1, 'rohan')</pre> In\u00a0[\u00a0]: Copied! <pre>def myFunc():\n  print(\"THis is our first fuinction\")\n\nmyFunc()\n</pre> def myFunc():   print(\"THis is our first fuinction\")  myFunc() <pre>THis is our first fuinction\n</pre> In\u00a0[\u00a0]: Copied! <pre>myFunc\n</pre> myFunc Out[\u00a0]: <pre>&lt;function __main__.myFunc()&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>print\n</pre> print Out[\u00a0]: <pre>&lt;function print&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>def my_func(a, b):\n  c = a + b\n  return c\nmy_func(1, 4)\n</pre> def my_func(a, b):   c = a + b   return c my_func(1, 4) Out[\u00a0]: <pre>5</pre> In\u00a0[\u00a0]: Copied! <pre>def my_func(a, b):\n  c = a + b\n  return c\nmy_func(1.5, \"4\")\n</pre> def my_func(a, b):   c = a + b   return c my_func(1.5, \"4\") <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-34-f13fd04bdb18&gt; in &lt;cell line: 4&gt;()\n      2   c = a + b\n      3   return c\n----&gt; 4 my_func(1.5, \"4\")\n\n&lt;ipython-input-34-f13fd04bdb18&gt; in my_func(a, b)\n      1 def my_func(a, b):\n----&gt; 2   c = a + b\n      3   return c\n      4 my_func(1.5, \"4\")\n\nTypeError: unsupported operand type(s) for +: 'float' and 'str'</pre> In\u00a0[\u00a0]: Copied! <pre>def my_func(a, b):\n  c = a * b\n  return c\nmy_func(3, \"4\")\n</pre> def my_func(a, b):   c = a * b   return c my_func(3, \"4\") Out[\u00a0]: <pre>'444'</pre> In\u00a0[\u00a0]: Copied! <pre>c\n</pre> c <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-36-2b66fd261ee5&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 c\n\nNameError: name 'c' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>def my_func(a, b):\n  print(\"This is a session:\", a, \".\", b)\n\nmy_func(\"3\")\n</pre> def my_func(a, b):   print(\"This is a session:\", a, \".\", b)  my_func(\"3\") <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-38-94742965512f&gt; in &lt;cell line: 4&gt;()\n      2   print(\"This is a session:\", a, \".\", b)\n      3 \n----&gt; 4 my_func(\"3\")\n\nTypeError: my_func() missing 1 required positional argument: 'b'</pre> In\u00a0[\u00a0]: Copied! <pre>def my_func(a , b = \"5\"):\n  print(\"This is a session:\", a, \".\", b)\n\nmy_func( 4)\n</pre> def my_func(a , b = \"5\"):   print(\"This is a session:\", a, \".\", b)  my_func( 4) <pre>This is a session: 4 . 5\n</pre> In\u00a0[\u00a0]: Copied! <pre>def func(a, b, d, c = 0):\n  print(a, b, d, c)\n\nfunc(1, 2, 3, 4)\n</pre> def func(a, b, d, c = 0):   print(a, b, d, c)  func(1, 2, 3, 4) <pre>1 2 3 4\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = 10\nb = -2\nc = 30\n\nif a &gt; 5 and b &lt; 0 or c &gt; 100:\n    print(f\"Yes {a} {c + b}\")\n</pre> a = 10 b = -2 c = 30  if a &gt; 5 and b &lt; 0 or c &gt; 100:     print(f\"Yes {a} {c + b}\") <pre>Yes 10 28\n</pre> In\u00a0[\u00a0]: Copied! <pre>if 4 &gt; 2:\n  print(\"yeah\")\n</pre> if 4 &gt; 2:   print(\"yeah\") <pre>yeah\n</pre> In\u00a0[\u00a0]: Copied! <pre>if a &gt; 5 \\\n  and b &lt; 0 \\\n  or c &gt; 100:\n  print(f\"Yes {a} {c + b}\")\n</pre> if a &gt; 5 \\   and b &lt; 0 \\   or c &gt; 100:   print(f\"Yes {a} {c + b}\") <pre>Yes 10 28\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = 10\nb = 4\n\nprint(\"A: \", a, \"B: \", b)\n</pre> a = 10 b = 4  print(\"A: \", a, \"B: \", b) <pre>A:  10 B:  4\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(f\"A: {a*a}, B: {b + a*b}\")\n</pre> print(f\"A: {a*a}, B: {b + a*b}\") <pre>A: 100, B: 44\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = \"This is one of the lines taken from Shashi Tharoors interview. \\\nHe said, that one the democracy is hindered for its progress by the \\\nrightist movements and their jabs at the structure of the secularism,\\\nthen it becomes very difficult to beat them at the democratic\\\nelectctions becuase now people are infused with the ideas that have \\\nno relation to their earnings or finances, but the whims and fan\\\ncies of what they believe to be true...\"\n</pre> a = \"This is one of the lines taken from Shashi Tharoors interview. \\ He said, that one the democracy is hindered for its progress by the \\ rightist movements and their jabs at the structure of the secularism,\\ then it becomes very difficult to beat them at the democratic\\ electctions becuase now people are infused with the ideas that have \\ no relation to their earnings or finances, but the whims and fan\\ cies of what they believe to be true...\" In\u00a0[\u00a0]: Copied! <pre>print(a)\n</pre> print(a) <pre>This is one of the lines taken from Shashi Tharoors interview. He said, that one the democracy is hindered for its progress by the rightist movements and their jabs at the structure of the secularism,then it becomes very difficult to beat them at the democraticelectctions becuase now people are infused with the ideas that have no relation to their earnings or finances, but the whims and fancies of what they believe to be true...\n</pre> In\u00a0[\u00a0]: Copied! <pre>\"\"\"This is one of the lines taken from Shashi Tharoors interview.\nHe said, that one the democracy is hindered for its progress by the\nrightist movements and their jabs at the structure of the secularism,\nthen it becomes very difficult to beat them at the democratic\nelectctions becuase now people are infused with the ideas that have no\nrelation to their earnings or finances, but the whims and fancies of\nwhat they believe to be true...\"\"\"\n</pre> \"\"\"This is one of the lines taken from Shashi Tharoors interview. He said, that one the democracy is hindered for its progress by the rightist movements and their jabs at the structure of the secularism, then it becomes very difficult to beat them at the democratic electctions becuase now people are infused with the ideas that have no relation to their earnings or finances, but the whims and fancies of what they believe to be true...\"\"\" Out[\u00a0]: <pre>'This is one of the lines taken from Shashi Tharoors interview. \\nHe said, that one the democracy is hindered for its progress by the \\nrightist movements and their jabs at the structure of the secularism, \\nthen it becomes very difficult to beat them at the democratic \\nelectctions becuase now people are infused with the ideas that have no \\nrelation to their earnings or finances, but the whims and fancies of \\nwhat they believe to be true...'</pre> In\u00a0[\u00a0]: Copied! <pre>a = \"\"\"This is one of the lines taken from Shashi Tharoors interview.\nHe said, that one the democracy is hindered for its progress by the\nrightist movements and their jabs at the structure of the secularism,\nthen it becomes very difficult to beat them at the democratic\nelectctions becuase now people are infused with the ideas that have no\nrelation to their earnings or finances, but the whims and fancies of\nwhat they believe to be true...\"\"\"\n</pre> a = \"\"\"This is one of the lines taken from Shashi Tharoors interview. He said, that one the democracy is hindered for its progress by the rightist movements and their jabs at the structure of the secularism, then it becomes very difficult to beat them at the democratic electctions becuase now people are infused with the ideas that have no relation to their earnings or finances, but the whims and fancies of what they believe to be true...\"\"\" In\u00a0[\u00a0]: Copied! <pre>a\n</pre> a Out[\u00a0]: <pre>'This is one of the lines taken from Shashi Tharoors interview. \\nHe said, that one the democracy is hindered for its progress by the \\nrightist movements and their jabs at the structure of the secularism, \\nthen it becomes very difficult to beat them at the democratic \\nelectctions becuase now people are infused with the ideas that have no \\nrelation to their earnings or finances, but the whims and fancies of \\nwhat they believe to be true...'</pre> In\u00a0[\u00a0]: Copied! <pre># if, else, elif, ternary\n\na = 4\nif a &lt; 3:\n  print(\"a&lt;3\")\nelse:\n  print(\"a&gt;3\")\n</pre> # if, else, elif, ternary  a = 4 if a &lt; 3:   print(\"a&lt;3\") else:   print(\"a&gt;3\") <pre>a&gt;3\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = 15000\n\nif a &lt; 5:\n  print(\"Lets sit at home\")\nelse:\n  if a &gt; 10000:\n    print(\"I may go with you as I am in a good mood\")\n  else:\n    print(\"I will just watch movie at home\")\n</pre> a = 15000  if a &lt; 5:   print(\"Lets sit at home\") else:   if a &gt; 10000:     print(\"I may go with you as I am in a good mood\")   else:     print(\"I will just watch movie at home\") <pre>I may go with you as I am in a good mood\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = 15\n\nif a &lt; 5:\n  print(\"a is less than 5\")\nelif a &lt; 10:\n  print(\" a is more than 5 but less than 10\")\nelse:\n  print(\"a is more than 10\")\n</pre> a = 15  if a &lt; 5:   print(\"a is less than 5\") elif a &lt; 10:   print(\" a is more than 5 but less than 10\") else:   print(\"a is more than 10\") <pre>a is more than 10\n</pre> In\u00a0[\u00a0]: Copied! <pre>def say_hello():\n  print(\"hello\")\n\ndef say_goodbye():\n  print(\"goodbye\")\n\ntime = 6\n\nsay_hello() if time &lt; 5 else say_goodbye()\n# DOTHIS IF CONTIDION=TRUE ELSE DOTHISINSTEAD\n</pre> def say_hello():   print(\"hello\")  def say_goodbye():   print(\"goodbye\")  time = 6  say_hello() if time &lt; 5 else say_goodbye() # DOTHIS IF CONTIDION=TRUE ELSE DOTHISINSTEAD <pre>goodbye\n</pre> In\u00a0[\u00a0]: Copied! <pre>s = [1, 2, 3]\n\nlen(s), s[0]\n</pre> s = [1, 2, 3]  len(s), s[0] Out[\u00a0]: <pre>(3, 1)</pre> In\u00a0[\u00a0]: Copied! <pre>s[0], s[2], s[-1], s[-2]\n</pre> s[0], s[2], s[-1], s[-2] Out[\u00a0]: <pre>(1, 3, 3, 2)</pre> In\u00a0[\u00a0]: Copied! <pre>len, print\n</pre> len, print Out[\u00a0]: <pre>(&lt;function len(obj, /)&gt;, &lt;function print&gt;)</pre> In\u00a0[\u00a0]: Copied! <pre>sqrt(4)\n</pre> sqrt(4) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-68-317e033d29d5&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 sqrt(4)\n\nNameError: name 'sqrt' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>from math import sqrt\n\nsqrt(4)\n</pre> from math import sqrt  sqrt(4) Out[\u00a0]: <pre>2.0</pre> In\u00a0[\u00a0]: Copied! <pre>math.exp(4)\n</pre> math.exp(4) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-70-1e2b1dcf1e3a&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 math.exp(4)\n\nNameError: name 'math' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>import math\n\nmath.cos(0)\n</pre> import math  math.cos(0) Out[\u00a0]: <pre>1.0</pre> In\u00a0[\u00a0]: Copied! <pre>dir()\n</pre> dir() Out[\u00a0]: <pre>['In',\n 'Out',\n '_',\n '_18',\n '_19',\n '_20',\n '_22',\n '_23',\n '_26',\n '_27',\n '_28',\n '_30',\n '_31',\n '_32',\n '_33',\n '_35',\n '_57',\n '_59',\n '_6',\n '_64',\n '_65',\n '_66',\n '_67',\n '_69',\n '_71',\n '__',\n '___',\n '__builtin__',\n '__builtins__',\n '__doc__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n '_dh',\n '_i',\n '_i1',\n '_i10',\n '_i11',\n '_i12',\n '_i13',\n '_i14',\n '_i15',\n '_i16',\n '_i17',\n '_i18',\n '_i19',\n '_i2',\n '_i20',\n '_i21',\n '_i22',\n '_i23',\n '_i24',\n '_i25',\n '_i26',\n '_i27',\n '_i28',\n '_i29',\n '_i3',\n '_i30',\n '_i31',\n '_i32',\n '_i33',\n '_i34',\n '_i35',\n '_i36',\n '_i37',\n '_i38',\n '_i39',\n '_i4',\n '_i40',\n '_i41',\n '_i42',\n '_i43',\n '_i44',\n '_i45',\n '_i46',\n '_i47',\n '_i48',\n '_i49',\n '_i5',\n '_i50',\n '_i51',\n '_i52',\n '_i53',\n '_i54',\n '_i55',\n '_i56',\n '_i57',\n '_i58',\n '_i59',\n '_i6',\n '_i60',\n '_i61',\n '_i62',\n '_i63',\n '_i64',\n '_i65',\n '_i66',\n '_i67',\n '_i68',\n '_i69',\n '_i7',\n '_i70',\n '_i71',\n '_i72',\n '_i8',\n '_i9',\n '_ih',\n '_ii',\n '_iii',\n '_oh',\n 'a',\n 'b',\n 'c',\n 'd',\n 'exit',\n 'func',\n 'get_ipython',\n 'l',\n 'math',\n 'myFunc',\n 'my_func',\n 'quit',\n 's',\n 'say_goodbye',\n 'say_hello',\n 'sqrt',\n 't',\n 'time']</pre> In\u00a0[\u00a0]: Copied! <pre>math.sin(45), dir()\n</pre> math.sin(45), dir() Out[\u00a0]: <pre>(0.8509035245341184,\n ['In',\n  'Out',\n  '_',\n  '_18',\n  '_19',\n  '_20',\n  '_22',\n  '_23',\n  '_26',\n  '_27',\n  '_28',\n  '_30',\n  '_31',\n  '_32',\n  '_33',\n  '_35',\n  '_57',\n  '_59',\n  '_6',\n  '_64',\n  '_65',\n  '_66',\n  '_67',\n  '_69',\n  '_71',\n  '_72',\n  '__',\n  '___',\n  '__builtin__',\n  '__builtins__',\n  '__doc__',\n  '__loader__',\n  '__name__',\n  '__package__',\n  '__spec__',\n  '_dh',\n  '_i',\n  '_i1',\n  '_i10',\n  '_i11',\n  '_i12',\n  '_i13',\n  '_i14',\n  '_i15',\n  '_i16',\n  '_i17',\n  '_i18',\n  '_i19',\n  '_i2',\n  '_i20',\n  '_i21',\n  '_i22',\n  '_i23',\n  '_i24',\n  '_i25',\n  '_i26',\n  '_i27',\n  '_i28',\n  '_i29',\n  '_i3',\n  '_i30',\n  '_i31',\n  '_i32',\n  '_i33',\n  '_i34',\n  '_i35',\n  '_i36',\n  '_i37',\n  '_i38',\n  '_i39',\n  '_i4',\n  '_i40',\n  '_i41',\n  '_i42',\n  '_i43',\n  '_i44',\n  '_i45',\n  '_i46',\n  '_i47',\n  '_i48',\n  '_i49',\n  '_i5',\n  '_i50',\n  '_i51',\n  '_i52',\n  '_i53',\n  '_i54',\n  '_i55',\n  '_i56',\n  '_i57',\n  '_i58',\n  '_i59',\n  '_i6',\n  '_i60',\n  '_i61',\n  '_i62',\n  '_i63',\n  '_i64',\n  '_i65',\n  '_i66',\n  '_i67',\n  '_i68',\n  '_i69',\n  '_i7',\n  '_i70',\n  '_i71',\n  '_i72',\n  '_i73',\n  '_i8',\n  '_i9',\n  '_ih',\n  '_ii',\n  '_iii',\n  '_oh',\n  'a',\n  'b',\n  'c',\n  'd',\n  'exit',\n  'func',\n  'get_ipython',\n  'l',\n  'math',\n  'myFunc',\n  'my_func',\n  'quit',\n  's',\n  'say_goodbye',\n  'say_hello',\n  'sqrt',\n  't',\n  'time'])</pre> In\u00a0[\u00a0]: Copied! <pre>cc = \"organge\"\ndd = \"organge\"\nee = \"organge\"\n</pre> cc = \"organge\" dd = \"organge\" ee = \"organge\" In\u00a0[\u00a0]: Copied! <pre>id(a)\n</pre> id(a) Out[\u00a0]: <pre>136805355864752</pre> In\u00a0[\u00a0]: Copied! <pre>id(cc), id(dd), id(ee)\n</pre> id(cc), id(dd), id(ee) Out[\u00a0]: <pre>(136804086654448, 136804086654448, 136804086654448)</pre> In\u00a0[\u00a0]: Copied! <pre>id(1)\n</pre> id(1) Out[\u00a0]: <pre>136805355864304</pre> In\u00a0[\u00a0]: Copied! <pre>g = 1\nid(g), id(id(g))\n</pre> g = 1 id(g), id(id(g)) Out[\u00a0]: <pre>(136805355864304, 136804085607984)</pre> In\u00a0[\u00a0]: Copied! <pre>del cc, dd, ee\n</pre> del cc, dd, ee In\u00a0[\u00a0]: Copied! <pre>cc\n</pre> cc <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-85-2bd2965b6208&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 cc\n\nNameError: name 'cc' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>kk = \"organge\"\nid(kk)\n</pre> kk = \"organge\" id(kk) Out[\u00a0]: <pre>136804085758384</pre> In\u00a0[\u00a0]: Copied! <pre>p = 147\nprint(id(p))\ndel p\nu = 147\nprint(id(u))\n</pre> p = 147 print(id(p)) del p u = 147 print(id(u)) <pre>136805355868976\n136805355868976\n</pre> In\u00a0[\u00a0]: Copied! <pre>def new_func(rr):\n  rr = 45\n\nrr = 33\nnew_func(rr)\nprint(rr)\n</pre> def new_func(rr):   rr = 45  rr = 33 new_func(rr) print(rr) <pre>33\n</pre> In\u00a0[\u00a0]: Copied! <pre># lambda\n\ndef mouse1(a, b):\n  return a* b\n\nmouse2 = lambda a, b : a * b\n\nmouse1(1, 2), mouse2(1, 2)\n</pre> # lambda  def mouse1(a, b):   return a* b  mouse2 = lambda a, b : a * b  mouse1(1, 2), mouse2(1, 2) Out[\u00a0]: <pre>(2, 2)</pre> In\u00a0[\u00a0]: Copied! <pre>lambda a, b : a * b\n</pre> lambda a, b : a * b Out[\u00a0]: <pre>&lt;function __main__.&lt;lambda&gt;(a, b)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>def newnew(a, b):\n  red = lambda a, b : a * b\n  return red(a, b)\n\nnewnew(3, 4)\n</pre> def newnew(a, b):   red = lambda a, b : a * b   return red(a, b)  newnew(3, 4) Out[\u00a0]: <pre>12</pre> In\u00a0[\u00a0]: Copied! <pre>red = lambda f, g:  f * g\ndef new3(a, blue):\n  return blue(a, 3)\nnew3(4, red)\n</pre> red = lambda f, g:  f * g def new3(a, blue):   return blue(a, 3) new3(4, red) Out[\u00a0]: <pre>12</pre> In\u00a0[\u00a0]: Copied! <pre>adder = lambda f, g:  f + g\nsubtractor = lambda f, g:  f - g\ndivider = lambda f, g:  f / g\nmultiplier = lambda f, g:  f * g\ndef new3(a, any_func):\n  if a == 1:\n    return adder(a, 3)\n  elif a == 2:\n    return subtractor(a, 3)\n  elif a == 3:\n    return divider(a, 3)\nprint(new3(2, adder))\n</pre> adder = lambda f, g:  f + g subtractor = lambda f, g:  f - g divider = lambda f, g:  f / g multiplier = lambda f, g:  f * g def new3(a, any_func):   if a == 1:     return adder(a, 3)   elif a == 2:     return subtractor(a, 3)   elif a == 3:     return divider(a, 3) print(new3(2, adder)) <pre>-1\n</pre> In\u00a0[\u00a0]: Copied! <pre>adder = lambda f, g:  f + g\nsubtractor = lambda f, g:  f - g\ndivider = lambda f, g:  f / g\nmultiplier = lambda f, g:  f * g\n\ndef new3(a, any_func):\n  return any_func(a, 4)\nprint(new3(2, multiplier))\n</pre> adder = lambda f, g:  f + g subtractor = lambda f, g:  f - g divider = lambda f, g:  f / g multiplier = lambda f, g:  f * g  def new3(a, any_func):   return any_func(a, 4) print(new3(2, multiplier)) <pre>8\n</pre> In\u00a0[\u00a0]: Copied! <pre>i = 0\n\nwhile i &lt; 5:\n  print(i)\n  i = i + 1\n</pre> i = 0  while i &lt; 5:   print(i)   i = i + 1 <pre>0\n1\n2\n3\n4\n</pre> In\u00a0[\u00a0]: Copied! <pre>while True:\n  print(\"something\")\n  break\n</pre> while True:   print(\"something\")   break <pre>something\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = 10\nb = 0\n\na / b\n</pre> a = 10 b = 0  a / b <pre>\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\n&lt;ipython-input-105-aec8b3488910&gt; in &lt;cell line: 4&gt;()\n      2 b = 0\n      3 \n----&gt; 4 a / b\n\nZeroDivisionError: division by zero</pre> In\u00a0[\u00a0]: Copied! <pre>a = 10\nb = 10\n\ntry:\n  a/b\n  print(\"works\")\nexcept ZeroDivisionError:\n  print(\"b was 0 dude!\")\nfinally:\n  print(\"I will always work\")\n</pre> a = 10 b = 10  try:   a/b   print(\"works\") except ZeroDivisionError:   print(\"b was 0 dude!\") finally:   print(\"I will always work\")  <pre>works\nI will always work\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = 10\nb = 0\n\ntry:\n  a/b\n  print(\"works\")\nexcept ZeroDivisionError:\n  print(\"b was 0 dude!\")\n</pre> a = 10 b = 0  try:   a/b   print(\"works\") except ZeroDivisionError:   print(\"b was 0 dude!\") <pre>b was 0 dude!\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = 10\nb = 0\n\ntry:\n  a/b\n  print(\"works\")\nexcept ZeroDivisionError:\n  print(\"b was 0 dude!\")\nfinally:\n  print(\"I will always work\")\n</pre> a = 10 b = 0  try:   a/b   print(\"works\") except ZeroDivisionError:   print(\"b was 0 dude!\") finally:   print(\"I will always work\") <pre>b was 0 dude!\nI will always work\n</pre> In\u00a0[\u00a0]: Copied! <pre>a = 0\nb = 2\n\nwhile a &lt; 3:\n    print(\"____________________\")\n    a += 1 # a = a + 1\n    b -= 1\n    try:\n        res = a / b\n    except ZeroDivisionError:\n        print(f'a = {a}, b = {b} - division by 0 ZDE Step')\n        res = 0\n        continue\n    finally:\n        print(f'a = {a}, b = {b} - Finally Step')\n    print(f'a = {a}, b = {b} - main loop')\n</pre> a = 0 b = 2  while a &lt; 3:     print(\"____________________\")     a += 1 # a = a + 1     b -= 1     try:         res = a / b     except ZeroDivisionError:         print(f'a = {a}, b = {b} - division by 0 ZDE Step')         res = 0         continue     finally:         print(f'a = {a}, b = {b} - Finally Step')     print(f'a = {a}, b = {b} - main loop')   <pre>____________________\na = 1, b = 1 - Finally Step\na = 1, b = 1 - main loop\n____________________\na = 2, b = 0 - division by 0 ZDE Step\na = 2, b = 0 - Finally Step\n____________________\na = 3, b = -1 - Finally Step\na = 3, b = -1 - main loop\n</pre> In\u00a0[\u00a0]: Copied! <pre>range(10)\n</pre> range(10) Out[\u00a0]: <pre>range(0, 10)</pre> <p>for(i = 0; i &lt;10 ; i++)</p> In\u00a0[\u00a0]: Copied! <pre>l = [1, 2, 3, 4, 8, \"red\"]\nt = (\"red\", \"green\", \"blue\", 45)\nr = range(10)\n\nfor x in l:\n  print(x)\n</pre> l = [1, 2, 3, 4, 8, \"red\"] t = (\"red\", \"green\", \"blue\", 45) r = range(10)  for x in l:   print(x) <pre>1\n2\n3\n4\n8\nred\n</pre> In\u00a0[\u00a0]: Copied! <pre>for x in t:\n  print(x)\n</pre> for x in t:   print(x) <pre>red\ngreen\nblue\n45\n</pre> In\u00a0[\u00a0]: Copied! <pre>for x in r:\n  print(x)\n</pre> for x in r:   print(x) <pre>0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n</pre> In\u00a0[\u00a0]: Copied! <pre>for x in range(len(l)):\n  print(x)\n</pre> for x in range(len(l)):   print(x) <pre>0\n1\n2\n3\n4\n5\n</pre> In\u00a0[\u00a0]: Copied! <pre>for num, data in enumerate(l):\n  print(f'at {num} we have {data}')\n</pre> for num, data in enumerate(l):   print(f'at {num} we have {data}') <pre>at 0 we have 1\nat 1 we have 2\nat 2 we have 3\nat 3 we have 4\nat 4 we have 8\nat 5 we have red\n</pre> In\u00a0[\u00a0]: Copied! <pre>t = (45, 55)\na1 = t[0]\na2 = t[1]\na1, a2\n</pre> t = (45, 55) a1 = t[0] a2 = t[1] a1, a2 Out[\u00a0]: <pre>(45, 55)</pre> In\u00a0[\u00a0]: Copied! <pre>a1, a2 = (45, 55)\na1, a2\n</pre> a1, a2 = (45, 55) a1, a2 Out[\u00a0]: <pre>(45, 55)</pre> In\u00a0[\u00a0]: Copied! <pre>class Rectangle:\n    def __init__(self, w, h):\n        self.widht = w\n        self.height = h\n        print(id(self))\n\nr1 = Rectangle(3, 4)\nr2 = Rectangle(30, 40)\n\nr1.widht, r2.height\n</pre> class Rectangle:     def __init__(self, w, h):         self.widht = w         self.height = h         print(id(self))  r1 = Rectangle(3, 4) r2 = Rectangle(30, 40)  r1.widht, r2.height <pre>136804086771200\n136804086774368\n</pre> Out[\u00a0]: <pre>(3, 40)</pre> In\u00a0[\u00a0]: Copied! <pre>class Rectangle:\n    def __init__(rohan, w, h):\n        rohan.widht = w\n        rohan.height = h\n        print(id(rohan))\n\nr1 = Rectangle(3, 4)\nr2 = Rectangle(30, 40)\n\nr1.widht, r2.height, id(r1), id(r2)\n</pre> class Rectangle:     def __init__(rohan, w, h):         rohan.widht = w         rohan.height = h         print(id(rohan))  r1 = Rectangle(3, 4) r2 = Rectangle(30, 40)  r1.widht, r2.height, id(r1), id(r2) <pre>136804086766784\n136804086765488\n</pre> Out[\u00a0]: <pre>(3, 40, 136804086766784, 136804086765488)</pre> In\u00a0[\u00a0]: Copied! <pre>class Rectangle:\n    def __init__(rohan, w, h):\n        rohan.widht = w\n        rohan.height = h\n        print(id(rohan))\n\nr1 = Rectangle(3, 4)\nr2 = Rectangle(30, 40)\n\nr1.widht, r2.height\n</pre> class Rectangle:     def __init__(rohan, w, h):         rohan.widht = w         rohan.height = h         print(id(rohan))  r1 = Rectangle(3, 4) r2 = Rectangle(30, 40)  r1.widht, r2.height <pre>136804086768128\n136804086770816\n</pre> Out[\u00a0]: <pre>(3, 40)</pre> In\u00a0[\u00a0]: Copied! <pre>class Rectangle():\n    def __init__ (self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n    def perimeter(self):\n        return 2 *  ( self.width + self.height)\n</pre> class Rectangle():     def __init__ (self, width, height):         self.width = width         self.height = height      def area(self):         return self.width * self.height      def perimeter(self):         return 2 *  ( self.width + self.height) In\u00a0[\u00a0]: Copied! <pre>r1 = Rectangle(10, 20)\nr1.area()\n</pre> r1 = Rectangle(10, 20) r1.area() Out[\u00a0]: <pre>200</pre> In\u00a0[\u00a0]: Copied! <pre>help(Rectangle)\n</pre> help(Rectangle) <pre>Help on class Rectangle in module __main__:\n\nclass Rectangle(builtins.object)\n |  Rectangle(width, height)\n |  \n |  Methods defined here:\n |  \n |  __init__(self, width, height)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  area(self)\n |  \n |  perimeter(self)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>r1\n</pre> r1 Out[\u00a0]: <pre>&lt;__main__.Rectangle at 0x7c6c2efed7e0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>class Rectangle:\n    def __init__ (self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n    def perimeter(self):\n        return 2 *  ( self.width + self.height)\n\n    def __str__(self):\n        return \"This is a rectangle stored at: \" + str(id(self))\n\nr1 = Rectangle(10, 20)\nr1.__str__()\n</pre> class Rectangle:     def __init__ (self, width, height):         self.width = width         self.height = height      def area(self):         return self.width * self.height      def perimeter(self):         return 2 *  ( self.width + self.height)      def __str__(self):         return \"This is a rectangle stored at: \" + str(id(self))  r1 = Rectangle(10, 20) r1.__str__() Out[\u00a0]: <pre>'This is a rectangle stored at: 136804086765248'</pre> In\u00a0[\u00a0]: Copied! <pre>help(Rectangle)\n</pre> help(Rectangle) <pre>Help on class Rectangle in module __main__:\n\nclass Rectangle(builtins.object)\n |  Rectangle(width, height)\n |  \n |  Methods defined here:\n |  \n |  __init__(self, width, height)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __str__(self)\n |      Return str(self).\n |  \n |  area(self)\n |  \n |  perimeter(self)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>dir(r1)\n</pre> dir(r1) Out[\u00a0]: <pre>['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n 'area',\n 'height',\n 'perimeter',\n 'width']</pre> In\u00a0[\u00a0]: Copied! <pre>4 == 5\n</pre> 4 == 5 Out[\u00a0]: <pre>False</pre> In\u00a0[\u00a0]: Copied! <pre>type(4)\n</pre> type(4) Out[\u00a0]: <pre>int</pre> In\u00a0[\u00a0]: Copied! <pre>dir(int)\n</pre> dir(int) Out[\u00a0]: <pre>['__abs__',\n '__add__',\n '__and__',\n '__bool__',\n '__ceil__',\n '__class__',\n '__delattr__',\n '__dir__',\n '__divmod__',\n '__doc__',\n '__eq__',\n '__float__',\n '__floor__',\n '__floordiv__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__index__',\n '__init__',\n '__init_subclass__',\n '__int__',\n '__invert__',\n '__le__',\n '__lshift__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__new__',\n '__or__',\n '__pos__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdivmod__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rfloordiv__',\n '__rlshift__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__round__',\n '__rpow__',\n '__rrshift__',\n '__rshift__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__sub__',\n '__subclasshook__',\n '__truediv__',\n '__trunc__',\n '__xor__',\n 'as_integer_ratio',\n 'bit_count',\n 'bit_length',\n 'conjugate',\n 'denominator',\n 'from_bytes',\n 'imag',\n 'numerator',\n 'real',\n 'to_bytes']</pre> In\u00a0[\u00a0]: Copied! <pre>class Rectangle:\n    def __init__ (self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n    def perimeter(self):\n        return 2 *  ( self.width + self.height)\n\n    def __repr__(self):\n        return \"This is a rectangle stored at: \" + str(id(self))\n\nr1 = Rectangle(10, 20)\nr1\n</pre> class Rectangle:     def __init__ (self, width, height):         self.width = width         self.height = height      def area(self):         return self.width * self.height      def perimeter(self):         return 2 *  ( self.width + self.height)      def __repr__(self):         return \"This is a rectangle stored at: \" + str(id(self))  r1 = Rectangle(10, 20) r1 Out[\u00a0]: <pre>This is a rectangle stored at: 136804086537696</pre> In\u00a0[\u00a0]: Copied! <pre>class Rectangle:\n    def __init__ (self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n    def perimeter(self):\n        return 2 *  ( self.width + self.height)\n\n    def __str__(self):\n        return \"This is rectangle stored: \" + str(id(self))\n\n    def __eq__(self, other):\n        print(f'self={self}, other={other}')\n        if isinstance(other, Rectangle):\n            return (self.width, self.height) == (other.width, other.height)\n        else:\n            return False\nr1 = Rectangle(10, 20)\nr2 = Rectangle(1, 2)\n\nr1 == r2, r2 &lt; r1\n</pre> class Rectangle:     def __init__ (self, width, height):         self.width = width         self.height = height      def area(self):         return self.width * self.height      def perimeter(self):         return 2 *  ( self.width + self.height)      def __str__(self):         return \"This is rectangle stored: \" + str(id(self))      def __eq__(self, other):         print(f'self={self}, other={other}')         if isinstance(other, Rectangle):             return (self.width, self.height) == (other.width, other.height)         else:             return False r1 = Rectangle(10, 20) r2 = Rectangle(1, 2)  r1 == r2, r2 &lt; r1 <pre>self=This is rectangle stored: 136804652632912, other=This is rectangle stored: 136804652640784\n</pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-142-305e76579610&gt; in &lt;cell line: 24&gt;()\n     22 r2 = Rectangle(1, 2)\n     23 \n---&gt; 24 r1 == r2, r2 &lt; r1\n\nTypeError: '&lt;' not supported between instances of 'Rectangle' and 'Rectangle'</pre> In\u00a0[\u00a0]: Copied! <pre>class Rectangle:\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n    def perimeter(self):\n        return 2 * (self.width + self.height)\n\n    def __str__(self):\n        return 'Rectangle (width={0}, height={1})'.format(self.width, self.height)\n\n    def __repr__(self):\n        return 'Rectangle({0}, {1})'.format(self.width, self.height)\n\n    def __eq__(self, other):\n        if isinstance(other, Rectangle):\n            return (self.width, self.height) == (other.width, other.height)\n        else:\n            return False\n\n    def __lt__(self, other):\n        if isinstance(other, Rectangle):\n            return self.area() &lt; other.area()\n        else:\n            return NotImplemented\n\nr1 = Rectangle(100, 200)\nr2 = Rectangle(10, 20)\nr1 &lt; r2\n</pre> class Rectangle:     def __init__(self, width, height):         self.width = width         self.height = height      def area(self):         return self.width * self.height      def perimeter(self):         return 2 * (self.width + self.height)      def __str__(self):         return 'Rectangle (width={0}, height={1})'.format(self.width, self.height)      def __repr__(self):         return 'Rectangle({0}, {1})'.format(self.width, self.height)      def __eq__(self, other):         if isinstance(other, Rectangle):             return (self.width, self.height) == (other.width, other.height)         else:             return False      def __lt__(self, other):         if isinstance(other, Rectangle):             return self.area() &lt; other.area()         else:             return NotImplemented  r1 = Rectangle(100, 200) r2 = Rectangle(10, 20) r1 &lt; r2 Out[\u00a0]: <pre>False</pre> In\u00a0[\u00a0]: Copied! <pre>r1.__ge__(r2)\n</pre> r1.__ge__(r2) Out[\u00a0]: <pre>NotImplemented</pre> In\u00a0[\u00a0]: Copied! <pre>dir(r1)\n</pre> dir(r1) Out[\u00a0]: <pre>['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n 'area',\n 'height',\n 'perimeter',\n 'width']</pre> In\u00a0[\u00a0]: Copied! <pre>dataSet = [\"bat\", \"dog\", \"cat\", \"bat\", \"dog\", \"cow\", \"bat\", \"dog\", \"dog\", \"cow\", \"cow\", \"cow\", \"cow\", \"car\", \"bat\", \"dog\", \"car\", \"car\", \"cat\", \"dog\", \"cat\", \"dog\", \"bat\", \"cat\", \"car\", \"car\", \"dog\", \"cat\", \"cow\", \"cow\", \"cow\", \"cow\", \"cow\", \"car\", \"cat\", \"car\", \"cat\", \"cat\", \"bat\", \"dog\", \"bat\", \"bat\", \"car\", \"dog\", \"dog\", \"cow\", \"dog\", \"cow\", \"dog\", \"bat\", \"cow\", \"dog\", \"bat\", \"dog\", \"bat\", \"cow\", \"bat\", \"bat\", \"car\", \"bat\", \"car\", \"bat\", \"cat\", \"bat\", \"cat\", \"car\", \"cow\", \"car\", \"bat\", \"dog\", \"dog\", \"cat\", \"cat\", \"car\", \"cow\", \"car\", \"cow\", \"cat\", \"cat\", \"cat\", \"cow\", \"bat\", \"cat\", \"car\", \"cat\", \"bat\", \"cat\", \"car\", \"cow\", \"car\", \"dog\", \"dog\", \"dog\", \"car\", \"bat\", \"bat\", \"car\", \"car\", \"dog\", \"car\", \"car\", \"cow\", \"car\", \"cow\", \"dog\", \"bat\", \"cow\", \"bat\", \"bat\", \"car\", \"dog\", \"cat\", \"car\", \"car\", \"dog\", \"dog\", \"cow\", \"cow\", \"cat\", \"cat\", \"cat\", \"bat\", \"dog\", \"cat\", \"bat\", \"cat\", \"cat\", \"cow\", \"cow\", \"cat\"]\nlabel = [1,2,0,1,2,3,1,2,2,3,3,3,3,4,1,2,4,4,0,2,0,2,1,0,4,4,2,0,3,3,3,3,3,4,0,4,0,0,1,2,1,1,4,2,2,3,2,3,2,1,3,2,1,2,1,3,1,1,4,1,4,1,0,1,0,4,3,4,1,2,2,0,0,4,3,4,3,0,0,0,3,1,0,4,0,1,0,4,3,4,2,2,2,4,1,1,4,4,2,4,4,3,4,3,2,1,3,1,1,4,2,0,4,4,2,2,3,3,0,0,0,1,2,0,1,0,0,3,3,0]\n\nclass Animals:\n  def __init__(self, dataSet, label):\n    self.dataSet = dataSet\n    self.label = label\n\n  def __len__(self):\n    return len(self.dataSet)\n\nanimal = Animals(dataSet, label)\nlen(animal), animal.__len__()\n</pre> dataSet = [\"bat\", \"dog\", \"cat\", \"bat\", \"dog\", \"cow\", \"bat\", \"dog\", \"dog\", \"cow\", \"cow\", \"cow\", \"cow\", \"car\", \"bat\", \"dog\", \"car\", \"car\", \"cat\", \"dog\", \"cat\", \"dog\", \"bat\", \"cat\", \"car\", \"car\", \"dog\", \"cat\", \"cow\", \"cow\", \"cow\", \"cow\", \"cow\", \"car\", \"cat\", \"car\", \"cat\", \"cat\", \"bat\", \"dog\", \"bat\", \"bat\", \"car\", \"dog\", \"dog\", \"cow\", \"dog\", \"cow\", \"dog\", \"bat\", \"cow\", \"dog\", \"bat\", \"dog\", \"bat\", \"cow\", \"bat\", \"bat\", \"car\", \"bat\", \"car\", \"bat\", \"cat\", \"bat\", \"cat\", \"car\", \"cow\", \"car\", \"bat\", \"dog\", \"dog\", \"cat\", \"cat\", \"car\", \"cow\", \"car\", \"cow\", \"cat\", \"cat\", \"cat\", \"cow\", \"bat\", \"cat\", \"car\", \"cat\", \"bat\", \"cat\", \"car\", \"cow\", \"car\", \"dog\", \"dog\", \"dog\", \"car\", \"bat\", \"bat\", \"car\", \"car\", \"dog\", \"car\", \"car\", \"cow\", \"car\", \"cow\", \"dog\", \"bat\", \"cow\", \"bat\", \"bat\", \"car\", \"dog\", \"cat\", \"car\", \"car\", \"dog\", \"dog\", \"cow\", \"cow\", \"cat\", \"cat\", \"cat\", \"bat\", \"dog\", \"cat\", \"bat\", \"cat\", \"cat\", \"cow\", \"cow\", \"cat\"] label = [1,2,0,1,2,3,1,2,2,3,3,3,3,4,1,2,4,4,0,2,0,2,1,0,4,4,2,0,3,3,3,3,3,4,0,4,0,0,1,2,1,1,4,2,2,3,2,3,2,1,3,2,1,2,1,3,1,1,4,1,4,1,0,1,0,4,3,4,1,2,2,0,0,4,3,4,3,0,0,0,3,1,0,4,0,1,0,4,3,4,2,2,2,4,1,1,4,4,2,4,4,3,4,3,2,1,3,1,1,4,2,0,4,4,2,2,3,3,0,0,0,1,2,0,1,0,0,3,3,0]  class Animals:   def __init__(self, dataSet, label):     self.dataSet = dataSet     self.label = label    def __len__(self):     return len(self.dataSet)  animal = Animals(dataSet, label) len(animal), animal.__len__()  Out[\u00a0]: <pre>(130, 130)</pre> In\u00a0[\u00a0]: Copied! <pre>class Animals:\n  def __init__(self, dataSet, label):\n    self.dataSet = dataSet\n    self.label = label\n\n  def __len__(self):\n    return len(self.dataSet)\n\n  def __getitem__(self, index):\n    return dataSet[index], label[index]\n\nanimal = Animals(dataSet, label)\n\nanimal.__getitem__(3)\n</pre> class Animals:   def __init__(self, dataSet, label):     self.dataSet = dataSet     self.label = label    def __len__(self):     return len(self.dataSet)    def __getitem__(self, index):     return dataSet[index], label[index]  animal = Animals(dataSet, label)  animal.__getitem__(3) Out[\u00a0]: <pre>('bat', 1)</pre> In\u00a0[\u00a0]: Copied! <pre>for a in animal:\n  print(a)\n</pre> for a in animal:   print(a) <pre>('bat', 1)\n('dog', 2)\n('cat', 0)\n('bat', 1)\n('dog', 2)\n('cow', 3)\n('bat', 1)\n('dog', 2)\n('dog', 2)\n('cow', 3)\n('cow', 3)\n('cow', 3)\n('cow', 3)\n('car', 4)\n('bat', 1)\n('dog', 2)\n('car', 4)\n('car', 4)\n('cat', 0)\n('dog', 2)\n('cat', 0)\n('dog', 2)\n('bat', 1)\n('cat', 0)\n('car', 4)\n('car', 4)\n('dog', 2)\n('cat', 0)\n('cow', 3)\n('cow', 3)\n('cow', 3)\n('cow', 3)\n('cow', 3)\n('car', 4)\n('cat', 0)\n('car', 4)\n('cat', 0)\n('cat', 0)\n('bat', 1)\n('dog', 2)\n('bat', 1)\n('bat', 1)\n('car', 4)\n('dog', 2)\n('dog', 2)\n('cow', 3)\n('dog', 2)\n('cow', 3)\n('dog', 2)\n('bat', 1)\n('cow', 3)\n('dog', 2)\n('bat', 1)\n('dog', 2)\n('bat', 1)\n('cow', 3)\n('bat', 1)\n('bat', 1)\n('car', 4)\n('bat', 1)\n('car', 4)\n('bat', 1)\n('cat', 0)\n('bat', 1)\n('cat', 0)\n('car', 4)\n('cow', 3)\n('car', 4)\n('bat', 1)\n('dog', 2)\n('dog', 2)\n('cat', 0)\n('cat', 0)\n('car', 4)\n('cow', 3)\n('car', 4)\n('cow', 3)\n('cat', 0)\n('cat', 0)\n('cat', 0)\n('cow', 3)\n('bat', 1)\n('cat', 0)\n('car', 4)\n('cat', 0)\n('bat', 1)\n('cat', 0)\n('car', 4)\n('cow', 3)\n('car', 4)\n('dog', 2)\n('dog', 2)\n('dog', 2)\n('car', 4)\n('bat', 1)\n('bat', 1)\n('car', 4)\n('car', 4)\n('dog', 2)\n('car', 4)\n('car', 4)\n('cow', 3)\n('car', 4)\n('cow', 3)\n('dog', 2)\n('bat', 1)\n('cow', 3)\n('bat', 1)\n('bat', 1)\n('car', 4)\n('dog', 2)\n('cat', 0)\n('car', 4)\n('car', 4)\n('dog', 2)\n('dog', 2)\n('cow', 3)\n('cow', 3)\n('cat', 0)\n('cat', 0)\n('cat', 0)\n('bat', 1)\n('dog', 2)\n('cat', 0)\n('bat', 1)\n('cat', 0)\n('cat', 0)\n('cow', 3)\n('cow', 3)\n('cat', 0)\n</pre> In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MyDataset(Dataset):\n  def __init__(self, dataSet, label):\n    self.dataSet = dataSet\n    self.label = label\n\n  def __len__(self):\n    return len(self.dataSet)\n\n  def __getitem__(self, index):\n    return self.dataSet[index], self.label[index]\n\n\ndataset = MyDataset(dataSet, label)\n</pre> import torch from torch.utils.data import Dataset, DataLoader  class MyDataset(Dataset):   def __init__(self, dataSet, label):     self.dataSet = dataSet     self.label = label    def __len__(self):     return len(self.dataSet)    def __getitem__(self, index):     return self.dataSet[index], self.label[index]   dataset = MyDataset(dataSet, label) In\u00a0[\u00a0]: Copied! <pre>mydataloader = DataLoader(dataset, batch_size = 8, shuffle = True)\n</pre> mydataloader = DataLoader(dataset, batch_size = 8, shuffle = True) In\u00a0[\u00a0]: Copied! <pre>for x, y in mydataloader:\n  print(x, y)\n  break\n</pre> for x, y in mydataloader:   print(x, y)   break <pre>('cow', 'bat', 'cow', 'bat', 'cat', 'dog', 'car', 'cow') tensor([3, 1, 3, 1, 0, 2, 4, 3])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/deep_learning/Python_101/#contidionals","title":"CONTIDIONALS\u00b6","text":""},{"location":"notebooks/deep_learning/code1/","title":"Code1","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n</pre> from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Train Phase transformations\ntrain_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\n\n# Test Phase transformations\ntest_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n</pre> # Train Phase transformations train_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ])  # Test Phase transformations test_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                        transforms.Normalize((0.1307,), (0.3081,))                                        ])  In\u00a0[\u00a0]: Copied! <pre>train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n</pre> train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms) test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00&lt;00:00, 102760359.88it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00&lt;00:00, 59148288.00it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00&lt;00:00, 25181936.14it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00&lt;00:00, 15653680.17it/s]\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n</pre> SEED = 1  # CUDA? cuda = torch.cuda.is_available() print(\"CUDA Available?\", cuda)  # For reproducibility torch.manual_seed(SEED)  if cuda:     torch.cuda.manual_seed(SEED)  # dataloader arguments - something you'll fetch these from cmdprmt dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)  # train dataloader train_loader = torch.utils.data.DataLoader(train, **dataloader_args)  # test dataloader test_loader = torch.utils.data.DataLoader(test, **dataloader_args) <pre>CUDA Available? True\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</pre> In\u00a0[\u00a0]: Copied! <pre># We'd need to convert it into Numpy! Remember above we have converted it into tensors already\ntrain_data = train.train_data\ntrain_data = train.transform(train_data.numpy())\n\nprint('[Train]')\nprint(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\nprint(' - Tensor Shape:', train.train_data.size())\nprint(' - min:', torch.min(train_data))\nprint(' - max:', torch.max(train_data))\nprint(' - mean:', torch.mean(train_data))\nprint(' - std:', torch.std(train_data))\nprint(' - var:', torch.var(train_data))\n\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\nprint(images.shape)\nprint(labels.shape)\n\n# Let's visualize some of the images\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n</pre> # We'd need to convert it into Numpy! Remember above we have converted it into tensors already train_data = train.train_data train_data = train.transform(train_data.numpy())  print('[Train]') print(' - Numpy Shape:', train.train_data.cpu().numpy().shape) print(' - Tensor Shape:', train.train_data.size()) print(' - min:', torch.min(train_data)) print(' - max:', torch.max(train_data)) print(' - mean:', torch.mean(train_data)) print(' - std:', torch.std(train_data)) print(' - var:', torch.var(train_data))  dataiter = iter(train_loader) images, labels = next(dataiter)  print(images.shape) print(labels.shape)  # Let's visualize some of the images %matplotlib inline import matplotlib.pyplot as plt  plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')  <pre>[Train]\n - Numpy Shape: (60000, 28, 28)\n - Tensor Shape: torch.Size([60000, 28, 28])\n - min: tensor(-0.4242)\n - max: tensor(2.8215)\n - mean: tensor(-0.0001)\n - std: tensor(1.0000)\n - var: tensor(1.0001)\ntorch.Size([128, 1, 28, 28])\ntorch.Size([128])\n</pre> Out[\u00a0]: <pre>&lt;matplotlib.image.AxesImage at 0x7f62fa034f10&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>figure = plt.figure()\nnum_of_images = 60\nfor index in range(1, num_of_images + 1):\n    plt.subplot(6, 10, index)\n    plt.axis('off')\n    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n</pre> figure = plt.figure() num_of_images = 60 for index in range(1, num_of_images + 1):     plt.subplot(6, 10, index)     plt.axis('off')     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r') In\u00a0[\u00a0]: Copied! <pre># simple transform\nsimple_transforms = transforms.Compose([\n                                      #  transforms.Resize((28, 28)),\n                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n                                       transforms.ToTensor(),\n                                      #  transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n                                       # Note the difference between (0.1307) and (0.1307,)\n                                       ])\nexp = datasets.MNIST('./data', train=True, download=True, transform=simple_transforms)\nexp_data = exp.train_data\nexp_data = exp.transform(exp_data.numpy())\n\nprint('[Train]')\nprint(' - Numpy Shape:', exp.train_data.cpu().numpy().shape)\nprint(' - Tensor Shape:', exp.train_data.size())\nprint(' - min:', torch.min(exp_data))\nprint(' - max:', torch.max(exp_data))\nprint(' - mean:', torch.mean(exp_data))\nprint(' - std:', torch.std(exp_data))\nprint(' - var:', torch.var(exp_data))\n</pre> # simple transform simple_transforms = transforms.Compose([                                       #  transforms.Resize((28, 28)),                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),                                        transforms.ToTensor(),                                       #  transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.                                        # Note the difference between (0.1307) and (0.1307,)                                        ]) exp = datasets.MNIST('./data', train=True, download=True, transform=simple_transforms) exp_data = exp.train_data exp_data = exp.transform(exp_data.numpy())  print('[Train]') print(' - Numpy Shape:', exp.train_data.cpu().numpy().shape) print(' - Tensor Shape:', exp.train_data.size()) print(' - min:', torch.min(exp_data)) print(' - max:', torch.max(exp_data)) print(' - mean:', torch.mean(exp_data)) print(' - std:', torch.std(exp_data)) print(' - var:', torch.var(exp_data)) <pre>[Train]\n - Numpy Shape: (60000, 28, 28)\n - Tensor Shape: torch.Size([60000, 28, 28])\n - min: tensor(0.)\n - max: tensor(1.)\n - mean: tensor(0.1307)\n - std: tensor(0.3081)\n - var: tensor(0.0949)\n</pre> In\u00a0[\u00a0]: Copied! <pre>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) # 28&gt;28 | 3\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 28 &gt; 28 |  5\n        self.pool1 = nn.MaxPool2d(2, 2) # 28 &gt; 14 | 10\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1) # 14&gt; 14 | 12\n        self.conv4 = nn.Conv2d(128, 256, 3, padding=1) #14 &gt; 14 | 14\n        self.pool2 = nn.MaxPool2d(2, 2) # 14 &gt; 7 | 28\n        self.conv5 = nn.Conv2d(256, 512, 3) # 7 &gt; 5 | 30\n        self.conv6 = nn.Conv2d(512, 1024, 3) # 5 &gt; 3 | 32 | 3*3*1024 | 3x3x1024x10 |\n        self.conv7 = nn.Conv2d(1024, 10, 3) # 3 &gt; 1 | 34 | &gt; 1x1x10\n\n    def forward(self, x):\n        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n        # x = F.relu(self.conv7(x))\n        x = self.conv7(x)\n        x = x.view(-1, 10) #1x1x10&gt; 10\n        return F.log_softmax(x, dim=-1)\n</pre> class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         self.conv1 = nn.Conv2d(1, 32, 3, padding=1) # 28&gt;28 | 3         self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 28 &gt; 28 |  5         self.pool1 = nn.MaxPool2d(2, 2) # 28 &gt; 14 | 10         self.conv3 = nn.Conv2d(64, 128, 3, padding=1) # 14&gt; 14 | 12         self.conv4 = nn.Conv2d(128, 256, 3, padding=1) #14 &gt; 14 | 14         self.pool2 = nn.MaxPool2d(2, 2) # 14 &gt; 7 | 28         self.conv5 = nn.Conv2d(256, 512, 3) # 7 &gt; 5 | 30         self.conv6 = nn.Conv2d(512, 1024, 3) # 5 &gt; 3 | 32 | 3*3*1024 | 3x3x1024x10 |         self.conv7 = nn.Conv2d(1024, 10, 3) # 3 &gt; 1 | 34 | &gt; 1x1x10      def forward(self, x):         x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))         x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))         x = F.relu(self.conv6(F.relu(self.conv5(x))))         # x = F.relu(self.conv7(x))         x = self.conv7(x)         x = x.view(-1, 10) #1x1x10&gt; 10         return F.log_softmax(x, dim=-1) In\u00a0[\u00a0]: Copied! <pre>!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))\n</pre> !pip install torchsummary from torchsummary import summary use_cuda = torch.cuda.is_available() device = torch.device(\"cuda\" if use_cuda else \"cpu\") print(device) model = Net().to(device) summary(model, input_size=(1, 28, 28)) <pre>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 28, 28]             320\n            Conv2d-2           [-1, 64, 28, 28]          18,496\n         MaxPool2d-3           [-1, 64, 14, 14]               0\n            Conv2d-4          [-1, 128, 14, 14]          73,856\n            Conv2d-5          [-1, 256, 14, 14]         295,168\n         MaxPool2d-6            [-1, 256, 7, 7]               0\n            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n            Conv2d-9             [-1, 10, 1, 1]          92,170\n================================================================\nTotal params: 6,379,786\nTrainable params: 6,379,786\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 1.51\nParams size (MB): 24.34\nEstimated Total Size (MB): 25.85\n----------------------------------------------------------------\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))\n</pre> from tqdm import tqdm  train_losses = [] test_losses = [] train_acc = [] test_acc = []  def train(model, device, train_loader, optimizer, epoch):   model.train()   pbar = tqdm(train_loader)   correct = 0   processed = 0   for batch_idx, (data, target) in enumerate(pbar):     # get samples     data, target = data.to(device), target.to(device)      # Init     optimizer.zero_grad()     # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.     # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.      # Predict     y_pred = model(data)      # Calculate loss     loss = F.nll_loss(y_pred, target)     train_losses.append(loss)      # Backpropagation     loss.backward()     optimizer.step()      # Update pbar-tqdm      pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability     correct += pred.eq(target.view_as(pred)).sum().item()     processed += len(data)      pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')     train_acc.append(100*correct/processed)  def test(model, device, test_loader):     model.eval()     test_loss = 0     correct = 0     with torch.no_grad():         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability             correct += pred.eq(target.view_as(pred)).sum().item()      test_loss /= len(test_loader.dataset)     test_losses.append(test_loss)      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(         test_loss, correct, len(test_loader.dataset),         100. * correct / len(test_loader.dataset)))      test_acc.append(100. * correct / len(test_loader.dataset)) In\u00a0[\u00a0]: Copied! <pre>model =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)\n</pre> model =  Net().to(device) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) EPOCHS = 20 for epoch in range(EPOCHS):     print(\"EPOCH:\", epoch)     train(model, device, train_loader, optimizer, epoch)     test(model, device, test_loader) <pre>EPOCH: 0\n</pre> <pre>Loss=0.0525066964328289 Batch_id=468 Accuracy=88.03: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.47it/s]\n</pre> <pre>\nTest set: Average loss: 0.0626, Accuracy: 9787/10000 (97.87%)\n\nEPOCH: 1\n</pre> <pre>Loss=0.021765952929854393 Batch_id=468 Accuracy=98.41: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:23&lt;00:00, 20.25it/s]\n</pre> <pre>\nTest set: Average loss: 0.0349, Accuracy: 9883/10000 (98.83%)\n\nEPOCH: 2\n</pre> <pre>Loss=0.06084553897380829 Batch_id=468 Accuracy=98.93: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.58it/s]\n</pre> <pre>\nTest set: Average loss: 0.0324, Accuracy: 9885/10000 (98.85%)\n\nEPOCH: 3\n</pre> <pre>Loss=0.006625731009989977 Batch_id=468 Accuracy=99.22: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.62it/s]\n</pre> <pre>\nTest set: Average loss: 0.0289, Accuracy: 9906/10000 (99.06%)\n\nEPOCH: 4\n</pre> <pre>Loss=0.010127569548785686 Batch_id=468 Accuracy=99.44: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.98it/s]\n</pre> <pre>\nTest set: Average loss: 0.0286, Accuracy: 9901/10000 (99.01%)\n\nEPOCH: 5\n</pre> <pre>Loss=0.005708999931812286 Batch_id=468 Accuracy=99.54: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.40it/s]\n</pre> <pre>\nTest set: Average loss: 0.0269, Accuracy: 9913/10000 (99.13%)\n\nEPOCH: 6\n</pre> <pre>Loss=0.039499230682849884 Batch_id=468 Accuracy=99.67: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 21.55it/s]\n</pre> <pre>\nTest set: Average loss: 0.0263, Accuracy: 9917/10000 (99.17%)\n\nEPOCH: 7\n</pre> <pre>Loss=0.011014115996658802 Batch_id=468 Accuracy=99.71: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.90it/s]\n</pre> <pre>\nTest set: Average loss: 0.0292, Accuracy: 9915/10000 (99.15%)\n\nEPOCH: 8\n</pre> <pre>Loss=0.003996113780885935 Batch_id=468 Accuracy=99.76: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.08it/s]\n</pre> <pre>\nTest set: Average loss: 0.0279, Accuracy: 9921/10000 (99.21%)\n\nEPOCH: 9\n</pre> <pre>Loss=0.0004153123591095209 Batch_id=468 Accuracy=99.81: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.37it/s]\n</pre> <pre>\nTest set: Average loss: 0.0275, Accuracy: 9924/10000 (99.24%)\n\nEPOCH: 10\n</pre> <pre>Loss=0.0015012939693406224 Batch_id=468 Accuracy=99.86: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.36it/s]\n</pre> <pre>\nTest set: Average loss: 0.0247, Accuracy: 9929/10000 (99.29%)\n\nEPOCH: 11\n</pre> <pre>Loss=0.0007965001277625561 Batch_id=468 Accuracy=99.87: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.38it/s]\n</pre> <pre>\nTest set: Average loss: 0.0351, Accuracy: 9910/10000 (99.10%)\n\nEPOCH: 12\n</pre> <pre>Loss=1.392918511555763e-05 Batch_id=468 Accuracy=99.85: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.25it/s]\n</pre> <pre>\nTest set: Average loss: 0.0290, Accuracy: 9926/10000 (99.26%)\n\nEPOCH: 13\n</pre> <pre>Loss=0.004478180781006813 Batch_id=468 Accuracy=99.95: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 22.23it/s]\n</pre> <pre>\nTest set: Average loss: 0.0414, Accuracy: 9913/10000 (99.13%)\n\nEPOCH: 14\n</pre> <pre>Loss=6.267857679631561e-05 Batch_id=468 Accuracy=99.86: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.59it/s]\n</pre> <pre>\nTest set: Average loss: 0.0277, Accuracy: 9927/10000 (99.27%)\n\nEPOCH: 15\n</pre> <pre>Loss=0.0001122226458392106 Batch_id=468 Accuracy=99.94: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.11it/s]\n</pre> <pre>\nTest set: Average loss: 0.0259, Accuracy: 9935/10000 (99.35%)\n\nEPOCH: 16\n</pre> <pre>Loss=1.5033109775686171e-05 Batch_id=468 Accuracy=99.97: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 22.45it/s]\n</pre> <pre>\nTest set: Average loss: 0.0323, Accuracy: 9929/10000 (99.29%)\n\nEPOCH: 17\n</pre> <pre>Loss=0.0002341804065508768 Batch_id=468 Accuracy=99.96: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 22.03it/s]\n</pre> <pre>\nTest set: Average loss: 0.0282, Accuracy: 9936/10000 (99.36%)\n\nEPOCH: 18\n</pre> <pre>Loss=0.0012386600719764829 Batch_id=468 Accuracy=99.95: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:20&lt;00:00, 23.23it/s]\n</pre> <pre>\nTest set: Average loss: 0.0302, Accuracy: 9928/10000 (99.28%)\n\nEPOCH: 19\n</pre> <pre>Loss=1.492558112659026e-06 Batch_id=468 Accuracy=99.99: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 469/469 [00:21&lt;00:00, 22.15it/s]\n</pre> <pre>\nTest set: Average loss: 0.0303, Accuracy: 9934/10000 (99.34%)\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>train_losses\n</pre> train_losses Out[\u00a0]: <pre>[tensor(2.3017, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.3031, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.3023, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.3017, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2997, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.3009, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2987, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2978, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2964, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2973, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2921, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2912, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2936, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2912, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2883, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2890, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2869, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2879, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2867, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2794, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2727, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2695, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2696, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2647, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2660, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2716, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2394, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2451, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2322, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.2143, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1959, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1854, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1601, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1201, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.1235, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(2.0537, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.9767, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.8391, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.7797, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.6066, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.4733, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.2462, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.2161, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.9700, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.7728, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.6099, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.6225, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.2694, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.3795, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.2932, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.1571, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.8804, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.7325, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.5766, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.6660, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.6990, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.8783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.7058, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.4005, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.0583, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.9190, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.0533, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.0997, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(1.0225, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.8526, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.6226, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.5293, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.5073, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.4585, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.5133, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.7091, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.5926, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.7015, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.6666, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.4147, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.6787, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.5541, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3308, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.5109, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.5278, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.4731, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.4186, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3945, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.4328, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.4493, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3218, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3294, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.4839, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3504, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.4219, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2784, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2291, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2040, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3992, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.4382, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3328, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3648, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2544, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2816, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2609, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1906, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2133, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3710, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2301, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2277, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2315, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2982, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2603, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2822, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2255, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1995, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2657, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1882, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3082, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1633, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1405, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2106, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2234, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2429, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2433, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2071, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1918, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2642, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1369, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3280, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1692, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2386, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3241, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1662, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1961, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1562, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1780, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1519, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1098, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1901, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1509, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2257, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2206, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1399, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3302, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3123, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2246, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2256, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1706, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2163, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1828, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1191, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1493, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1711, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1561, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3076, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1575, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1269, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1468, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1230, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1241, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1401, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1587, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1165, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1186, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1944, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1229, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0980, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1055, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1635, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2880, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1290, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1565, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1673, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1917, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2034, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1961, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0661, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1972, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2917, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1575, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1529, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1041, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0854, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1079, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1533, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1552, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1427, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1219, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1399, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1215, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1687, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1871, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2230, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1476, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1364, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1187, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0443, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1589, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0508, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1469, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0876, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1374, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0679, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1836, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0786, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1186, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0924, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1033, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1099, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1369, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0383, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2853, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0533, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2387, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0986, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0618, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1002, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0592, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2270, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2207, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1620, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1939, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1356, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0937, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1434, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0637, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2101, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1594, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0676, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0808, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1336, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1426, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1319, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2041, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1066, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1084, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0724, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0733, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0872, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0846, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1223, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1848, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1747, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0706, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1140, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0698, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0832, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1799, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0664, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1327, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1067, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0651, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0751, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1003, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1892, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.3820, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0688, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1810, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0998, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1344, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0830, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0686, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1871, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1163, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1065, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1505, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0880, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0676, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1369, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0713, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0439, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0939, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0989, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0586, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1520, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1258, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1419, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1814, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1644, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0721, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1483, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1016, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0596, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0453, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0849, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1247, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0740, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0646, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0768, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1919, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1751, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1171, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2716, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0860, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0935, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0988, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0811, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1221, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0596, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0499, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1776, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0818, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0869, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1389, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0579, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0784, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0887, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0332, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1031, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0699, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1481, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0666, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1766, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1354, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0924, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0693, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0385, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0646, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0693, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0508, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0942, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1357, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0415, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0857, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0738, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1298, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0923, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0831, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1011, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0601, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0434, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0496, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1219, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1127, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0374, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0928, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1133, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.2473, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0756, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0634, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0627, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0856, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0557, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1570, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1215, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0687, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0156, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1284, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0250, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0660, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0415, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0760, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1981, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0157, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0770, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1149, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1253, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0447, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0362, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1104, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0553, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0439, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0699, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0786, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0308, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0763, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0675, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0283, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0856, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0207, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0699, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0366, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0515, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0478, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0328, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0540, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0958, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1159, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0197, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0845, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0942, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0272, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1141, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0520, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0586, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0909, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0651, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0742, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0699, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0716, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1061, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0578, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0831, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0850, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1498, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0474, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0441, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0938, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1247, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1127, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0958, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0938, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1806, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1307, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0975, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0822, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1236, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0522, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0796, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0623, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0995, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1027, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1499, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1812, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1009, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0900, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0452, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1152, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0729, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0759, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0520, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0618, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0705, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1204, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0998, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0895, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0519, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0361, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0494, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0933, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0275, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0479, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0561, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0441, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1158, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0583, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0262, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1222, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0352, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0532, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0383, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0531, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0127, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0769, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0532, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0692, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0196, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0839, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0258, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0443, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0376, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0208, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0877, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0570, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0362, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1042, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0382, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0525, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0378, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0941, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0329, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0533, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0231, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0878, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0741, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0998, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0469, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0227, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0263, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1132, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0527, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1177, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1036, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0220, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0437, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0992, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0717, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0730, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1401, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0805, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0221, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0437, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0197, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0663, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0683, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0275, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0250, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0767, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0508, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0511, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0254, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0400, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0234, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0842, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0998, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1193, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0320, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0216, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0572, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0115, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0539, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0462, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1070, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0520, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0538, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0362, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0800, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0134, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0880, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0346, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0726, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0282, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0874, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0114, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0137, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0241, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0219, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0758, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0429, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1406, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0792, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0426, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0558, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0419, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0519, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0327, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1461, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0457, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1110, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0800, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0170, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0869, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0324, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0488, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0825, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0936, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0370, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0321, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0106, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0343, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1179, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0295, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0911, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0157, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0193, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0288, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0167, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0369, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0293, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0767, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0254, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0426, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0494, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0400, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0802, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0266, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1075, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0104, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0194, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1075, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1072, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0474, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0990, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0848, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0741, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0456, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0050, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0570, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1117, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1268, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0465, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1716, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1120, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0406, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1019, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0439, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0273, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1072, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0710, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0391, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0113, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0520, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1232, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0434, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0356, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0641, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0276, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0318, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0218, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0347, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0606, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0269, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0566, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0726, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0631, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0784, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1415, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0637, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0880, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0187, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0846, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0194, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0440, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0287, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0288, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0453, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0386, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0237, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0845, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0418, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0429, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0483, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0431, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0924, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0391, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0243, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0233, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0301, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0077, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0600, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0634, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0259, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0645, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0165, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1441, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0717, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0684, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0175, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0292, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0871, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0375, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0360, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0498, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0257, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0470, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0233, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0430, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0888, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1345, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1017, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1583, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0410, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0317, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0566, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0710, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0601, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0399, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0226, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0464, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0832, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0272, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0324, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0949, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1150, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0436, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1117, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0580, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0175, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0327, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0290, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1070, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0876, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0499, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0300, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0212, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0345, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0166, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1238, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1039, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0177, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0550, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0487, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0232, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0102, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0192, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0893, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0181, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0447, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0391, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0242, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0096, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0924, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0487, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0480, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0677, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1031, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0162, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0926, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0381, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0554, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0309, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0459, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0710, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0337, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0284, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0433, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0641, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0481, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0910, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0421, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0899, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0267, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0761, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0793, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0448, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0131, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0122, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0271, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0610, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0500, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0482, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0795, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0459, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0143, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0345, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0145, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0072, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0650, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1040, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0290, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0830, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0693, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0192, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0248, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1082, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0106, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0221, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0713, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0694, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0663, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0906, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0286, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0504, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0472, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0243, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0718, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0248, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0512, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0535, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0973, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0166, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0983, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0715, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0134, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0234, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0383, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0158, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0709, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1208, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0671, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0382, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0308, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0506, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1453, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0922, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0434, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0339, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0995, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0430, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0166, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0706, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0460, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0324, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0363, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0595, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0399, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0205, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0583, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0594, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0205, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0249, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0102, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0298, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0318, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0088, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0217, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0243, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0107, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0322, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0379, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0171, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0542, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1216, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0283, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0215, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0223, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0325, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0646, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0049, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0662, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0681, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0138, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0073, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0234, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0804, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0401, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0430, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0647, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0641, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0405, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0197, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0508, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0576, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1799, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0672, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0837, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0421, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0977, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0476, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0762, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0763, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0531, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0415, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0197, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0229, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0507, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0429, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0518, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0176, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0307, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0281, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0254, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0255, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0135, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0714, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0201, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0108, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0418, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0763, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0164, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0708, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0162, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0635, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0446, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0257, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0456, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0336, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0680, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0082, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0259, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0534, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0139, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0566, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0827, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1062, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0196, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0798, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0371, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0517, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1145, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0296, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0188, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0284, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0679, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0423, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0101, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0272, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0440, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0200, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0087, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0170, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0368, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0520, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0479, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0559, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0324, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1313, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0238, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0891, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1167, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0916, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0143, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0702, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0496, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0299, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0456, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0776, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0256, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0996, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0243, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1597, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0770, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0675, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0403, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0936, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1172, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0470, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0809, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0340, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0225, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0313, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0294, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1213, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0393, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0186, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0540, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0190, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0146, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0523, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0528, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0416, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0473, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0421, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0559, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0281, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.1331, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0371, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0415, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0156, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0196, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0338, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0741, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0483, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0370, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0412, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0457, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0128, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0246, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0218, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0134, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0203, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0281, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0253, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0132, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0130, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0097, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0097, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0410, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0215, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0280, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0404, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0155, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0168, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0871, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0147, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0355, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0209, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0249, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0041, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0439, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0192, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0304, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0286, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0353, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0196, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0342, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0500, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0363, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0200, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0635, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0251, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0048, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0115, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0138, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0234, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0409, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0228, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0304, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0413, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0282, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0113, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0341, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0646, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0076, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0104, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0061, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0204, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0116, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0340, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0690, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0597, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0118, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0258, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0509, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0124, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0117, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0221, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0298, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0900, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0478, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n tensor(0.0207, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n ...]</pre> In\u00a0[\u00a0]: Copied! <pre>t = [t_items.item() for t_items in train_losses]\n</pre> t = [t_items.item() for t_items in train_losses] In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(2,2,figsize=(15,10))\naxs[0, 0].plot(t)\naxs[0, 0].set_title(\"Training Loss\")\naxs[1, 0].plot(train_acc)\naxs[1, 0].set_title(\"Training Accuracy\")\naxs[0, 1].plot(test_losses)\naxs[0, 1].set_title(\"Test Loss\")\naxs[1, 1].plot(test_acc)\naxs[1, 1].set_title(\"Test Accuracy\")\n</pre> fig, axs = plt.subplots(2,2,figsize=(15,10)) axs[0, 0].plot(t) axs[0, 0].set_title(\"Training Loss\") axs[1, 0].plot(train_acc) axs[1, 0].set_title(\"Training Accuracy\") axs[0, 1].plot(test_losses) axs[0, 1].set_title(\"Test Loss\") axs[1, 1].plot(test_acc) axs[1, 1].set_title(\"Test Accuracy\") Out[\u00a0]: <pre>Text(0.5, 1.0, 'Test Accuracy')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/deep_learning/code1/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"notebooks/deep_learning/code1/#data-transformations","title":"Data Transformations\u00b6","text":"<p>We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.</p> <p>Here is the list of all the transformations which come pre-built with PyTorch</p> <ol> <li>Compose</li> <li>ToTensor</li> <li>ToPILImage</li> <li>Normalize</li> <li>Resize</li> <li>Scale</li> <li>CenterCrop</li> <li>Pad</li> <li>Lambda</li> <li>RandomApply</li> <li>RandomChoice</li> <li>RandomOrder</li> <li>RandomCrop</li> <li>RandomHorizontalFlip</li> <li>RandomVerticalFlip</li> <li>RandomResizedCrop</li> <li>RandomSizedCrop</li> <li>FiveCrop</li> <li>TenCrop</li> <li>LinearTransformation</li> <li>ColorJitter</li> <li>RandomRotation</li> <li>RandomAffine</li> <li>Grayscale</li> <li>RandomGrayscale</li> <li>RandomPerspective</li> <li>RandomErasing</li> </ol> <p>You can read more about them here</p>"},{"location":"notebooks/deep_learning/code1/#dataset-and-creating-traintest-split","title":"Dataset and Creating Train/Test Split\u00b6","text":""},{"location":"notebooks/deep_learning/code1/#dataloader-arguments-testtrain-dataloaders","title":"Dataloader Arguments &amp; Test/Train Dataloaders\u00b6","text":""},{"location":"notebooks/deep_learning/code1/#data-statistics","title":"Data Statistics\u00b6","text":"<p>It is important to know your data very well. Let's check some of the statistics around our data and how it actually looks like</p>"},{"location":"notebooks/deep_learning/code1/#more","title":"MORE\u00b6","text":"<p>It is important that we view as many images as possible. This is required to get some idea on image augmentation later on</p>"},{"location":"notebooks/deep_learning/code1/#how-did-we-get-those-mean-and-std-values-which-we-used-above","title":"How did we get those mean and std values which we used above?\u00b6","text":"<p>Let's run a small experiment</p>"},{"location":"notebooks/deep_learning/code1/#the-model","title":"The model\u00b6","text":"<p>Let's start with the model we first saw</p>"},{"location":"notebooks/deep_learning/code1/#model-params","title":"Model Params\u00b6","text":"<p>Can't emphasize on how important viewing Model Summary is. Unfortunately, there is no in-built model visualizer, so we have to take external help</p>"},{"location":"notebooks/deep_learning/code1/#training-and-testing","title":"Training and Testing\u00b6","text":"<p>All right, so we have 6.3M params, and that's too many, we know that. But the purpose of this notebook is to set things right for our future experiments.</p> <p>Looking at logs can be boring, so we'll introduce tqdm progressbar to get cooler logs.</p> <p>Let's write train and test functions</p>"},{"location":"notebooks/deep_learning/code1/#lets-train-and-test-our-model","title":"Let's Train and test our model\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit1_introduction/","title":"Unit1 introduction","text":"In\u00a0[\u00a0]: Copied! <pre>!apt install swig cmake\n</pre> !apt install swig cmake In\u00a0[\u00a0]: Copied! <pre>!pip install stable-baselines3==2.0.0a5\n!pip install swig\n!pip install gymnasium[box2d]\n</pre> !pip install stable-baselines3==2.0.0a5 !pip install swig !pip install gymnasium[box2d] In\u00a0[\u00a0]: Copied! <pre>!sudo apt-get update\n!sudo apt-get install -y python3-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay\n</pre> !sudo apt-get update !sudo apt-get install -y python3-opengl !apt install ffmpeg !apt install xvfb !pip3 install pyvirtualdisplay <p>To make sure the new installed libraries are used, sometimes it's required to restart the notebook runtime. The next cell will force the runtime to crash, so you'll need to connect again and run the code starting from here. Thanks to this trick, we will be able to run our virtual screen.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nos.kill(os.getpid(), 9)\n</pre> import os os.kill(os.getpid(), 9) In\u00a0[\u00a0]: Copied! <pre># Virtual display\nfrom pyvirtualdisplay import Display\nimport matplotlib.pyplot as plt\nfrom IPython import display as ipythondisplay\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n</pre> # Virtual display from pyvirtualdisplay import Display import matplotlib.pyplot as plt from IPython import display as ipythondisplay  virtual_display = Display(visible=0, size=(1400, 900)) virtual_display.start() In\u00a0[\u00a0]: Copied! <pre>import gymnasium\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.monitor import Monitor\n</pre> import gymnasium  from stable_baselines3 import PPO from stable_baselines3.common.env_util import make_vec_env from stable_baselines3.common.evaluation import evaluate_policy from stable_baselines3.common.monitor import Monitor <p>At each step:</p> <ul> <li>Our Agent receives\u00a0a state (S0)\u00a0from the\u00a0Environment\u00a0\u2014 we receive the first frame of our game (Environment).</li> <li>Based on that\u00a0state (S0),\u00a0the Agent takes an\u00a0action (A0)\u00a0\u2014 our Agent will move to the right.</li> <li>The environment transitions to a\u00a0new state (S1)\u00a0\u2014 new frame.</li> <li>The environment gives some\u00a0reward (R1)\u00a0to the Agent \u2014 we\u2019re not dead\u00a0(Positive Reward +1).</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import gymnasium as gym\n\n# First, we create our environment called LunarLander-v2\nenv = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n\n# Then we reset this environment\nobservation, info = env.reset()\n\nfor _ in range(20):\n  # Take a random action\n  action = env.action_space.sample()\n  print(\"Action taken:\", action)\n\n  # Do this action in the environment and get\n  # next_state, reward, terminated, truncated and info\n  observation, reward, terminated, truncated, info = env.step(action)\n  screen = env.render()\n\n  plt.imshow(screen)\n  ipythondisplay.clear_output(wait=True)\n  ipythondisplay.display(plt.gcf())\n\n\n  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n  if terminated or truncated:\n      # Reset the environment\n      print(\"Environment is reset\")\n      observation, info = env.reset()\n\nenv.close()\n</pre> import gymnasium as gym  # First, we create our environment called LunarLander-v2 env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")  # Then we reset this environment observation, info = env.reset()  for _ in range(20):   # Take a random action   action = env.action_space.sample()   print(\"Action taken:\", action)    # Do this action in the environment and get   # next_state, reward, terminated, truncated and info   observation, reward, terminated, truncated, info = env.step(action)   screen = env.render()    plt.imshow(screen)   ipythondisplay.clear_output(wait=True)   ipythondisplay.display(plt.gcf())     # If the game is terminated (in our case we land, crashed) or truncated (timeout)   if terminated or truncated:       # Reset the environment       print(\"Environment is reset\")       observation, info = env.reset()  env.close() In\u00a0[\u00a0]: Copied! <pre># We create our environment with gym.make(\"&lt;name_of_the_environment&gt;\")\nenv = gym.make(\"LunarLander-v2\")\nenv.reset()\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space Shape\", env.observation_space.shape)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n</pre> # We create our environment with gym.make(\"\") env = gym.make(\"LunarLander-v2\") env.reset() print(\"_____OBSERVATION SPACE_____ \\n\") print(\"Observation Space Shape\", env.observation_space.shape) print(\"Sample observation\", env.observation_space.sample()) # Get a random observation <p>We see with <code>Observation Space Shape (8,)</code> that the observation is a vector of size 8, where each value contains different information about the lander:</p> <ul> <li>Horizontal pad coordinate (x)</li> <li>Vertical pad coordinate (y)</li> <li>Horizontal speed (x)</li> <li>Vertical speed (y)</li> <li>Angle</li> <li>Angular speed</li> <li>If the left leg contact point has touched the land (boolean)</li> <li>If the right leg contact point has touched the land (boolean)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n</pre> print(\"\\n _____ACTION SPACE_____ \\n\") print(\"Action Space Shape\", env.action_space.n) print(\"Action Space Sample\", env.action_space.sample()) # Take a random action <p>The action space (the set of possible actions the agent can take) is discrete with 4 actions available \ud83c\udfae:</p> <ul> <li>Action 0: Do nothing,</li> <li>Action 1: Fire left orientation engine,</li> <li>Action 2: Fire the main engine,</li> <li>Action 3: Fire right orientation engine.</li> </ul> <p>Reward function (the function that will give a reward at each timestep) \ud83d\udcb0:</p> <p>After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.</p> <p>For each step, the reward:</p> <ul> <li>Is increased/decreased the closer/further the lander is to the landing pad.</li> <li>Is increased/decreased the slower/faster the lander is moving.</li> <li>Is decreased the more the lander is tilted (angle not horizontal).</li> <li>Is increased by 10 points for each leg that is in contact with the ground.</li> <li>Is decreased by 0.03 points each frame a side engine is firing.</li> <li>Is decreased by 0.3 points each frame the main engine is firing.</li> </ul> <p>The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.</p> <p>An episode is considered a solution if it scores at least 200 points.</p> In\u00a0[\u00a0]: Copied! <pre># Create the environment\nenv = make_vec_env('LunarLander-v2', n_envs=16)\n</pre> # Create the environment env = make_vec_env('LunarLander-v2', n_envs=16) In\u00a0[\u00a0]: Copied! <pre># TODO: Define a PPO MlpPolicy architecture\n# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n# if we had frames as input we would use CnnPolicy\nmodel =\n</pre> # TODO: Define a PPO MlpPolicy architecture # We use MultiLayerPerceptron (MLPPolicy) because the input is a vector, # if we had frames as input we would use CnnPolicy model = In\u00a0[\u00a0]: Copied! <pre># SOLUTION\n# We added some parameters to accelerate the training\nmodel = PPO(\n    policy = 'MlpPolicy',\n    env = env,\n    n_steps = 1024,\n    batch_size = 64,\n    n_epochs = 4,\n    gamma = 0.999,\n    gae_lambda = 0.98,\n    ent_coef = 0.01,\n    verbose=1)\n</pre> # SOLUTION # We added some parameters to accelerate the training model = PPO(     policy = 'MlpPolicy',     env = env,     n_steps = 1024,     batch_size = 64,     n_epochs = 4,     gamma = 0.999,     gae_lambda = 0.98,     ent_coef = 0.01,     verbose=1) In\u00a0[\u00a0]: Copied! <pre># TODO: Train it for 1,000,000 timesteps\n\n# TODO: Specify file name for model and save the model to file\nmodel_name = \"ppo-LunarLander-v2\"\n</pre> # TODO: Train it for 1,000,000 timesteps  # TODO: Specify file name for model and save the model to file model_name = \"ppo-LunarLander-v2\"  In\u00a0[\u00a0]: Copied! <pre># SOLUTION\n# Train it for 1,000,000 timesteps\nmodel.learn(total_timesteps=1000000)\n# Save the model\nmodel_name = \"ppo-LunarLander-v2\"\nmodel.save(model_name)\n</pre> # SOLUTION # Train it for 1,000,000 timesteps model.learn(total_timesteps=1000000) # Save the model model_name = \"ppo-LunarLander-v2\" model.save(model_name) In\u00a0[\u00a0]: Copied! <pre># TODO: Evaluate the agent\n# Create a new environment for evaluation\neval_env =\n\n# Evaluate the model with 10 evaluation episodes and deterministic=True\nmean_reward, std_reward =\n\n# Print the results\n</pre> # TODO: Evaluate the agent # Create a new environment for evaluation eval_env =  # Evaluate the model with 10 evaluation episodes and deterministic=True mean_reward, std_reward =  # Print the results   In\u00a0[\u00a0]: Copied! <pre>#@title\neval_env = Monitor(gym.make(\"LunarLander-v2\", render_mode='rgb_array'))\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n</pre> #@title eval_env = Monitor(gym.make(\"LunarLander-v2\", render_mode='rgb_array')) mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True) print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"},{"location":"notebooks/reinforcement_learning/unit1_introduction/#unit-1-train-your-first-deep-reinforcement-learning-agent","title":"Unit 1: Train your first Deep Reinforcement Learning Agent\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit1_introduction/#install-dependencies-and-create-a-virtual-screen","title":"Install dependencies and create a virtual screen\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit1_introduction/#import-the-packages","title":"Import the packages\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit1_introduction/#understand-gymnasium-and-how-it-works","title":"Understand Gymnasium and how it works\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit1_introduction/#vectorized-environment","title":"Vectorized Environment\u00b6","text":"<ul> <li>We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, we'll have more diverse experiences during the training.</li> </ul>"},{"location":"notebooks/reinforcement_learning/unit1_introduction/#create-the-model","title":"Create the Model\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit1_introduction/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit1_introduction/#train-the-ppo-agent","title":"Train the PPO agent\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit1_introduction/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit1_introduction/#evaluate-the-agent","title":"Evaluate the agent\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit1_introduction/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/","title":"Unit2 q learning","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install gymnasium\n!pip install pygame\n!pip install numpy\n!pip install pickle5\n!pip install pyyaml==6.0\n!pip install imageio\n!pip install imageio_ffmpeg\n!pip install pyglet==1.5.1\n!pip install tqdm\n</pre> !pip install gymnasium !pip install pygame !pip install numpy !pip install pickle5 !pip install pyyaml==6.0 !pip install imageio !pip install imageio_ffmpeg !pip install pyglet==1.5.1 !pip install tqdm In\u00a0[\u00a0]: Copied! <pre>!sudo apt-get update\n!sudo apt-get install -y python3-opengl\n!apt install ffmpeg xvfb\n!pip3 install pyvirtualdisplay\n</pre> !sudo apt-get update !sudo apt-get install -y python3-opengl !apt install ffmpeg xvfb !pip3 install pyvirtualdisplay <p>To make sure the new installed libraries are used, sometimes it's required to restart the notebook runtime. The next cell will force the runtime to crash, so you'll need to connect again and run the code starting from here. Thanks to this trick, we will be able to run our virtual screen.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nos.kill(os.getpid(), 9)\n</pre> import os os.kill(os.getpid(), 9) In\u00a0[\u00a0]: Copied! <pre># Virtual display\nfrom pyvirtualdisplay import Display\nimport matplotlib.pyplot as plt\nfrom IPython import display as ipythondisplay\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n</pre> # Virtual display from pyvirtualdisplay import Display import matplotlib.pyplot as plt from IPython import display as ipythondisplay  virtual_display = Display(visible=0, size=(1400, 900)) virtual_display.start() In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport gymnasium as gym\nimport random\nimport imageio\nimport os\nimport tqdm\n\nimport pickle5 as pickle\nfrom tqdm.notebook import tqdm\n</pre> import numpy as np import gymnasium as gym import random import imageio import os import tqdm  import pickle5 as pickle from tqdm.notebook import tqdm In\u00a0[\u00a0]: Copied! <pre># Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\nenv = gym.make() # TODO use the correct parameters\n</pre> # Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\" env = gym.make() # TODO use the correct parameters In\u00a0[\u00a0]: Copied! <pre>env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n</pre> env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\") <p>You can create your own custom grid like this:</p> <pre>desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\ngym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n</pre> <p>but we'll use the default environment for now.</p> In\u00a0[\u00a0]: Copied! <pre># We create our environment with gym.make(\"&lt;name_of_the_environment&gt;\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space\", env.observation_space)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n</pre> # We create our environment with gym.make(\"\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic). print(\"_____OBSERVATION SPACE_____ \\n\") print(\"Observation Space\", env.observation_space) print(\"Sample observation\", env.observation_space.sample()) # Get a random observation <p>We see with <code>Observation Space Shape Discrete(16)</code> that the observation is an integer representing the agent\u2019s current position as current_row * ncols + current_col (where both the row and col start at 0).</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n</pre> print(\"\\n _____ACTION SPACE_____ \\n\") print(\"Action Space Shape\", env.action_space.n) print(\"Action Space Sample\", env.action_space.sample()) # Take a random action <p>The action space (the set of possible actions the agent can take) is discrete with 4 actions available \ud83c\udfae:</p> <ul> <li>0: GO LEFT</li> <li>1: GO DOWN</li> <li>2: GO RIGHT</li> <li>3: GO UP</li> </ul> <p>Reward function \ud83d\udcb0:</p> <ul> <li>Reach goal: +1</li> <li>Reach hole: 0</li> <li>Reach frozen: 0</li> </ul> In\u00a0[\u00a0]: Copied! <pre>state_space =\nprint(\"There are \", state_space, \" possible states\")\n\naction_space =\nprint(\"There are \", action_space, \" possible actions\")\n</pre> state_space = print(\"There are \", state_space, \" possible states\")  action_space = print(\"There are \", action_space, \" possible actions\") In\u00a0[\u00a0]: Copied! <pre># Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\ndef initialize_q_table(state_space, action_space):\n  Qtable =\n  return Qtable\n</pre> # Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b) def initialize_q_table(state_space, action_space):   Qtable =   return Qtable In\u00a0[\u00a0]: Copied! <pre>Qtable_frozenlake = initialize_q_table(state_space, action_space)\n</pre> Qtable_frozenlake = initialize_q_table(state_space, action_space) In\u00a0[\u00a0]: Copied! <pre>state_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible states\")\n\naction_space = env.action_space.n\nprint(\"There are \", action_space, \" possible actions\")\n</pre> state_space = env.observation_space.n print(\"There are \", state_space, \" possible states\")  action_space = env.action_space.n print(\"There are \", action_space, \" possible actions\") In\u00a0[\u00a0]: Copied! <pre># Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\ndef initialize_q_table(state_space, action_space):\n  Qtable = np.zeros((state_space, action_space))\n  return Qtable\n</pre> # Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros def initialize_q_table(state_space, action_space):   Qtable = np.zeros((state_space, action_space))   return Qtable In\u00a0[\u00a0]: Copied! <pre>Qtable_frozenlake = initialize_q_table(state_space, action_space)\n</pre> Qtable_frozenlake = initialize_q_table(state_space, action_space) In\u00a0[\u00a0]: Copied! <pre>def greedy_policy(Qtable, state):\n  # Exploitation: take the action with the highest state, action value\n  action =\n\n  return action\n</pre> def greedy_policy(Qtable, state):   # Exploitation: take the action with the highest state, action value   action =    return action In\u00a0[\u00a0]: Copied! <pre>def greedy_policy(Qtable, state):\n  # Exploitation: take the action with the highest state, action value\n  action = np.argmax(Qtable[state][:])\n\n  return action\n</pre> def greedy_policy(Qtable, state):   # Exploitation: take the action with the highest state, action value   action = np.argmax(Qtable[state][:])    return action In\u00a0[\u00a0]: Copied! <pre>def epsilon_greedy_policy(Qtable, state, epsilon):\n  # Randomly generate a number between 0 and 1\n  random_num =\n  # if random_num &gt; greater than epsilon --&gt; exploitation\n  if random_num &gt; epsilon:\n    # Take the action with the highest value given a state\n    # np.argmax can be useful here\n    action =\n  # else --&gt; exploration\n  else:\n    action = # Take a random action\n\n  return action\n</pre> def epsilon_greedy_policy(Qtable, state, epsilon):   # Randomly generate a number between 0 and 1   random_num =   # if random_num &gt; greater than epsilon --&gt; exploitation   if random_num &gt; epsilon:     # Take the action with the highest value given a state     # np.argmax can be useful here     action =   # else --&gt; exploration   else:     action = # Take a random action    return action In\u00a0[\u00a0]: Copied! <pre>def epsilon_greedy_policy(Qtable, state, epsilon):\n  # Randomly generate a number between 0 and 1\n  random_num = random.uniform(0,1)\n  # if random_num &gt; greater than epsilon --&gt; exploitation\n  if random_num &gt; epsilon:\n    # Take the action with the highest value given a state\n    # np.argmax can be useful here\n    action = greedy_policy(Qtable, state)\n  # else --&gt; exploration\n  else:\n    action = env.action_space.sample()\n\n  return action\n</pre> def epsilon_greedy_policy(Qtable, state, epsilon):   # Randomly generate a number between 0 and 1   random_num = random.uniform(0,1)   # if random_num &gt; greater than epsilon --&gt; exploitation   if random_num &gt; epsilon:     # Take the action with the highest value given a state     # np.argmax can be useful here     action = greedy_policy(Qtable, state)   # else --&gt; exploration   else:     action = env.action_space.sample()    return action In\u00a0[\u00a0]: Copied! <pre># Training parameters\nn_training_episodes = 10000  # Total training episodes\nlearning_rate = 0.7          # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# Environment parameters\nenv_id = \"FrozenLake-v1\"     # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate\neval_seed = []               # The evaluation seed of the environment\n\n# Exploration parameters\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05            # Minimum exploration probability\ndecay_rate = 0.0005            # Exponential decay rate for exploration prob\n</pre> # Training parameters n_training_episodes = 10000  # Total training episodes learning_rate = 0.7          # Learning rate  # Evaluation parameters n_eval_episodes = 100        # Total number of test episodes  # Environment parameters env_id = \"FrozenLake-v1\"     # Name of the environment max_steps = 99               # Max steps per episode gamma = 0.95                 # Discounting rate eval_seed = []               # The evaluation seed of the environment  # Exploration parameters max_epsilon = 1.0             # Exploration probability at start min_epsilon = 0.05            # Minimum exploration probability decay_rate = 0.0005            # Exponential decay rate for exploration prob In\u00a0[\u00a0]: Copied! <pre>def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n  for episode in tqdm(range(n_training_episodes)):\n    # Reduce epsilon (because we need less and less exploration)\n    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n    # Reset the environment\n    state, info = env.reset()\n    step = 0\n    terminated = False\n    truncated = False\n\n    # repeat\n    for step in range(max_steps):\n      # Choose the action At using epsilon greedy policy\n      action =\n\n      # Take action At and observe Rt+1 and St+1\n      # Take the action (a) and observe the outcome state(s') and reward (r)\n      new_state, reward, terminated, truncated, info =\n\n      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n      Qtable[state][action] =\n\n      # If terminated or truncated finish the episode\n      if terminated or truncated:\n        break\n\n      # Our next state is the new state\n      state = new_state\n  return Qtable\n</pre> def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):   for episode in tqdm(range(n_training_episodes)):     # Reduce epsilon (because we need less and less exploration)     epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)     # Reset the environment     state, info = env.reset()     step = 0     terminated = False     truncated = False      # repeat     for step in range(max_steps):       # Choose the action At using epsilon greedy policy       action =        # Take action At and observe Rt+1 and St+1       # Take the action (a) and observe the outcome state(s') and reward (r)       new_state, reward, terminated, truncated, info =        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]       Qtable[state][action] =        # If terminated or truncated finish the episode       if terminated or truncated:         break        # Our next state is the new state       state = new_state   return Qtable In\u00a0[\u00a0]: Copied! <pre>def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n  for episode in tqdm(range(n_training_episodes)):\n    # Reduce epsilon (because we need less and less exploration)\n    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n    # Reset the environment\n    state, info = env.reset()\n    step = 0\n    terminated = False\n    truncated = False\n\n    # repeat\n    for step in range(max_steps):\n      # Choose the action At using epsilon greedy policy\n      action = epsilon_greedy_policy(Qtable, state, epsilon)\n\n      # Take action At and observe Rt+1 and St+1\n      # Take the action (a) and observe the outcome state(s') and reward (r)\n      new_state, reward, terminated, truncated, info = env.step(action)\n\n      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n\n      # If terminated or truncated finish the episode\n      if terminated or truncated:\n        break\n\n      # Our next state is the new state\n      state = new_state\n  return Qtable\n</pre> def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):   for episode in tqdm(range(n_training_episodes)):     # Reduce epsilon (because we need less and less exploration)     epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)     # Reset the environment     state, info = env.reset()     step = 0     terminated = False     truncated = False      # repeat     for step in range(max_steps):       # Choose the action At using epsilon greedy policy       action = epsilon_greedy_policy(Qtable, state, epsilon)        # Take action At and observe Rt+1 and St+1       # Take the action (a) and observe the outcome state(s') and reward (r)       new_state, reward, terminated, truncated, info = env.step(action)        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]       Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])        # If terminated or truncated finish the episode       if terminated or truncated:         break        # Our next state is the new state       state = new_state   return Qtable In\u00a0[\u00a0]: Copied! <pre>Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)\n</pre> Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake) In\u00a0[\u00a0]: Copied! <pre>Qtable_frozenlake\n</pre> Qtable_frozenlake In\u00a0[\u00a0]: Copied! <pre>def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param max_steps: Maximum number of steps per episode\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param Q: The Q-table\n  :param seed: The evaluation seed array (for taxi-v3)\n  \"\"\"\n  episode_rewards = []\n  for episode in tqdm(range(n_eval_episodes)):\n    if seed:\n      state, info = env.reset(seed=seed[episode])\n    else:\n      state, info = env.reset()\n    step = 0\n    truncated = False\n    terminated = False\n    total_rewards_ep = 0\n\n    for step in range(max_steps):\n      # Take the action (index) that have the maximum expected future reward given that state\n      action = greedy_policy(Q, state)\n      new_state, reward, terminated, truncated, info = env.step(action)\n      total_rewards_ep += reward\n\n      if terminated or truncated:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward\n</pre> def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):   \"\"\"   Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.   :param env: The evaluation environment   :param max_steps: Maximum number of steps per episode   :param n_eval_episodes: Number of episode to evaluate the agent   :param Q: The Q-table   :param seed: The evaluation seed array (for taxi-v3)   \"\"\"   episode_rewards = []   for episode in tqdm(range(n_eval_episodes)):     if seed:       state, info = env.reset(seed=seed[episode])     else:       state, info = env.reset()     step = 0     truncated = False     terminated = False     total_rewards_ep = 0      for step in range(max_steps):       # Take the action (index) that have the maximum expected future reward given that state       action = greedy_policy(Q, state)       new_state, reward, terminated, truncated, info = env.step(action)       total_rewards_ep += reward        if terminated or truncated:         break       state = new_state     episode_rewards.append(total_rewards_ep)   mean_reward = np.mean(episode_rewards)   std_reward = np.std(episode_rewards)    return mean_reward, std_reward In\u00a0[\u00a0]: Copied! <pre># Evaluate our Agent\nmean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\nprint(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n</pre> # Evaluate our Agent mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed) print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\") In\u00a0[\u00a0]: Copied! <pre>env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n</pre> env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\") <p>There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.</p> In\u00a0[\u00a0]: Copied! <pre>state_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible states\")\n</pre> state_space = env.observation_space.n print(\"There are \", state_space, \" possible states\") In\u00a0[\u00a0]: Copied! <pre>action_space = env.action_space.n\nprint(\"There are \", action_space, \" possible actions\")\n</pre> action_space = env.action_space.n print(\"There are \", action_space, \" possible actions\") <p>The action space (the set of possible actions the agent can take) is discrete with 6 actions available \ud83c\udfae:</p> <ul> <li>0: move south</li> <li>1: move north</li> <li>2: move east</li> <li>3: move west</li> <li>4: pickup passenger</li> <li>5: drop off passenger</li> </ul> <p>Reward function \ud83d\udcb0:</p> <ul> <li>-1 per step unless other reward is triggered.</li> <li>+20 delivering passenger.</li> <li>-10 executing \u201cpickup\u201d and \u201cdrop-off\u201d actions illegally.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Create our Q table with state_size rows and action_size columns (500x6)\nQtable_taxi = initialize_q_table(state_space, action_space)\nprint(Qtable_taxi)\nprint(\"Q-table shape: \", Qtable_taxi .shape)\n</pre> # Create our Q table with state_size rows and action_size columns (500x6) Qtable_taxi = initialize_q_table(state_space, action_space) print(Qtable_taxi) print(\"Q-table shape: \", Qtable_taxi .shape) In\u00a0[\u00a0]: Copied! <pre># Training parameters\nn_training_episodes = 25000   # Total training episodes\nlearning_rate = 0.7           # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100        # Total number of test episodes\n\n# DO NOT MODIFY EVAL_SEED\neval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n                                                          # Each seed has a specific starting state\n\n# Environment parameters\nenv_id = \"Taxi-v3\"           # Name of the environment\nmax_steps = 99               # Max steps per episode\ngamma = 0.95                 # Discounting rate\n\n# Exploration parameters\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.05           # Minimum exploration probability\ndecay_rate = 0.005            # Exponential decay rate for exploration prob\n</pre> # Training parameters n_training_episodes = 25000   # Total training episodes learning_rate = 0.7           # Learning rate  # Evaluation parameters n_eval_episodes = 100        # Total number of test episodes  # DO NOT MODIFY EVAL_SEED eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,  161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,  112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position                                                           # Each seed has a specific starting state  # Environment parameters env_id = \"Taxi-v3\"           # Name of the environment max_steps = 99               # Max steps per episode gamma = 0.95                 # Discounting rate  # Exploration parameters max_epsilon = 1.0             # Exploration probability at start min_epsilon = 0.05           # Minimum exploration probability decay_rate = 0.005            # Exponential decay rate for exploration prob  In\u00a0[\u00a0]: Copied! <pre>Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\nQtable_taxi\n</pre> Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi) Qtable_taxi"},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#unit-2-q-learning-with-frozenlake-v1-and-taxi-v3","title":"Unit 2: Q-Learning with FrozenLake-v1 and Taxi-v3\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#install-dependencies-and-create-a-virtual-display","title":"Install dependencies and create a virtual display\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#import-the-packages","title":"Import the packages\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#part-1-frozen-lake-non-slippery-version","title":"Part 1: Frozen Lake (non slippery version)\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#lets-see-what-the-environment-looks-like","title":"Let's see what the Environment looks like:\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#create-and-initialize-the-q-table","title":"Create and Initialize the Q-table\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#define-the-greedy-policy","title":"Define the greedy policy\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#define-the-epsilon-greedy-policy","title":"Define the epsilon-greedy policy\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#define-the-hyperparameters","title":"Define the hyperparameters\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#create-the-training-loop-method","title":"Create the training loop method\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#train-the-q-learning-agent","title":"Train the Q-Learning agent\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#lets-see-what-our-q-learning-table-looks-like-now","title":"Let's see what our Q-Learning table looks like now\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#evaluation-function","title":"Evaluation function\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#part-2-taxi-v3","title":"Part 2: Taxi-v3\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#define-the-hyperparameters","title":"Define the hyperparameters\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit2_q_learning/#train-our-q-learning-agent","title":"Train our Q-Learning agent\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/","title":"Unit3 policy gradient","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip install pyvirtualdisplay\n!pip install pyglet==1.5.1\n</pre> %%capture !apt install python-opengl !apt install ffmpeg !apt install xvfb !pip install pyvirtualdisplay !pip install pyglet==1.5.1 In\u00a0[\u00a0]: Copied! <pre># Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n</pre> # Virtual display from pyvirtualdisplay import Display  virtual_display = Display(visible=0, size=(1400, 900)) virtual_display.start() In\u00a0[\u00a0]: Copied! <pre>!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n!pip install git+https://github.com/simoninithomas/gym-games\n!pip install imageio-ffmpeg\n!pip install pyyaml==6.0\n</pre> !pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git !pip install git+https://github.com/simoninithomas/gym-games !pip install imageio-ffmpeg !pip install pyyaml==6.0 In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Gym\nimport gym\nimport gym_pygame\n\n# Hugging Face Hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nimport imageio\n</pre> import numpy as np  from collections import deque  import matplotlib.pyplot as plt %matplotlib inline  # PyTorch import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.distributions import Categorical  # Gym import gym import gym_pygame  # Hugging Face Hub from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub. import imageio In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n</pre> device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") In\u00a0[\u00a0]: Copied! <pre>print(device)\n</pre> print(device) In\u00a0[\u00a0]: Copied! <pre>env_id = \"CartPole-v1\"\n# Create the env\nenv = gym.make(env_id)\n\n# Create the evaluation env\neval_env = gym.make(env_id)\n\n# Get the state space and action space\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n\n</pre> env_id = \"CartPole-v1\" # Create the env env = gym.make(env_id)  # Create the evaluation env eval_env = gym.make(env_id)  # Get the state space and action space s_size = env.observation_space.shape[0] a_size = env.action_space.n In\u00a0[\u00a0]: Copied! <pre>print(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n</pre> print(\"_____OBSERVATION SPACE_____ \\n\") print(\"The State Space is: \", s_size) print(\"Sample observation\", env.observation_space.sample()) # Get a random observation In\u00a0[\u00a0]: Copied! <pre>print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n</pre> print(\"\\n _____ACTION SPACE_____ \\n\") print(\"The Action Space is: \", a_size) print(\"Action Space Sample\", env.action_space.sample()) # Take a random action <p>So we want:</p> <ul> <li>Two fully connected layers (fc1 and fc2).</li> <li>Using ReLU as activation function of fc1</li> <li>Using Softmax to output a probability distribution over actions</li> </ul> In\u00a0[\u00a0]: Copied! <pre>class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        # Create two fully connected layers\n\n\n\n    def forward(self, x):\n        # Define the forward pass\n        # state goes to fc1 then we apply ReLU activation function\n\n        # fc1 outputs goes to fc2\n\n        # We output the softmax\n\n    def act(self, state):\n        \"\"\"\n        Given a state, take action\n        \"\"\"\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = np.argmax(m)\n        return action.item(), m.log_prob(action)\n</pre> class Policy(nn.Module):     def __init__(self, s_size, a_size, h_size):         super(Policy, self).__init__()         # Create two fully connected layers        def forward(self, x):         # Define the forward pass         # state goes to fc1 then we apply ReLU activation function          # fc1 outputs goes to fc2          # We output the softmax      def act(self, state):         \"\"\"         Given a state, take action         \"\"\"         state = torch.from_numpy(state).float().unsqueeze(0).to(device)         probs = self.forward(state).cpu()         m = Categorical(probs)         action = np.argmax(m)         return action.item(), m.log_prob(action) In\u00a0[\u00a0]: Copied! <pre>class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = np.argmax(m)\n        return action.item(), m.log_prob(action)\n</pre> class Policy(nn.Module):     def __init__(self, s_size, a_size, h_size):         super(Policy, self).__init__()         self.fc1 = nn.Linear(s_size, h_size)         self.fc2 = nn.Linear(h_size, a_size)      def forward(self, x):         x = F.relu(self.fc1(x))         x = self.fc2(x)         return F.softmax(x, dim=1)      def act(self, state):         state = torch.from_numpy(state).float().unsqueeze(0).to(device)         probs = self.forward(state).cpu()         m = Categorical(probs)         action = np.argmax(m)         return action.item(), m.log_prob(action) <p>I make a mistake, can you guess where?</p> <ul> <li>To find out let's make a forward pass:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>debug_policy = Policy(s_size, a_size, 64).to(device)\ndebug_policy.act(env.reset())\n</pre> debug_policy = Policy(s_size, a_size, 64).to(device) debug_policy.act(env.reset()) <ul> <li><p>Here we see that the error says <code>ValueError: The value argument to log_prob must be a Tensor</code></p> </li> <li><p>It means that <code>action</code> in <code>m.log_prob(action)</code> must be a Tensor but it's not.</p> </li> <li><p>Do you know why? Check the act function and try to see why it does not work.</p> </li> </ul> <p>Advice \ud83d\udca1: Something is wrong in this implementation. Remember that we act function we want to sample an action from the probability distribution over actions.</p> In\u00a0[\u00a0]: Copied! <pre>class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n</pre> class Policy(nn.Module):     def __init__(self, s_size, a_size, h_size):         super(Policy, self).__init__()         self.fc1 = nn.Linear(s_size, h_size)         self.fc2 = nn.Linear(h_size, a_size)      def forward(self, x):         x = F.relu(self.fc1(x))         x = self.fc2(x)         return F.softmax(x, dim=1)      def act(self, state):         state = torch.from_numpy(state).float().unsqueeze(0).to(device)         probs = self.forward(state).cpu()         m = Categorical(probs)         action = m.sample()         return action.item(), m.log_prob(action) <p>By using CartPole, it was easier to debug since we know that the bug comes from our integration and not from our simple environment.</p> <ul> <li><p>Since we want to sample an action from the probability distribution over actions, we can't use <code>action = np.argmax(m)</code> since it will always output the action that have the highest probability.</p> </li> <li><p>We need to replace with <code>action = m.sample()</code> that will sample an action from the probability distribution P(.|s)</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # Help us to calculate the score during the training\n    scores_deque = deque(maxlen=100)\n    scores = []\n    # Line 3 of pseudocode\n    for i_episode in range(1, n_training_episodes+1):\n        saved_log_probs = []\n        rewards = []\n        state = # TODO: reset the environment\n        # Line 4 of pseudocode\n        for t in range(max_t):\n            action, log_prob = # TODO get the action\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = # TODO: take an env step\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n\n        # Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        # Compute the discounted returns at each timestep,\n        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n\n        # In O(N) time, where N is the number of time steps\n        # (this definition of the discounted return G_t follows the definition of this quantity\n        # shown at page 44 of Sutton&amp;Barto 2017 2nd draft)\n        # G_t = r_(t+1) + r_(t+2) + ...\n\n        # Given this formulation, the returns at each timestep t can be computed\n        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n        # G_t = r_(t+1) + gamma*G_(t+1)\n        # G_(t-1) = r_t + gamma* G_t\n        # (this follows a dynamic programming approach, with which we memorize solutions in order\n        # to avoid computing them multiple times)\n\n        # This is correct since the above is equivalent to (see also page 46 of Sutton&amp;Barto 2017 2nd draft)\n        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n\n\n        ## Given the above, we calculate the returns at timestep t as:\n        #               gamma[t] * return[t] + reward[t]\n        #\n        ## We compute this starting from the last timestep to the first, in order\n        ## to employ the formula presented above and avoid redundant computations that would be needed\n        ## if we were to do it from first to last.\n\n        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n        ## a normal python list would instead require O(N) to do this.\n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns)&gt;0 else 0)\n            returns.appendleft(    ) # TODO: complete here\n\n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item()\n\n        ## eps is the smallest representable float, which is\n        # added to the standard deviation of the returns to avoid numerical instabilities\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n\n        # Line 7:\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n\n        # Line 8: PyTorch prefers gradient descent\n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n\n        if i_episode % print_every == 0:\n            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n\n    return scores\n</pre> def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):     # Help us to calculate the score during the training     scores_deque = deque(maxlen=100)     scores = []     # Line 3 of pseudocode     for i_episode in range(1, n_training_episodes+1):         saved_log_probs = []         rewards = []         state = # TODO: reset the environment         # Line 4 of pseudocode         for t in range(max_t):             action, log_prob = # TODO get the action             saved_log_probs.append(log_prob)             state, reward, done, _ = # TODO: take an env step             rewards.append(reward)             if done:                 break         scores_deque.append(sum(rewards))         scores.append(sum(rewards))          # Line 6 of pseudocode: calculate the return         returns = deque(maxlen=max_t)         n_steps = len(rewards)         # Compute the discounted returns at each timestep,         # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t          # In O(N) time, where N is the number of time steps         # (this definition of the discounted return G_t follows the definition of this quantity         # shown at page 44 of Sutton&amp;Barto 2017 2nd draft)         # G_t = r_(t+1) + r_(t+2) + ...          # Given this formulation, the returns at each timestep t can be computed         # by re-using the computed future returns G_(t+1) to compute the current return G_t         # G_t = r_(t+1) + gamma*G_(t+1)         # G_(t-1) = r_t + gamma* G_t         # (this follows a dynamic programming approach, with which we memorize solutions in order         # to avoid computing them multiple times)          # This is correct since the above is equivalent to (see also page 46 of Sutton&amp;Barto 2017 2nd draft)         # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...           ## Given the above, we calculate the returns at timestep t as:         #               gamma[t] * return[t] + reward[t]         #         ## We compute this starting from the last timestep to the first, in order         ## to employ the formula presented above and avoid redundant computations that would be needed         ## if we were to do it from first to last.          ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps         ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)         ## a normal python list would instead require O(N) to do this.         for t in range(n_steps)[::-1]:             disc_return_t = (returns[0] if len(returns)&gt;0 else 0)             returns.appendleft(    ) # TODO: complete here          ## standardization of the returns is employed to make training more stable         eps = np.finfo(np.float32).eps.item()          ## eps is the smallest representable float, which is         # added to the standard deviation of the returns to avoid numerical instabilities         returns = torch.tensor(returns)         returns = (returns - returns.mean()) / (returns.std() + eps)          # Line 7:         policy_loss = []         for log_prob, disc_return in zip(saved_log_probs, returns):             policy_loss.append(-log_prob * disc_return)         policy_loss = torch.cat(policy_loss).sum()          # Line 8: PyTorch prefers gradient descent         optimizer.zero_grad()         policy_loss.backward()         optimizer.step()          if i_episode % print_every == 0:             print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))      return scores In\u00a0[\u00a0]: Copied! <pre>def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # Help us to calculate the score during the training\n    scores_deque = deque(maxlen=100)\n    scores = []\n    # Line 3 of pseudocode\n    for i_episode in range(1, n_training_episodes+1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()\n        # Line 4 of pseudocode\n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = env.step(action)\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n\n        # Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        # Compute the discounted returns at each timestep,\n        # as\n        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n        #\n        # In O(N) time, where N is the number of time steps\n        # (this definition of the discounted return G_t follows the definition of this quantity\n        # shown at page 44 of Sutton&amp;Barto 2017 2nd draft)\n        # G_t = r_(t+1) + r_(t+2) + ...\n\n        # Given this formulation, the returns at each timestep t can be computed\n        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n        # G_t = r_(t+1) + gamma*G_(t+1)\n        # G_(t-1) = r_t + gamma* G_t\n        # (this follows a dynamic programming approach, with which we memorize solutions in order\n        # to avoid computing them multiple times)\n\n        # This is correct since the above is equivalent to (see also page 46 of Sutton&amp;Barto 2017 2nd draft)\n        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n\n\n        ## Given the above, we calculate the returns at timestep t as:\n        #               gamma[t] * return[t] + reward[t]\n        #\n        ## We compute this starting from the last timestep to the first, in order\n        ## to employ the formula presented above and avoid redundant computations that would be needed\n        ## if we were to do it from first to last.\n\n        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n        ## a normal python list would instead require O(N) to do this.\n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns)&gt;0 else 0)\n            returns.appendleft( gamma*disc_return_t + rewards[t]   )\n\n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item()\n        ## eps is the smallest representable float, which is\n        # added to the standard deviation of the returns to avoid numerical instabilities\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n\n        # Line 7:\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n\n        # Line 8: PyTorch prefers gradient descent\n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n\n        if i_episode % print_every == 0:\n            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n\n    return scores\n</pre> def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):     # Help us to calculate the score during the training     scores_deque = deque(maxlen=100)     scores = []     # Line 3 of pseudocode     for i_episode in range(1, n_training_episodes+1):         saved_log_probs = []         rewards = []         state = env.reset()         # Line 4 of pseudocode         for t in range(max_t):             action, log_prob = policy.act(state)             saved_log_probs.append(log_prob)             state, reward, done, _ = env.step(action)             rewards.append(reward)             if done:                 break         scores_deque.append(sum(rewards))         scores.append(sum(rewards))          # Line 6 of pseudocode: calculate the return         returns = deque(maxlen=max_t)         n_steps = len(rewards)         # Compute the discounted returns at each timestep,         # as         #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t         #         # In O(N) time, where N is the number of time steps         # (this definition of the discounted return G_t follows the definition of this quantity         # shown at page 44 of Sutton&amp;Barto 2017 2nd draft)         # G_t = r_(t+1) + r_(t+2) + ...          # Given this formulation, the returns at each timestep t can be computed         # by re-using the computed future returns G_(t+1) to compute the current return G_t         # G_t = r_(t+1) + gamma*G_(t+1)         # G_(t-1) = r_t + gamma* G_t         # (this follows a dynamic programming approach, with which we memorize solutions in order         # to avoid computing them multiple times)          # This is correct since the above is equivalent to (see also page 46 of Sutton&amp;Barto 2017 2nd draft)         # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...           ## Given the above, we calculate the returns at timestep t as:         #               gamma[t] * return[t] + reward[t]         #         ## We compute this starting from the last timestep to the first, in order         ## to employ the formula presented above and avoid redundant computations that would be needed         ## if we were to do it from first to last.          ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps         ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)         ## a normal python list would instead require O(N) to do this.         for t in range(n_steps)[::-1]:             disc_return_t = (returns[0] if len(returns)&gt;0 else 0)             returns.appendleft( gamma*disc_return_t + rewards[t]   )          ## standardization of the returns is employed to make training more stable         eps = np.finfo(np.float32).eps.item()         ## eps is the smallest representable float, which is         # added to the standard deviation of the returns to avoid numerical instabilities         returns = torch.tensor(returns)         returns = (returns - returns.mean()) / (returns.std() + eps)          # Line 7:         policy_loss = []         for log_prob, disc_return in zip(saved_log_probs, returns):             policy_loss.append(-log_prob * disc_return)         policy_loss = torch.cat(policy_loss).sum()          # Line 8: PyTorch prefers gradient descent         optimizer.zero_grad()         policy_loss.backward()         optimizer.step()          if i_episode % print_every == 0:             print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))      return scores In\u00a0[\u00a0]: Copied! <pre>cartpole_hyperparameters = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}\n</pre> cartpole_hyperparameters = {     \"h_size\": 16,     \"n_training_episodes\": 1000,     \"n_evaluation_episodes\": 10,     \"max_t\": 1000,     \"gamma\": 1.0,     \"lr\": 1e-2,     \"env_id\": env_id,     \"state_space\": s_size,     \"action_space\": a_size, } In\u00a0[\u00a0]: Copied! <pre># Create policy and place it to the device\ncartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\ncartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])\n</pre> # Create policy and place it to the device cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device) cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"]) In\u00a0[\u00a0]: Copied! <pre>scores = reinforce(cartpole_policy,\n                   cartpole_optimizer,\n                   cartpole_hyperparameters[\"n_training_episodes\"],\n                   cartpole_hyperparameters[\"max_t\"],\n                   cartpole_hyperparameters[\"gamma\"],\n                   100)\n</pre> scores = reinforce(cartpole_policy,                    cartpole_optimizer,                    cartpole_hyperparameters[\"n_training_episodes\"],                    cartpole_hyperparameters[\"max_t\"],                    cartpole_hyperparameters[\"gamma\"],                    100) In\u00a0[\u00a0]: Copied! <pre>def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The Reinforce agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()\n    step = 0\n    done = False\n    total_rewards_ep = 0\n\n    for step in range(max_steps):\n      action, _ = policy.act(state)\n      new_state, reward, done, info = env.step(action)\n      total_rewards_ep += reward\n\n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward\n</pre> def evaluate_agent(env, max_steps, n_eval_episodes, policy):   \"\"\"   Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.   :param env: The evaluation environment   :param n_eval_episodes: Number of episode to evaluate the agent   :param policy: The Reinforce agent   \"\"\"   episode_rewards = []   for episode in range(n_eval_episodes):     state = env.reset()     step = 0     done = False     total_rewards_ep = 0      for step in range(max_steps):       action, _ = policy.act(state)       new_state, reward, done, info = env.step(action)       total_rewards_ep += reward        if done:         break       state = new_state     episode_rewards.append(total_rewards_ep)   mean_reward = np.mean(episode_rewards)   std_reward = np.std(episode_rewards)    return mean_reward, std_reward In\u00a0[\u00a0]: Copied! <pre>evaluate_agent(eval_env,\n               cartpole_hyperparameters[\"max_t\"],\n               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n               cartpole_policy)\n</pre> evaluate_agent(eval_env,                cartpole_hyperparameters[\"max_t\"],                cartpole_hyperparameters[\"n_evaluation_episodes\"],                cartpole_policy) In\u00a0[\u00a0]: Copied! <pre>env_id = \"Pixelcopter-PLE-v0\"\nenv = gym.make(env_id)\neval_env = gym.make(env_id)\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n\n</pre> env_id = \"Pixelcopter-PLE-v0\" env = gym.make(env_id) eval_env = gym.make(env_id) s_size = env.observation_space.shape[0] a_size = env.action_space.n In\u00a0[\u00a0]: Copied! <pre>print(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n</pre> print(\"_____OBSERVATION SPACE_____ \\n\") print(\"The State Space is: \", s_size) print(\"Sample observation\", env.observation_space.sample()) # Get a random observation In\u00a0[\u00a0]: Copied! <pre>print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n</pre> print(\"\\n _____ACTION SPACE_____ \\n\") print(\"The Action Space is: \", a_size) print(\"Action Space Sample\", env.action_space.sample()) # Take a random action <p>The observation space (7) \ud83d\udc40:</p> <ul> <li>player y position</li> <li>player velocity</li> <li>player distance to floor</li> <li>player distance to ceiling</li> <li>next block x distance to player</li> <li>next blocks top y location</li> <li>next blocks bottom y location</li> </ul> <p>The action space(2) \ud83c\udfae:</p> <ul> <li>Up (press accelerator)</li> <li>Do nothing (don't press accelerator)</li> </ul> <p>The reward function \ud83d\udcb0:</p> <ul> <li>For each vertical block it passes through it gains a positive reward of +1. Each time a terminal state reached it receives a negative reward of -1.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        # Define the three layers here\n\n    def forward(self, x):\n        # Define the forward process here\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n</pre> class Policy(nn.Module):     def __init__(self, s_size, a_size, h_size):         super(Policy, self).__init__()         # Define the three layers here      def forward(self, x):         # Define the forward process here         return F.softmax(x, dim=1)      def act(self, state):         state = torch.from_numpy(state).float().unsqueeze(0).to(device)         probs = self.forward(state).cpu()         m = Categorical(probs)         action = m.sample()         return action.item(), m.log_prob(action) In\u00a0[\u00a0]: Copied! <pre>class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, h_size*2)\n        self.fc3 = nn.Linear(h_size*2, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n</pre> class Policy(nn.Module):     def __init__(self, s_size, a_size, h_size):         super(Policy, self).__init__()         self.fc1 = nn.Linear(s_size, h_size)         self.fc2 = nn.Linear(h_size, h_size*2)         self.fc3 = nn.Linear(h_size*2, a_size)      def forward(self, x):         x = F.relu(self.fc1(x))         x = F.relu(self.fc2(x))         x = self.fc3(x)         return F.softmax(x, dim=1)      def act(self, state):         state = torch.from_numpy(state).float().unsqueeze(0).to(device)         probs = self.forward(state).cpu()         m = Categorical(probs)         action = m.sample()         return action.item(), m.log_prob(action) In\u00a0[\u00a0]: Copied! <pre>pixelcopter_hyperparameters = {\n    \"h_size\": 64,\n    \"n_training_episodes\": 50000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 10000,\n    \"gamma\": 0.99,\n    \"lr\": 1e-4,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}\n</pre> pixelcopter_hyperparameters = {     \"h_size\": 64,     \"n_training_episodes\": 50000,     \"n_evaluation_episodes\": 10,     \"max_t\": 10000,     \"gamma\": 0.99,     \"lr\": 1e-4,     \"env_id\": env_id,     \"state_space\": s_size,     \"action_space\": a_size, } In\u00a0[\u00a0]: Copied! <pre># Create policy and place it to the device\n# torch.manual_seed(50)\npixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\npixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])\n</pre> # Create policy and place it to the device # torch.manual_seed(50) pixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device) pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"]) In\u00a0[\u00a0]: Copied! <pre>scores = reinforce(pixelcopter_policy,\n                   pixelcopter_optimizer,\n                   pixelcopter_hyperparameters[\"n_training_episodes\"],\n                   pixelcopter_hyperparameters[\"max_t\"],\n                   pixelcopter_hyperparameters[\"gamma\"],\n                   1000)\n</pre> scores = reinforce(pixelcopter_policy,                    pixelcopter_optimizer,                    pixelcopter_hyperparameters[\"n_training_episodes\"],                    pixelcopter_hyperparameters[\"max_t\"],                    pixelcopter_hyperparameters[\"gamma\"],                    1000)"},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#unit-3-code-your-first-deep-reinforcement-learning-algorithm-with-pytorch-reinforce","title":"Unit 3: Code your first Deep Reinforcement Learning Algorithm with PyTorch: Reinforce.\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#create-a-virtual-display","title":"Create a virtual display\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#install-the-dependencies","title":"Install the dependencies\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#import-the-packages","title":"Import the packages\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#check-if-we-have-a-gpu","title":"Check if we have a GPU\u00b6","text":"<ul> <li>Let's check if we have a GPU</li> <li>If it's the case you should see <code>device:cuda0</code></li> </ul>"},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#first-agent-playing-cartpole-v1","title":"First agent: Playing CartPole-v1 \ud83e\udd16\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#real-solution","title":"(Real) Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#lets-build-the-reinforce-training-algorithm","title":"Let's build the Reinforce Training Algorithm\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#train-it","title":"Train it\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#define-evaluation-method","title":"Define evaluation method\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#evaluate-our-agent","title":"Evaluate our agent\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#second-agent-pixelcopter","title":"Second agent: PixelCopter\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#define-the-new-policy","title":"Define the new Policy\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#solution","title":"Solution\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#define-the-hyperparameters","title":"Define the hyperparameters\u00b6","text":""},{"location":"notebooks/reinforcement_learning/unit3_policy_gradient/#train-it","title":"Train it\u00b6","text":""},{"location":"reinforcement_learning/","title":"Reinforcement Learning Module","text":""},{"location":"reinforcement_learning/#overview","title":"Overview","text":"<p>This module focuses on reinforcement learning techniques for robotic control, decision-making, and autonomous behavior. Learn how robots can learn from interaction with their environment.</p>"},{"location":"reinforcement_learning/#topics-covered","title":"Topics Covered","text":"<ul> <li> <p>Introduction to RL: Intuitive explanation of RL concepts with real-world examples from Boston Dynamics, drone racing, and more.</p> </li> <li> <p>Markov Decision Processes (MDPs): Mathematical framework: states, actions, rewards, transitions, and the Bellman equations.</p> </li> <li> <p>Policy vs Value-Based Methods: Understanding the fundamental difference between learning values vs learning policies directly.</p> </li> <li> <p>Q-Learning: The classic value-based algorithm with detailed implementation and convergence properties.</p> </li> <li> <p>Policy Gradients: Direct policy optimization using gradient ascent on expected returns.</p> </li> <li> <p>Practical Implementation: Build REINFORCE from scratch and compare with PPO from Stable-Baselines3.</p> </li> </ul>"},{"location":"reinforcement_learning/#learning-path","title":"Learning Path","text":"<pre><code>graph LR\n    A[\"Introduction\"] --&gt; B[\"MDP\"]\n    B --&gt; C[\"Policy vs Value\"]\n    C --&gt; D[\"Q-Learning\"]\n    C --&gt; E[\"REINFORCE\"]\n    D --&gt; F[\"Practical Tutorial\"]\n    E --&gt; F</code></pre>"},{"location":"reinforcement_learning/#module-contents","title":"Module Contents","text":"<ul> <li>Introduction to RL - Intuitive introduction with real-world examples</li> <li>Markov Decision Processes - Mathematical framework for RL</li> <li>Policy vs Value-Based Methods - Understanding different approaches</li> <li>Q-Learning - Classic value-based algorithm</li> <li>Policy Gradients (REINFORCE) - Direct policy optimization</li> <li>Practical Tutorial - Hands-on implementation from scratch</li> </ul>"},{"location":"reinforcement_learning/#prerequisites","title":"Prerequisites","text":"<ul> <li>Foundations module completed</li> <li>Deep Learning basics</li> <li>Probability theory</li> </ul> <p>Start with Introduction to RL \u2192</p>"},{"location":"reinforcement_learning/1_introduction/","title":"Introduction to Reinforcement Learning","text":""},{"location":"reinforcement_learning/1_introduction/#what-is-reinforcement-learning","title":"What is Reinforcement Learning?","text":"<p>Imagine teaching a dog a new trick. You don't explain the physics of jumping or write out step-by-step instructions. Instead, you reward the dog when it does something right and provide no reward (or a gentle correction) when it doesn't. Over time, through trial and error, the dog learns which actions lead to treats and praise.</p> <p>This is exactly how Reinforcement Learning (RL) works for robots and AI systems!</p>"},{"location":"reinforcement_learning/1_introduction/#the-core-idea","title":"The Core Idea","text":"<p>Reinforcement Learning is about learning through interaction and feedback. Unlike supervised learning where we tell the system exactly what to do (like showing labeled examples), in RL:</p> <ul> <li>An agent (robot, drone, game AI) takes actions in an environment (physical world, simulation)</li> <li>The environment gives feedback in the form of rewards or penalties</li> <li>The agent learns from experience which actions lead to better outcomes</li> <li>Over time, the agent discovers strategies (called policies) that maximize its total reward</li> </ul> <p>Think of it like learning to ride a bike: - You try different balancing strategies (actions) - Staying upright feels good, falling hurts (rewards/penalties) - You don't need someone to tell you the exact muscle movements - Through practice, you develop an intuition for how to balance</p>"},{"location":"reinforcement_learning/1_introduction/#why-is-rl-important-for-robotics","title":"Why is RL Important for Robotics?","text":"<p>Traditional robot control often requires: - Expert knowledge to program every behavior - Extensive modeling of the environment physics - Manual tuning of parameters for different scenarios - Difficulty adapting to new or unexpected situations</p> <p>RL enables robots to: - Learn from experience rather than explicit programming - Discover novel solutions that humans might not think of - Adapt to changing environments and unexpected situations - Optimize complex behaviors that are hard to engineer manually</p>"},{"location":"reinforcement_learning/1_introduction/#real-world-examples-rl-in-action","title":"Real-World Examples: RL in Action","text":"<p>Let's look at some impressive examples where RL has been preferred over traditional control methods:</p>"},{"location":"reinforcement_learning/1_introduction/#boston-dynamics-spot-adaptive-locomotion","title":"Boston Dynamics Spot: Adaptive Locomotion","text":"<p>Boston Dynamics uses RL to teach Spot, their quadruped robot, to walk on challenging terrain. This application demonstrates RL's ability to handle environmental diversity that would overwhelm traditional control systems.</p> <p>Traditional approach limitations: - Requires detailed terrain mapping and classification for every surface type - Needs pre-programmed gaits for each terrain variation (grass, gravel, ice, stairs, etc.) - Complex state machines to handle transitions between different walking modes - Extensive manual tuning and calibration for each new environment - Brittle to unexpected terrain features not accounted for in the design</p> <p>RL approach advantages: - Learns to adapt its gait through trial and error in simulation - Discovers robust walking strategies that generalize to unseen terrains - Automatically learns recovery behaviors when it slips, trips, or encounters obstacles - Transfers learned skills from simulation to the real robot with minimal fine-tuning - Single policy handles diverse terrain without explicit terrain classification</p> <p>Why RL was chosen: The sheer variety of terrain variations makes manual programming infeasible. RL discovers novel recovery strategies that engineers didn't anticipate, and the system can adapt to completely new environments without reprogramming.</p>"},{"location":"reinforcement_learning/1_introduction/#autonomous-drone-racing-high-speed-navigation","title":"Autonomous Drone Racing: High-Speed Navigation","text":"<p>Researchers have trained drones to race through complex obstacle courses at speeds exceeding 40 mph, achieving performance that beats human pilots. This showcases RL's ability to learn reactive, high-speed control policies.</p> <p>Traditional approach challenges: - Requires perfect sensing and precise trajectory planning - Computational delays in processing sensor data and replanning trajectories - Difficult to handle aggressive maneuvers and complex aerodynamic effects - Brittle to sensor noise and unexpected obstacles - Conservative behaviors limit maximum achievable speed</p> <p>RL approach advantages: - Learns end-to-end control directly from camera images to motor commands - Develops reactive behaviors that don't require explicit planning or trajectory optimization - Discovers aggressive maneuvers through exploration that push physical limits - Naturally handles partial observability and sensor uncertainty - Achieves super-human performance through extensive simulated practice</p> <p>Key insight: The RL-trained policy makes decisions in milliseconds based on pattern recognition rather than slow deliberative planning, similar to how expert human pilots develop intuitive control. This reactive approach is essential for high-speed navigation where planning delays would be catastrophic.</p>"},{"location":"reinforcement_learning/1_introduction/#anymal-quadruped-robot-traversing-rough-terrain","title":"ANYmal: Quadruped Robot Traversing Rough Terrain","text":"<p>ETH Zurich's ANYmal is a four-legged robot that can climb stairs, traverse rubble, and recover from slips using RL. This example highlights RL's ability to work with minimal sensing and handle model uncertainty.</p> <p>Traditional model-based control limitations: - Requires accurate terrain geometry (often unavailable in real-world scenarios) - Needs perfect knowledge of robot dynamics and contact models - Conservative behaviors to ensure stability, limiting agility - Separate controllers needed for different scenarios (walking, trotting, recovery) - Fails when assumptions about terrain or dynamics are violated</p> <p>RL approach benefits: - Learns directly from proprioceptive sensors (joint positions, torques, IMU) without vision - Doesn't need explicit terrain geometry or mapping - Discovers dynamic gaits that traditional methods might consider \"unsafe\" but are actually more robust - Single learned policy handles walking, trotting, and recovery behaviors seamlessly - Robust to model uncertainty and external disturbances</p> <p>Remarkable result: ANYmal can walk blindly (without vision) on very rough terrain by learning to predict terrain properties from how its legs interact with the ground. This proprioceptive sensing approach is more robust than vision-based methods in challenging lighting or visual conditions.</p>"},{"location":"reinforcement_learning/1_introduction/#robotic-manipulation-dexterous-in-hand-manipulation","title":"Robotic Manipulation: Dexterous In-Hand Manipulation","text":"<p>OpenAI trained a robotic hand to solve a Rubik's cube using RL, demonstrating human-level dexterity in complex manipulation tasks. This showcases RL's ability to handle high-dimensional control and contact dynamics.</p> <p>Traditional approach limitations: - Extremely difficult to model contact dynamics accurately (friction, slip, deformation) - Hand-crafted control policies are brittle and task-specific - Requires precise sensing and perfect calibration - Fails when objects slip or unexpected perturbations occur - Cannot handle the complexity of multi-finger coordination</p> <p>RL approach advantages: - Learns robust manipulation through millions of simulated attempts - Develops recovery strategies for when objects slip or move unexpectedly - Discovers creative fingering strategies that humans might not consider - Handles visual ambiguity and partial observability naturally - Shows emergent behaviors like flipping the cube in creative ways</p> <p>Critical success factor: Domain randomization in simulation (varying physics parameters, visual appearance, friction coefficients, etc.) allows the learned policy to be robust enough to transfer to the real world despite the sim-to-real gap. This technique is essential for successful RL deployment in robotics.</p>"},{"location":"reinforcement_learning/1_introduction/#game-ai-alphago-and-strategic-decision-making","title":"Game AI: AlphaGo and Strategic Decision-Making","text":"<p>While not directly a robotics application, AlphaGo's achievement in mastering Go demonstrates RL's capability to solve problems previously thought to require human intuition. The principles apply directly to complex robotic decision-making.</p> <p>Why RL was essential: - Search space is too large for brute-force approaches (more positions than atoms in the universe!) - Human expert knowledge is incomplete and biased - Requires long-term strategic thinking and pattern recognition - Optimal strategies were unknown and had to be discovered</p> <p>What this means for robotics: - RL can solve problems where the optimal solution isn't known a priori - Can discover strategies that exceed human expertise - Learns hierarchical representations and abstract concepts - Demonstrates that RL scales to extremely complex decision-making problems - Shows that RL can develop intuition-like behaviors through experience</p> <p>Robotics connection: Many robotic tasks require similar strategic thinking\u2014balancing short-term actions with long-term goals, recognizing patterns in complex sensor data, and making decisions under uncertainty. AlphaGo's success validates RL as a framework for these challenges.</p>"},{"location":"reinforcement_learning/1_introduction/#when-should-you-use-rl-for-robotics","title":"When Should You Use RL for Robotics?","text":"<p>RL is particularly powerful when:</p> <p>The optimal behavior is unknown - You know what you want to achieve, but not exactly how</p> <p>The environment is complex or stochastic - Too many variables to model accurately</p> <p>You can simulate - RL needs lots of experience; simulation makes this feasible</p> <p>Adaptability is crucial - The robot needs to handle varied, unpredictable situations</p> <p>Trial-and-error is safe - At least in simulation, the robot can make mistakes</p> <p>Avoid RL when: - You have a well-understood problem with a known solution - Safety is critical and you cannot guarantee safe exploration - You cannot simulate effectively and real-world data is scarce - Simple control methods would suffice - Interpretability and guarantees are essential</p>"},{"location":"reinforcement_learning/1_introduction/#the-learning-process-a-high-level-view","title":"The Learning Process: A High-Level View","text":"<p>Here's how an RL system learns, using a robot learning to walk as an example:</p> <ol> <li>Start with random behavior: The robot tries random motor commands</li> <li> <p>It immediately falls over! Reward: -10</p> </li> <li> <p>Try something different: Through exploration, it finds that certain joint angles keep it upright longer</p> </li> <li> <p>It stands for 2 seconds before falling. Reward: +2</p> </li> <li> <p>Build on success: It remembers that these joint configurations were good and tries variations</p> </li> <li> <p>It takes a wobbly step forward. Reward: +5</p> </li> <li> <p>Discover better strategies: Through thousands of attempts, it finds that shifting weight to one side allows stepping with the other leg</p> </li> <li> <p>It takes several steps before falling. Reward: +15</p> </li> <li> <p>Refine and optimize: Continue learning subtle adjustments to balance, speed, and energy efficiency</p> </li> <li> <p>Smooth, stable walking. Reward: +100</p> </li> <li> <p>Handle edge cases: Learn recovery behaviors when pushed or walking on slopes</p> </li> <li>Successfully recovers from perturbations. Reward: +50</li> </ol> <p>The key insight: The robot never explicitly learned \"how\" to walk in human terms. It discovered patterns of actions that led to high rewards through systematic trial and error.</p>"},{"location":"reinforcement_learning/1_introduction/#the-three-key-questions","title":"The Three Key Questions","text":"<p>Every RL problem can be understood through three questions:</p> <ol> <li> <p>What can the agent observe? (Observations/State) - Camera images? Joint angles? GPS coordinates?</p> </li> <li> <p>What can the agent do? (Actions) - Motor commands? High-level waypoints? Discrete choices?</p> </li> <li> <p>What does the agent want to achieve? (Rewards) - Reach a goal? Move quickly? Use less energy? Stay balanced?</p> </li> </ol> <p>Designing these three components well is often the key to successful RL applications!</p>"},{"location":"reinforcement_learning/1_introduction/#coming-up-next","title":"Coming Up Next","text":"<p>Now that you understand the intuition and real-world applications of RL, we'll dive deeper into:</p> <ul> <li>Markov Decision Processes (MDPs): The mathematical framework that formalizes these concepts</li> <li>Value-based vs Policy-based methods: Two different philosophical approaches to learning</li> <li>Specific algorithms: Q-Learning, REINFORCE, PPO, and more</li> <li>Hands-on implementation: Building your own RL agent from scratch</li> </ul> <p>Ready to dive deeper? Let's start with the formal framework that makes all of this work!</p>"},{"location":"reinforcement_learning/1_introduction/#check-your-understanding","title":"Check your understanding","text":"<p>Quiz 1</p> <p>Continue to Markov Decision Processes \u2192</p>"},{"location":"reinforcement_learning/1_quiz/","title":"Quiz 1","text":"Submit Quiz"},{"location":"reinforcement_learning/1_quiz/#question-1","title":"Question 1","text":"<p> <p>What is the main idea behind Reinforcement Learning?</p> <ul><li> <p>Learning from labeled examples provided by experts</p></li><li> <p>Learning through interaction and feedback from the environment</p></li><li> <p>Learning by imitating a predefined optimal controller</p></li><li> <p>Learning only from offline datasets</p></li></ul> </p>"},{"location":"reinforcement_learning/1_quiz/#question-2","title":"Question 2","text":"<p> <p>Why is RL particularly useful for robotics?</p> <ul><li> <p>Robots prefer simpler algorithms</p></li><li> <p>RL eliminates the need for sensors</p></li><li> <p>RL allows robots to learn adaptable behaviors in complex, uncertain environments</p></li><li> <p>RL guarantees perfect performance without training</p></li></ul> </p>"},{"location":"reinforcement_learning/1_quiz/#question-3","title":"Question 3","text":"<p> <p>Which of the following is a key advantage of using RL for quadruped locomotion (e.g., ANYmal)?</p> <ul><li> <p>Requires perfect terrain modeling</p></li><li> <p>Learns robust gaits without explicit terrain geometry</p></li><li> <p>Needs a separate controller for every terrain type</p></li><li> <p>Cannot handle slips or disturbances</p></li></ul> </p>"},{"location":"reinforcement_learning/1_quiz/#question-4","title":"Question 4","text":"<p> <p>In the RL learning process, what happens early in training when the agent uses random actions?</p> <ul><li> <p>The agent immediately learns optimal behavior</p></li><li> <p>The agent receives high rewards</p></li><li> <p>The agent often performs poorly but begins gathering experience</p></li><li> <p>The environment automatically corrects the agent</p></li></ul> </p>"},{"location":"reinforcement_learning/1_quiz/#question-5","title":"Question 5","text":"<p> <p>Which of the following problems is not suitable for RL?</p> <ul><li> <p>Environments with high uncertainty</p></li><li> <p>Problems where the optimal solution is unknown</p></li><li> <p>Tasks with well-understood solutions and simple controllers</p></li><li> <p>Tasks that can be simulated safely</p></li></ul> </p>"},{"location":"reinforcement_learning/2_mdp/","title":"Markov Decision Processes (MDPs)","text":""},{"location":"reinforcement_learning/2_mdp/#from-intuition-to-formalism","title":"From Intuition to Formalism","text":"<p>In the introduction, we talked about agents learning through interaction with an environment. Now let's make this precise using the mathematical framework of Markov Decision Processes (MDPs).</p> <p>MDPs provide a formal way to model sequential decision-making problems. Don't worry\u2014we'll build up the concepts step by step!</p>"},{"location":"reinforcement_learning/2_mdp/#the-key-components","title":"The Key Components","text":"<p>An MDP is defined by a tuple: \\( \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma) \\)</p> <p>Let's break down each component:</p>"},{"location":"reinforcement_learning/2_mdp/#1-state-space","title":"1. State Space","text":"<p>Definition: The set of all possible states the environment can be in.</p> <p>What is a state? A state is a complete description of the environment at a particular time. It contains all the information needed to predict what happens next (given an action).</p> <p>Examples:</p> <ul> <li>Robot navigation: Position (x, y, \u03b8), velocity, battery level</li> <li>Inverted pendulum: Pole angle, angular velocity, cart position, cart velocity</li> <li>Robotic arm: Joint angles \\([\u03b8_1, \u03b8_2, ..., \u03b8_n]\\) and joint velocities</li> <li>Drone: 3D position, orientation (roll, pitch, yaw), velocities, angular rates</li> </ul> <p>Continuous vs. Discrete: - Discrete: Finite number of states (e.g., grid world positions) - Continuous: Infinite states (e.g., robot joint angles can be any real number in a range)</p>"},{"location":"reinforcement_learning/2_mdp/#2-action-space","title":"2. Action Space","text":"<p>Definition: The set of all possible actions the agent can take.</p> <p>Examples:</p> <ul> <li>Mobile robot: Forward, backward, turn left, turn right (discrete)</li> <li>Robotic arm: Joint torques \\([\\tau_1, \\tau_2, ..., \\tau_n]\\) (continuous)</li> <li>Drone: Throttle commands for each motor (continuous)</li> <li>Quadruped: Target foot positions or joint angle trajectories (continuous)</li> </ul> <p>Discrete vs. Continuous: - Discrete: Finite set of actions (e.g., {up, down, left, right}) - Continuous: Actions are real-valued vectors (e.g., motor torques in \\(\\mathbb{R}^n\\))</p>"},{"location":"reinforcement_learning/2_mdp/#3-transition-dynamics","title":"3. Transition Dynamics","text":"<p>Definition: The probability of transitioning to state \\( s' \\) when taking action \\( a \\) in state \\( s \\).</p> <p>Mathematically: \\( \\mathcal{P}(s' | s, a) = P(S_{t+1} = s' | S_t = s, A_t = a) \\)</p> <p>What does this mean? The transition function describes the \"physics\" or \"rules\" of the environment. It tells us: - If I'm in state \\( s \\) and take action \\( a \\), what state \\( s' \\) will I end up in?</p> <p>Important properties:</p> <ul> <li>Stochastic: The next state might be random (e.g., slipping on ice, sensor noise)</li> <li>Deterministic: Special case where \\( \\mathcal{P}(s' | s, a) = 1 \\) for one \\( s' \\) and 0 for all others</li> <li>Markov Property: The next state depends only on the current state and action, not on the history</li> </ul> <p>The Markov property is key: \\( P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, ...) = P(S_{t+1} | S_t, A_t) \\)</p> <p>Example - Robot on slippery floor: <pre><code>State: Robot at position (1, 1), action: Move Right\nPossible outcomes:\n  - 70% probability: Actually move right to (2, 1)\n  - 20% probability: Slip and stay at (1, 1)\n  - 10% probability: Slip forward to (1, 2)\n</code></pre></p> <p>In robotics, the transition dynamics are typically unknown (we don't have a perfect model of physics) which is why RL is valuable!</p>"},{"location":"reinforcement_learning/2_mdp/#4-reward-function","title":"4. Reward Function","text":"<p>Definition: The immediate reward received after taking action \\( a \\) in state \\( s \\) and transitioning to \\( s' \\).</p> <p>Mathematically: \\( \\mathcal{R}(s, a, s') \\) or often simplified as \\( \\mathcal{R}(s, a) \\) or \\( \\mathcal{R}(s) \\)</p> <p>What is reward? Reward is the feedback signal that tells the agent how good its action was. It's how we communicate our objective to the agent.</p> <p>Key principle: We don't tell the agent how to achieve the goal, only what the goal is through rewards!</p> <p>Examples:</p> <ul> <li>Reaching a goal: +100 when reaching target, -1 per timestep (encourages speed)</li> <li>Staying balanced: +1 for every timestep upright, -100 for falling</li> <li>Energy efficiency: Negative reward proportional to torque magnitude</li> <li>Smooth motion: Penalty for large accelerations or jerky movements</li> </ul> <p>Reward Engineering is Critical:</p> <p>Good reward design: <pre><code>Reach goal: +100\nEach timestep alive: -1\nCollision: -50\nResult: Fast, safe navigation\n</code></pre></p> <p>Poor reward design: <pre><code>Only goal reward: +100\nNothing else\nResult: Might take forever or behave dangerously\n</code></pre></p> <p>Reward Shaping Challenges</p> <p>Designing rewards is one of the hardest parts of RL! You need to: - Specify what you want, not how to achieve it - Avoid unintended behaviors (reward hacking) - Balance multiple objectives - Ensure rewards are achievable through exploration</p>"},{"location":"reinforcement_learning/2_mdp/#5-discount-factor","title":"5. Discount Factor","text":"<p>Definition: A number between 0 and 1 that determines how much the agent values future rewards versus immediate rewards.</p> <p>Why do we need discounting?</p> <ol> <li>Mathematical convenience: Ensures infinite sums converge</li> <li>Preference for earlier rewards: Captures that immediate rewards are often more certain</li> <li>Finite horizon approximation: Effectively limits the planning horizon</li> </ol> <p>Interpretation:</p> <ul> <li>\\( \\gamma = 0 \\): Only care about immediate reward (myopic)</li> <li>\\( \\gamma = 1 \\): Care equally about all future rewards (far-sighted)</li> <li>\\( \\gamma = 0.99 \\): Common value in robotics, balances near and far future</li> </ul> <p>Example: <pre><code>Reward sequence: [1, 1, 1, 1, ...]\n\nTotal value with \u03b3 = 0.9:\n  = 1 + 0.9\u00d71 + 0.9\u00b2\u00d71 + 0.9\u00b3\u00d71 + ...\n  = 1 + 0.9 + 0.81 + 0.729 + ...\n  = 10 (converges!)\n\nTotal value with \u03b3 = 1.0:\n  = 1 + 1 + 1 + 1 + ...\n  = \u221e (diverges!)\n</code></pre></p>"},{"location":"reinforcement_learning/2_mdp/#states-vs-observations","title":"States vs. Observations","text":"<p>An important distinction in real-world robotics:</p>"},{"location":"reinforcement_learning/2_mdp/#state-fully-observable","title":"State (Fully Observable)","text":"<p>The complete description of the environment. If you know the state, the past doesn't matter for predicting the future.</p>"},{"location":"reinforcement_learning/2_mdp/#observation-partially-observable","title":"Observation (Partially Observable)","text":"<p>What the agent actually perceives. Often incomplete or noisy!</p> <p>Example - Quadrotor Navigation:</p> <p>Full State (if we could see everything): - Position (x, y, z) - Velocity (vx, vy, vz) - Orientation (roll, pitch, yaw) - Angular rates (\u03c9x, \u03c9y, \u03c9z) - Wind speed and direction - Rotor speeds - Battery voltage</p> <p>Agent's Observation (what it actually gets): - Noisy IMU readings (accelerations, angular rates) - Noisy GPS (with delay and dropouts) - Monocular camera image (no direct depth) - Battery voltage</p> <p>The observation is partial and noisy\u2014we don't know the true state!</p>"},{"location":"reinforcement_learning/2_mdp/#pomdps-partially-observable-mdps","title":"POMDPs: Partially Observable MDPs","text":"<p>When the agent only gets observations \\( o \\) instead of full state \\( s \\), we technically have a POMDP (Partially Observable MDP).</p> <p>POMDP formalism: \\( (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\Omega, \\mathcal{O}, \\gamma) \\) - \\( \\Omega \\): Observation space - \\( \\mathcal{O}(o | s, a) \\): Observation function</p> <p>Practical approaches to POMDPs: 1. State estimation: Use filters (Kalman filter, particle filter) to estimate state from observations 2. History/Memory: Use recurrent networks (LSTM, GRU) to remember past observations 3. Treat observations as state: Often works if observations contain enough information (violates Markov property but can still work!)</p>"},{"location":"reinforcement_learning/2_mdp/#the-agents-goal-return","title":"The Agent's Goal: Return","text":"<p>The agent's objective is to maximize the expected cumulative discounted reward, called the return:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\] <p>Where: - \\( G_t \\) is the return starting from time \\( t \\) - \\( R_{t+k} \\) is the reward at time \\( t+k \\) - \\( \\gamma \\) discounts future rewards</p> <p>Example:</p> <p>Imagine a robot navigating a maze: - Each timestep: -1 (encourages fast solutions) - Reaching goal: +100 - Discount factor: \u03b3 = 0.9</p> <p>Path 1 (slow, 5 steps): <pre><code>G = -1 + 0.9(-1) + 0.9\u00b2(-1) + 0.9\u00b3(-1) + 0.9\u2074(100)\n  = -1 - 0.9 - 0.81 - 0.729 + 65.61\n  = 62.17\n</code></pre></p> <p>Path 2 (fast, 3 steps): <pre><code>G = -1 + 0.9(-1) + 0.9\u00b2(100)\n  = -1 - 0.9 + 81\n  = 79.1 (Better!)\n</code></pre></p> <p>The discount factor ensures faster solutions are preferred!</p>"},{"location":"reinforcement_learning/2_mdp/#policies-the-agents-strategy","title":"Policies: The Agent's Strategy","text":"<p>A policy \\( \\pi \\) defines the agent's behavior\u2014it's a mapping from states to actions.</p>"},{"location":"reinforcement_learning/2_mdp/#deterministic-policy","title":"Deterministic Policy","text":"\\[ a = \\pi(s) \\] <p>Given state \\( s \\), the policy outputs a specific action \\( a \\).</p> <p>Example: \"If robot is at (1,1), move right\"</p>"},{"location":"reinforcement_learning/2_mdp/#stochastic-policy","title":"Stochastic Policy","text":"\\[ \\pi(a|s) = P(A_t = a | S_t = s) \\] <p>Given state \\( s \\), the policy outputs a probability distribution over actions.</p> <p>Example: \"If robot is at (1,1), move right with 70% probability, forward with 30%\"</p> <p>Why stochastic? - Exploration: Randomness helps discover new strategies - Optimal in partially observable settings: Sometimes mixing strategies is better than committing to one - Natural gradient-based optimization: Easier to optimize smooth probability distributions</p>"},{"location":"reinforcement_learning/2_mdp/#trajectories-and-episodes","title":"Trajectories and Episodes","text":"<p>A trajectory (or episode or rollout) is a sequence of states, actions, and rewards:</p> \\[ \\tau = (s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3, ...) \\]"},{"location":"reinforcement_learning/2_mdp/#episodic-tasks","title":"Episodic Tasks","text":"<p>Tasks with a natural endpoint: - Robot reaching a goal: Episode ends at goal or timeout - Game playing: Episode ends when game is won/lost - Manipulation task: Episode ends when object is grasped or dropped</p>"},{"location":"reinforcement_learning/2_mdp/#continuing-tasks","title":"Continuing Tasks","text":"<p>Tasks that go on forever: - Server load balancing: Never truly \"ends\" - Temperature control: Continuous operation - Portfolio management: Ongoing decision making</p> <p>For episodic tasks, we often reset to a starting state after each episode. This allows the agent to try many times and learn from each attempt.</p>"},{"location":"reinforcement_learning/2_mdp/#value-functions-predicting-the-future","title":"Value Functions: Predicting the Future","text":"<p>Value functions are crucial for many RL algorithms. They answer: \"How good is it to be in a state (or take an action)?\"</p>"},{"location":"reinforcement_learning/2_mdp/#state-value-function","title":"State-Value Function","text":"<p>Definition: Expected return when starting in state \\( s \\) and following policy \\( \\pi \\).</p> \\[ V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right] \\] <p>Interpretation: \"If I'm in state \\( s \\) and follow policy \\( \\pi \\) from now on, what total reward can I expect?\"</p> <p>Example - Grid World Navigation: <pre><code>Goal (+10) is at top-right corner\nEach step gives -1 reward\n\u03b3 = 0.9\n\nV^\u03c0(top-right) = 10      (at goal)\nV^\u03c0(one step away) \u2248 -1 + 0.9\u00d710 = 8\nV^\u03c0(two steps away) \u2248 -1 + 0.9\u00d78 = 6.2\nV^\u03c0(far away) \u2248 -5       (many costly steps to goal)\n</code></pre></p>"},{"location":"reinforcement_learning/2_mdp/#action-value-function","title":"Action-Value Function","text":"<p>Definition: Expected return when starting in state \\( s \\), taking action \\( a \\), then following policy \\( \\pi \\).</p> \\[ Q^\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\] <p>Interpretation: \"If I'm in state \\( s \\), take action \\( a \\), then follow policy \\( \\pi \\), what total reward can I expect?\"</p> <p>Relationship between V and Q:</p> \\[ V^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) Q^\\pi(s, a) \\] <p>The value of a state is the expected action-value under the policy.</p>"},{"location":"reinforcement_learning/2_mdp/#optimal-policies-and-value-functions","title":"Optimal Policies and Value Functions","text":"<p>The goal of RL is to find the optimal policy \\( \\pi^* \\) that maximizes expected return.</p>"},{"location":"reinforcement_learning/2_mdp/#optimal-policy","title":"Optimal Policy","text":"\\[ \\pi^* = \\arg\\max_\\pi V^\\pi(s) \\quad \\forall s \\in \\mathcal{S} \\] <p>A policy that achieves the highest possible value in every state.</p>"},{"location":"reinforcement_learning/2_mdp/#optimal-value-functions","title":"Optimal Value Functions","text":"<p>Optimal state-value function: [ V^*(s) = \\max_\\pi V^\\pi(s) ]</p> <p>Optimal action-value function: [ Q^*(s, a) = \\max_\\pi Q^\\pi(s, a) ]</p>"},{"location":"reinforcement_learning/2_mdp/#extracting-optimal-policy-from-q","title":"Extracting Optimal Policy from Q","text":"<p>If we know \\( Q^*(s, a) \\), the optimal policy is:</p> \\[ \\pi^*(s) = \\arg\\max_a Q^*(s, a) \\] <p>Interpretation: In each state, choose the action with the highest Q-value!</p> <p>This is why Q-learning is so powerful\u2014if we learn \\( Q^* \\), we automatically know the optimal policy.</p>"},{"location":"reinforcement_learning/2_mdp/#bellman-equations-recursive-structure","title":"Bellman Equations: Recursive Structure","text":"<p>Value functions satisfy recursive relationships called Bellman equations. These are fundamental to RL algorithms!</p>"},{"location":"reinforcement_learning/2_mdp/#bellman-expectation-equation-for-v","title":"Bellman Expectation Equation (for V)","text":"\\[ V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} \\mathcal{P}(s'|s,a) [R(s,a,s') + \\gamma V^\\pi(s')] \\] <p>Intuition: The value of state \\( s \\) equals: - Expected immediate reward - Plus discounted value of next state</p>"},{"location":"reinforcement_learning/2_mdp/#bellman-expectation-equation-for-q","title":"Bellman Expectation Equation (for Q)","text":"\\[ Q^\\pi(s,a) = \\sum_{s'} \\mathcal{P}(s'|s,a) [R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')] \\]"},{"location":"reinforcement_learning/2_mdp/#bellman-optimality-equation-for-v","title":"Bellman Optimality Equation (for V)","text":"\\[ V^*(s) = \\max_a \\sum_{s'} \\mathcal{P}(s'|s,a) [R(s,a,s') + \\gamma V^*(s')] \\]"},{"location":"reinforcement_learning/2_mdp/#bellman-optimality-equation-for-q","title":"Bellman Optimality Equation (for Q)","text":"\\[ Q^*(s,a) = \\sum_{s'} \\mathcal{P}(s'|s,a) [R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')] \\] <p>Why are these important? - They provide a way to compute value functions iteratively - They form the basis of many RL algorithms (Q-learning, value iteration, policy iteration) - They show that optimal values satisfy a self-consistency condition</p>"},{"location":"reinforcement_learning/2_mdp/#the-rl-problem-what-are-we-trying-to-solve","title":"The RL Problem: What are we trying to solve?","text":"<p>Now we can state the RL problem formally:</p> <p>The Reinforcement Learning Problem</p> <p>Given: An MDP \\( (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma) \\) where \\( \\mathcal{P} \\) and \\( \\mathcal{R} \\) may be unknown</p> <p>Goal: Find a policy \\( \\pi^* \\) that maximizes expected cumulative discounted reward:</p> \\[ \\pi^* = \\arg\\max_\\pi \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_t\\right] \\] <p>Challenge: We must learn through interaction, without knowing \\( \\mathcal{P} \\) or \\( \\mathcal{R} \\) in advance!</p>"},{"location":"reinforcement_learning/2_mdp/#exploration-vs-exploitation","title":"Exploration vs. Exploitation","text":"<p>A fundamental challenge in RL:</p> <p>Exploitation: Choose actions that have given high rewards in the past Exploration: Try new actions to discover potentially better strategies</p> <p>Example - Restaurant choice: - Exploit: Go to your favorite restaurant (known good reward) - Explore: Try a new restaurant (might be better, might be worse)</p> <p>If you only exploit, you might miss out on better options. If you only explore, you never benefit from what you've learned.</p> <p>The key is to balance exploration and exploitation!</p> <p>Common strategies: - \u03b5-greedy: With probability \u03b5, choose random action; otherwise choose best known action - Boltzmann exploration: Sample actions proportional to their estimated value (using softmax) - Optimistic initialization: Start with high value estimates to encourage trying everything - Upper Confidence Bound (UCB): Favor actions you're uncertain about - Entropy regularization: Add bonus for policy randomness</p>"},{"location":"reinforcement_learning/2_mdp/#putting-it-all-together-the-rl-loop","title":"Putting It All Together: The RL Loop","text":"<p>The interaction cycle:</p> <ol> <li>Agent observes state \\( s_t \\)</li> <li>Agent selects action \\( a_t \\) according to policy \\( \\pi(a_t | s_t) \\)</li> <li>Environment transitions to \\( s_{t+1} \\sim \\mathcal{P}(s_{t+1} | s_t, a_t) \\)</li> <li>Environment returns reward \\( r_{t+1} = \\mathcal{R}(s_t, a_t, s_{t+1}) \\)</li> <li>Agent updates its policy based on experience \\( (s_t, a_t, r_{t+1}, s_{t+1}) \\)</li> <li>Repeat!</li> </ol>"},{"location":"reinforcement_learning/2_mdp/#summary-key-takeaways","title":"Summary: Key Takeaways","text":"<p>MDP: Formal framework for sequential decision-making</p> <p>Five components: States, actions, transitions, rewards, discount factor</p> <p>State vs. Observation: Full information vs. what agent perceives</p> <p>Policy: Agent's strategy for choosing actions</p> <p>Value functions: Predict expected future reward</p> <p>Bellman equations: Recursive relationships that enable learning</p> <p>Goal: Find optimal policy \\( \\pi^* \\) that maximizes expected return</p> <p>Challenge: Learn through interaction without knowing dynamics</p> <p>Now that we have the formal framework, we can understand different approaches to solving MDPs!</p>"},{"location":"reinforcement_learning/2_mdp/#check-your-understanding","title":"Check your understanding","text":"<p>Quiz 2</p> <p>\u2190 Back to Introduction Continue to Policy vs Value-Based Methods \u2192</p>"},{"location":"reinforcement_learning/2_quiz/","title":"Quiz 2","text":"Submit Quiz"},{"location":"reinforcement_learning/2_quiz/#question-1","title":"Question 1","text":"<p> <p>What are the five components of an MDP?</p> <ul><li> <p>States, Actions, Policy, Rewards, Discount</p></li><li> <p>States, Actions, Transitions, Rewards, Discount</p></li><li> <p>States, Actions, Observations, Rewards, Discount</p></li><li> <p>States, Actions, Value function, Rewards, Discount</p></li></ul> </p>"},{"location":"reinforcement_learning/2_quiz/#question-2","title":"Question 2","text":"<p> <p>What does the Markov property state?</p> <ul><li> <p>The next state depends on the entire history of states and actions</p></li><li> <p>The next state depends only on the current state and action, not on history</p></li><li> <p>The next state is always deterministic</p></li><li> <p>The next state depends only on the current action</p></li></ul> </p>"},{"location":"reinforcement_learning/2_quiz/#question-3","title":"Question 3","text":"<p> <p>What is the key difference between the state-value function V and the action-value function Q?</p> <ul><li> <p>V is for deterministic policies, Q is for stochastic policies</p></li><li> <p>V is the expected return from a state following a policy, while Q is the expected return from a state after taking a specific action then following the policy</p></li><li> <p>V is for continuous states, Q is for discrete states</p></li><li> <p>There is no difference; they are the same function</p></li></ul> </p>"},{"location":"reinforcement_learning/2_quiz/#question-4","title":"Question 4","text":"<p> <p>Why is a discount factor less than 1 necessary for infinite-horizon problems?</p> <ul><li> <p>It makes the agent prefer immediate rewards over future rewards</p></li><li> <p>It allows the agent to forget past experiences</p></li><li> <p>It ensures the infinite sum of discounted rewards converges mathematically</p></li><li> <p>It prevents the agent from exploring too much</p></li></ul> </p>"},{"location":"reinforcement_learning/2_quiz/#question-5","title":"Question 5","text":"<p> <p>What is the main difference between a state and an observation in robotics?</p> <ul><li> <p>States are discrete, observations are continuous</p></li><li> <p>States contain complete information about the environment, while observations are partial and noisy measurements that the agent actually perceives</p></li><li> <p>States are what the agent controls, observations are what the environment controls</p></li><li> <p>There is no difference; they are the same thing</p></li></ul> </p>"},{"location":"reinforcement_learning/3_policy_vs_value/","title":"Policy-Based vs Value-Based Methods","text":""},{"location":"reinforcement_learning/3_policy_vs_value/#two-philosophies-for-solving-rl-problems","title":"Two Philosophies for Solving RL Problems","text":"<p>Now that we understand the MDP framework, how do we actually find the optimal policy \\( \\pi^* \\)?</p> <p>There are two fundamentally different approaches:</p> <ol> <li>Value-Based Methods: Learn to estimate how good states or actions are, then derive a policy</li> <li>Policy-Based Methods: Directly learn the policy that maps states to actions</li> </ol> <p>Both aim to find \\( \\pi^* \\), but they take very different paths to get there!</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#value-based-methods-learn-the-value-extract-the-policy","title":"Value-Based Methods: Learn the Value, Extract the Policy","text":""},{"location":"reinforcement_learning/3_policy_vs_value/#core-idea","title":"Core Idea","text":"<p>\"If I know how valuable each action is, I can just pick the best one!\"</p> <p>Value-based methods learn a value function (typically \\( Q(s, a) \\)) that estimates the expected return for taking action \\( a \\) in state \\( s \\). Once we have good estimates of \\( Q(s, a) \\), the policy is trivial:</p> \\[ \\pi(s) = \\arg\\max_a Q(s, a) \\] <p>Simple interpretation: In each state, choose the action with the highest Q-value!</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#the-learning-process","title":"The Learning Process","text":"<pre><code>graph LR\n    A[\"Experience&lt;br/&gt;s, a, r, s'\"] --&gt; B[\"Update Q-values\"]\n    B --&gt; C[\"Q-table or&lt;br/&gt;Q-network\"]\n    C --&gt; D[\"Policy: argmax Q(s,a)\"]\n    D --&gt; E[\"Take action\"]\n    E --&gt; A</code></pre>"},{"location":"reinforcement_learning/3_policy_vs_value/#examples-of-value-based-methods","title":"Examples of Value-Based Methods","text":"<ul> <li>Q-Learning: Learn Q-values through Bellman updates</li> <li>SARSA: On-policy variant of Q-learning</li> <li>DQN (Deep Q-Network): Use neural networks to approximate Q-values</li> <li>Double DQN, Dueling DQN: Improvements on DQN</li> </ul>"},{"location":"reinforcement_learning/3_policy_vs_value/#advantages","title":"Advantages","text":"<ol> <li>Sample efficient: Each experience can update many action values</li> <li>Off-policy learning: Can learn from any experience (even random actions!)</li> <li>Deterministic optimal policy: For many problems, the optimal policy is deterministic</li> <li>Easy to understand: \"Pick the action with highest value\" is intuitive</li> <li>Stable in discrete action spaces: Clear max operation over actions</li> </ol>"},{"location":"reinforcement_learning/3_policy_vs_value/#disadvantages","title":"Disadvantages","text":"<ol> <li>Limited to discrete actions: Computing \\( \\arg\\max_a Q(s,a) \\) is hard when actions are continuous</li> <li>No stochasticity: Derived policy is deterministic (can be addressed with exploration strategies)</li> <li>Instability with function approximation: Can diverge when using neural networks (though solutions exist)</li> <li>Maximization bias: Taking max can overestimate values</li> </ol>"},{"location":"reinforcement_learning/3_policy_vs_value/#when-to-use-value-based-methods","title":"When to Use Value-Based Methods?","text":"<p>Discrete action spaces (e.g., game controls, discrete robot commands)</p> <p>Off-policy learning is valuable (want to learn from demonstrations or replay buffers)</p> <p>Deterministic policies are acceptable</p> <p>Continuous action spaces (robotics with motor torques, joint angles)</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#policy-based-methods-learn-the-policy-directly","title":"Policy-Based Methods: Learn the Policy Directly","text":""},{"location":"reinforcement_learning/3_policy_vs_value/#core-idea_1","title":"Core Idea","text":"<p>\"Why bother with value functions? Just learn the policy itself!\"</p> <p>Policy-based methods directly parameterize the policy \\( \\pi_\\theta(a|s) \\) with parameters \\( \\theta \\) and optimize it to maximize expected return.</p> <p>Key insight: We optimize \\( \\theta \\) to maximize:</p> \\[ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_t \\gamma^t r_t\\right] \\] <p>This is a direct optimization of what we care about!</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#the-learning-process_1","title":"The Learning Process","text":"<pre><code>graph LR\n    A[\"Collect trajectories&lt;br/&gt;using policy\"] --&gt; B[\"Compute returns\"]\n    B --&gt; C[\"Compute gradients\"]\n    C --&gt; D[\"Update policy\"]\n    D --&gt; A</code></pre>"},{"location":"reinforcement_learning/3_policy_vs_value/#examples-of-policy-based-methods","title":"Examples of Policy-Based Methods","text":"<ul> <li>REINFORCE: Basic policy gradient algorithm</li> <li>TRPO (Trust Region Policy Optimization): Safe policy updates</li> <li>PPO (Proximal Policy Optimization): More practical version of TRPO</li> <li>A3C (Asynchronous Advantage Actor-Critic): Parallel policy learning</li> </ul>"},{"location":"reinforcement_learning/3_policy_vs_value/#advantages_1","title":"Advantages","text":"<ol> <li>Natural for continuous actions: Can directly output continuous values</li> <li>Can learn stochastic policies: Sometimes optimal policy is stochastic!</li> <li>Better convergence properties: Typically more stable than value-based methods</li> <li>Effective in high-dimensional action spaces: No need to evaluate all actions</li> <li>Can learn from limited observability: Works naturally in POMDPs</li> </ol>"},{"location":"reinforcement_learning/3_policy_vs_value/#disadvantages_1","title":"Disadvantages","text":"<ol> <li>Sample inefficient: Need many trajectories to estimate gradients</li> <li>High variance: Gradient estimates can be noisy</li> <li>On-policy by default: Must collect new data after each update (can be expensive)</li> <li>Can converge to local optima: Gradient-based optimization doesn't guarantee global optimum</li> <li>Slower to train: Needs more environment interactions</li> </ol>"},{"location":"reinforcement_learning/3_policy_vs_value/#when-to-use-policy-based-methods","title":"When to Use Policy-Based Methods?","text":"<p>Continuous action spaces (robotic control, motor commands)</p> <p>High-dimensional action spaces</p> <p>Stochastic policies needed (e.g., rock-paper-scissors, partially observable environments)</p> <p>Stability is important</p> <p>Sample efficiency is critical (real-world robot learning with limited trials)</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#side-by-side-comparison","title":"Side-by-Side Comparison","text":"Aspect Value-Based Policy-Based What do we learn? \\( Q(s, a) \\) or \\( V(s) \\) \\( \\pi_\\theta(a \\mid s) \\) Policy extraction \\( \\pi(s) = \\arg\\max_a Q(s,a) \\) Policy is directly learned Action space Best for discrete Best for continuous Policy type Deterministic (usually) Can be stochastic Sample efficiency More efficient Less efficient Convergence Can be unstable More stable Off-policy learning Natural Requires importance sampling Exploration Via \u03b5-greedy, etc. Via stochastic policy Examples Q-Learning, DQN REINFORCE, PPO"},{"location":"reinforcement_learning/3_policy_vs_value/#intuitive-examples","title":"Intuitive Examples","text":"<p>Let's understand the difference with concrete examples:</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#example-1-grid-world-navigation","title":"Example 1: Grid World Navigation","text":"<p>Value-Based Approach:</p> <pre><code>Learn Q-values for each state-action pair:\n\nState (2,3):\n  Q(s, up)    = 5.2\n  Q(s, down)  = 3.1\n  Q(s, left)  = 4.8\n  Q(s, right) = 7.3  \u2190 Highest!\n\nPolicy: \u03c0(s) = right\n</code></pre> <p>The agent learns \"how good is each direction?\" then picks the best.</p> <p>Policy-Based Approach:</p> <pre><code>Learn policy directly:\n\nState (2,3):\n  \u03c0(up | s)    = 0.10\n  \u03c0(down | s)  = 0.05\n  \u03c0(left | s)  = 0.15\n  \u03c0(right | s) = 0.70  \u2190 Most likely\n\nSample action from this distribution\n</code></pre> <p>The agent learns \"which direction should I go?\" directly.</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#example-2-robot-arm-control","title":"Example 2: Robot Arm Control","text":"<p>Problem: Move robot arm to target position</p> <p>Action space: 7 joint torques (continuous, \\(\\mathbb{R}^7\\))</p> <p>Value-Based Challenge: <pre><code>How do we compute argmax Q(s, a) when a \u2208 \u211d\u2077?\n- Would need to evaluate Q for infinitely many actions!\n- Or discretize actions (loses precision)\n</code></pre></p> <p>Policy-Based Solution: <pre><code>Policy directly outputs continuous actions:\n\u03c0_\u03b8(s) \u2192 [\u03c4\u2081, \u03c4\u2082, \u03c4\u2083, \u03c4\u2084, \u03c4\u2085, \u03c4\u2086, \u03c4\u2087]\n\nOr outputs distribution parameters:\n\u03c0_\u03b8(s) \u2192 \u03bc(s), \u03c3(s)\nSample: a ~ N(\u03bc(s), \u03c3(s))\n</code></pre></p> <p>This is natural and efficient!</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#the-exploration-strategy-difference","title":"The Exploration Strategy Difference","text":""},{"location":"reinforcement_learning/3_policy_vs_value/#value-based-exploration","title":"Value-Based Exploration","text":"<p>Must add exploration explicitly:</p> <p>\u03b5-greedy: <pre><code>if random() &lt; epsilon:\n    action = random_action()  # Explore\nelse:\n    action = argmax_a Q(s, a)  # Exploit\n</code></pre></p> <p>Boltzmann exploration: <pre><code>probabilities = softmax(Q_values / temperature)\naction = sample(probabilities)\n</code></pre></p>"},{"location":"reinforcement_learning/3_policy_vs_value/#policy-based-exploration","title":"Policy-Based Exploration","text":"<p>Exploration is built into the policy!</p> <pre><code># Stochastic policy (e.g., Gaussian)\nmean, std = policy_network(state)\naction = sample_normal(mean, std)  # Naturally explores!\n\n# Entropy bonus encourages exploration\nloss = -expected_return + entropy_coefficient * entropy(policy)\n</code></pre> <p>The randomness of the policy provides natural exploration.</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#continuous-action-spaces-why-policy-methods-shine","title":"Continuous Action Spaces: Why Policy Methods Shine","text":""},{"location":"reinforcement_learning/3_policy_vs_value/#the-challenge-with-value-methods","title":"The Challenge with Value Methods","text":"<p>For continuous actions \\( a \\in \\mathbb{R}^n \\):</p> \\[ \\pi(s) = \\arg\\max_{a \\in \\mathbb{R}^n} Q(s, a) \\] <p>This is an optimization problem at every timestep!</p> <p>Options: 1. Discretize actions: Loses precision, curse of dimensionality 2. Use optimization: Expensive, need many Q-function evaluations 3. Assume Q is simple: Rarely true (quadratic approximations, etc.)</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#policy-methods-natural-fit","title":"Policy Methods: Natural Fit","text":"<p>Policy network directly outputs continuous actions:</p> <pre><code># Policy network architecture\nstate \u2192 [Neural Network] \u2192 action_mean, action_std\n\n# Gaussian policy\naction = action_mean + action_std * random_normal()\n</code></pre> <p>No argmax needed! Just a forward pass through the network.</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#stochastic-vs-deterministic-policies","title":"Stochastic vs Deterministic Policies","text":""},{"location":"reinforcement_learning/3_policy_vs_value/#when-is-a-stochastic-policy-optimal","title":"When is a Stochastic Policy Optimal?","text":"<p>Example 1: Rock-Paper-Scissors</p> <p>Optimal policy: \\( \\pi(\\text{rock}) = \\pi(\\text{paper}) = \\pi(\\text{scissors}) = 1/3 \\)</p> <p>If you're deterministic, opponent can exploit you!</p> <p>Example 2: Partially Observable Environments</p> <p>If you can't see the full state, randomizing can be beneficial.</p> <p>Example 3: Multi-Agent Settings</p> <p>Don't want to be predictable to other agents.</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#value-methods-struggle-here","title":"Value Methods Struggle Here","text":"<p>Value-based methods naturally give deterministic policies: [ \\pi(s) = \\arg\\max_a Q(s, a) ]</p> <p>To get stochasticity, must add it artificially (\u03b5-greedy, Boltzmann).</p> <p>Policy methods can naturally represent stochastic optimal policies!</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#actor-critic-best-of-both-worlds","title":"Actor-Critic: Best of Both Worlds?","text":"<p>We'll cover this in detail later, but here's a preview:</p> <p>Idea: Combine value and policy methods!</p> <ul> <li>Actor (policy): Decides which actions to take</li> <li>Critic (value): Evaluates how good those actions were</li> </ul> <pre><code>graph LR\n    A[\"State s\"] --&gt; B[\"Actor:&lt;br/&gt;Policy\"]\n    A --&gt; C[\"Critic:&lt;br/&gt;Value function\"]\n    B --&gt; D[\"Action a\"]\n    C --&gt; E[\"TD Error\"]\n    E --&gt; B\n    E --&gt; C</code></pre> <p>Benefits: - Lower variance than pure policy methods (critic helps) - Works with continuous actions (actor handles this) - More sample efficient than pure policy methods</p> <p>Examples: A2C, A3C, SAC, TD3, PPO (with value baseline)</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#summary-which-approach-should-you-use","title":"Summary: Which Approach Should You Use?","text":""},{"location":"reinforcement_learning/3_policy_vs_value/#use-value-based-methods-when","title":"Use Value-Based Methods When:","text":"<p>You have discrete action spaces Sample efficiency is critical You want off-policy learning Deterministic policies are fine</p> <p>Example tasks: - Atari games - Grid world navigation - Discrete robot control (waypoint selection)</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#use-policy-based-methods-when","title":"Use Policy-Based Methods When:","text":"<p>You have continuous action spaces You need stochastic policies Stability is more important than sample efficiency High-dimensional action spaces</p> <p>Example tasks: - Robot manipulation (continuous joint torques) - Locomotion (continuous motor commands) - Drone control - Autonomous driving</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#use-actor-critic-when","title":"Use Actor-Critic When:","text":"<p>You want a balance of both approaches Continuous actions and sample efficiency You want stability of policy methods with lower variance</p> <p>Example tasks: - Most modern robotics applications! - Continuous control with sample constraints</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#the-big-picture","title":"The Big Picture","text":"<pre><code>graph TB\n    A[\"RL Methods\"] --&gt; B[\"Value-Based\"]\n    A --&gt; C[\"Policy-Based\"]\n    A --&gt; D[\"Actor-Critic\"]\n    B --&gt; B1[\"Q-Learning\"]\n    B --&gt; B2[\"DQN\"]\n    B --&gt; B3[\"SARSA\"]\n    C --&gt; C1[\"REINFORCE\"]\n    C --&gt; C2[\"PPO\"]\n    C --&gt; C3[\"TRPO\"]\n    D --&gt; D1[\"A2C/A3C\"]\n    D --&gt; D2[\"SAC\"]\n    D --&gt; D3[\"TD3\"]</code></pre> <p>Historical note: - Classic RL: Started with value-based methods (tabular Q-learning) - Deep RL: DQN (2015) showed value methods could scale to complex tasks - Modern robotics: Policy methods (especially PPO) dominate due to continuous control - State-of-the-art: Actor-critic methods combining best of both</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#coming-up-next","title":"Coming Up Next","text":"<p>Now that you understand the two main approaches, let's dive into specific algorithms:</p> <ol> <li>Q-Learning: The classic value-based method</li> <li>REINFORCE: The classic policy gradient method</li> </ol> <p>Understanding these foundations will help you grasp modern algorithms like DQN, PPO, SAC, and more!</p>"},{"location":"reinforcement_learning/3_policy_vs_value/#check-your-understanding","title":"Check your understanding","text":"<p>Quiz 3</p> <p>\u2190 Back to MDP Continue to Q-Learning \u2192</p>"},{"location":"reinforcement_learning/3_quiz/","title":"Quiz 3","text":"Submit Quiz"},{"location":"reinforcement_learning/3_quiz/#question-1","title":"Question 1","text":"<p> <p>What is the main difference between value-based and policy-based methods?</p> <ul><li> <p>Value-based methods learn faster, policy-based methods learn slower</p></li><li> <p>Value-based methods learn value functions then derive a policy, while policy-based methods directly learn the policy</p></li><li> <p>Value-based methods work for continuous actions, policy-based methods work for discrete actions</p></li><li> <p>There is no difference; they are the same approach</p></li></ul> </p>"},{"location":"reinforcement_learning/3_quiz/#question-2","title":"Question 2","text":"<p> <p>Which type of method is better suited for continuous action spaces like robot joint torques?</p> <ul><li> <p>Value-based methods, because they can learn Q-values for any action</p></li><li> <p>Policy-based methods, because they can directly output continuous actions without needing to maximize over infinite possibilities</p></li><li> <p>Both work equally well for continuous actions</p></li><li> <p>Neither works for continuous actions; you must discretize first</p></li></ul> </p>"},{"location":"reinforcement_learning/3_quiz/#question-3","title":"Question 3","text":"<p> <p>What is a key advantage of value-based methods over policy-based methods?</p> <ul><li> <p>They work better for continuous actions</p></li><li> <p>They are more sample efficient and can learn off-policy from any experience</p></li><li> <p>They always converge faster</p></li><li> <p>They require less computation</p></li></ul> </p>"},{"location":"reinforcement_learning/3_quiz/#question-4","title":"Question 4","text":"<p> <p>How do policy-based methods handle exploration compared to value-based methods?</p> <ul><li> <p>Policy-based methods require explicit exploration strategies like epsilon-greedy</p></li><li> <p>Policy-based methods have exploration built into the stochastic policy, while value-based methods need explicit exploration strategies</p></li><li> <p>Both require the same exploration mechanisms</p></li><li> <p>Policy-based methods cannot explore at all</p></li></ul> </p>"},{"location":"reinforcement_learning/3_quiz/#question-5","title":"Question 5","text":"<p> <p>What is the main idea behind actor-critic methods?</p> <ul><li> <p>They use only value functions without any policy</p></li><li> <p>They use only policies without any value functions</p></li><li> <p>They combine both approaches: an actor (policy) decides actions while a critic (value function) evaluates them</p></li><li> <p>They are a completely different approach unrelated to value or policy methods</p></li></ul> </p>"},{"location":"reinforcement_learning/4_quiz/","title":"Quiz 4","text":"Submit Quiz"},{"location":"reinforcement_learning/4_quiz/#question-1","title":"Question 1","text":"<p> <p>What is the Q-Learning update rule?</p> <ul><li> <p>Q(s, a) \u2190 Q(s, a) + \u03b1[r + \u03b3 Q(s', a') - Q(s, a)] where a' is the action actually taken</p></li><li> <p>Q(s, a) \u2190 Q(s, a) + \u03b1[r + \u03b3 max_a' Q(s', a') - Q(s, a)] where we use the maximum Q-value in the next state</p></li><li> <p>Q(s, a) \u2190 Q(s, a) + \u03b1[r - Q(s, a)] without considering future rewards</p></li><li> <p>Q(s, a) \u2190 r + \u03b3 max_a' Q(s', a') replacing the old value completely</p></li></ul> </p>"},{"location":"reinforcement_learning/4_quiz/#question-2","title":"Question 2","text":"<p> <p>Why is Q-Learning considered an off-policy algorithm?</p> <ul><li> <p>Because it requires knowing the transition probabilities P(s'|s, a)</p></li><li> <p>Because it learns the optimal policy while following an exploratory policy (like \u03b5-greedy)</p></li><li> <p>Because it only works with deterministic policies</p></li><li> <p>Because it cannot learn from past experiences</p></li></ul> </p>"},{"location":"reinforcement_learning/4_quiz/#question-3","title":"Question 3","text":"<p> <p>What is the purpose of the \u03b5-greedy exploration strategy in Q-Learning?</p> <ul><li> <p>To always choose the best action to maximize immediate reward</p></li><li> <p>To balance exploration (trying new actions) and exploitation (using learned Q-values)</p></li><li> <p>To ensure the algorithm converges faster</p></li><li> <p>To reduce the variance in Q-value estimates</p></li></ul> </p>"},{"location":"reinforcement_learning/4_quiz/#question-4","title":"Question 4","text":"<p> <p>What is a key limitation of Q-Learning that makes it unsuitable for continuous action spaces?</p> <ul><li> <p>It requires too much memory to store Q-values</p></li><li> <p>It needs to compute argmax over all actions, which is impossible or very difficult with infinite continuous actions</p></li><li> <p>It converges too slowly for continuous actions</p></li><li> <p>It cannot handle stochastic environments</p></li></ul> </p>"},{"location":"reinforcement_learning/4_value_based/","title":"Q-Learning: Value-Based Reinforcement Learning","text":""},{"location":"reinforcement_learning/4_value_based/#introduction","title":"Introduction","text":"<p>Q-Learning is one of the most important algorithms in reinforcement learning. It's elegant, powerful, and surprisingly simple!</p> <p>Core idea: Learn the value of taking each action in each state, then act greedily by choosing the best action.</p>"},{"location":"reinforcement_learning/4_value_based/#the-q-function-what-are-we-learning","title":"The Q-Function: What Are We Learning?","text":"<p>Recall the action-value function:</p> \\[ Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid S_0 = s, A_0 = a\\right] \\] <p>Interpretation: \"If I'm in state \\( s \\), take action \\( a \\), then follow policy \\( \\pi \\), what total discounted reward will I get?\"</p> <p>The optimal Q-function \\( Q^*(s, a) \\) tells us the best possible value:</p> \\[ Q^*(s, a) = \\max_\\pi Q^\\pi(s, a) \\] <p>Key insight: If we know \\( Q^*(s, a) \\), the optimal policy is trivial:</p> \\[ \\pi^*(s) = \\arg\\max_a Q^*(s, a) \\] <p>Just pick the action with the highest Q-value!</p>"},{"location":"reinforcement_learning/4_value_based/#the-bellman-optimality-equation-for-q","title":"The Bellman Optimality Equation for Q","text":"<p>The optimal Q-function satisfies the Bellman optimality equation:</p> \\[ Q^*(s, a) = \\mathbb{E}_{s' \\sim P(s'|s,a)}\\left[R(s, a, s') + \\gamma \\max_{a'} Q^*(s', a')\\right] \\] <p>Intuition: The value of taking action \\( a \\) in state \\( s \\) equals: 1. The immediate reward \\( R(s, a, s') \\) 2. Plus the discounted value of the best action in the next state</p> <p>This gives us a way to iteratively improve our estimates!</p>"},{"location":"reinforcement_learning/4_value_based/#q-learning-algorithm","title":"Q-Learning Algorithm","text":""},{"location":"reinforcement_learning/4_value_based/#the-update-rule","title":"The Update Rule","text":"<p>Q-Learning uses the Bellman equation to update Q-values:</p> \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\\right] \\] <p>Where: - \\( \\alpha \\) is the learning rate (step size) - \\( r \\) is the immediate reward - \\( \\gamma \\) is the discount factor - The term in brackets is called the temporal difference (TD) error</p>"},{"location":"reinforcement_learning/4_value_based/#understanding-the-update","title":"Understanding the Update","text":"<p>Let's break down the TD error:</p> \\[ \\delta = \\underbrace{r + \\gamma \\max_{a'} Q(s', a')}_{\\text{TD target}} - \\underbrace{Q(s, a)}_{\\text{Current estimate}} \\] <ul> <li>TD target: Our new estimate of \\( Q(s, a) \\) based on the reward we just received and the best action in the next state</li> <li>Current estimate: What we currently think \\( Q(s, a) \\) is</li> <li>TD error: How wrong our current estimate was</li> </ul> <p>We move our estimate in the direction of the TD target:</p> <pre><code>New estimate = Old estimate + learning_rate \u00d7 TD_error\n</code></pre>"},{"location":"reinforcement_learning/4_value_based/#the-complete-algorithm","title":"The Complete Algorithm","text":"<pre><code># Pseudocode for Q-Learning\n\nInitialize Q(s, a) arbitrarily for all s, a\nSet Q(terminal_state, *) = 0\n\nFor each episode:\n    Initialize state s\n\n    While s is not terminal:\n        # Choose action (\u03b5-greedy)\n        With probability \u03b5:\n            a = random action\n        Otherwise:\n            a = argmax_a' Q(s, a')\n\n        # Take action, observe outcome\n        Take action a, observe reward r and next state s'\n\n        # Q-Learning update\n        Q(s, a) \u2190 Q(s, a) + \u03b1[r + \u03b3 max_a' Q(s', a') - Q(s, a)]\n\n        # Move to next state\n        s \u2190 s'\n</code></pre>"},{"location":"reinforcement_learning/4_value_based/#key-properties","title":"Key Properties","text":"<p>Off-policy: Learns optimal policy while following exploratory policy (\u03b5-greedy)</p> <p>Model-free: Doesn't need to know transition probabilities \\( P(s'|s, a) \\)</p> <p>Converges to optimal: Under certain conditions (visiting all state-action pairs infinitely often, appropriate learning rate schedule)</p>"},{"location":"reinforcement_learning/4_value_based/#exploration-the-greedy-policy","title":"Exploration: The \u03b5-Greedy Policy","text":"<p>Q-Learning needs to explore to discover good actions, but also exploit what it has learned.</p> <p>\u03b5-greedy policy:</p> \\[ \\pi(s) = \\begin{cases} \\text{random action} &amp; \\text{with probability } \\epsilon \\\\ \\arg\\max_a Q(s, a) &amp; \\text{with probability } 1 - \\epsilon \\end{cases} \\] <p>Typical schedule: - Start with high \u03b5 (e.g., 1.0): Explore extensively at first - Decay \u03b5 over time (e.g., to 0.01): Exploit more as we learn - Keep small \u03b5 forever: Continue exploring slightly to adapt to changes</p> <pre><code># Epsilon decay schedule\nepsilon = max(epsilon_min, epsilon_start * epsilon_decay ** episode)\n</code></pre>"},{"location":"reinforcement_learning/4_value_based/#worked-example-grid-world","title":"Worked Example: Grid World","text":"<p>Let's walk through Q-Learning on a simple 3\u00d73 grid world.</p> <p>Setup: - Start: Bottom-left (0, 0) - Goal: Top-right (2, 2), reward = +10 - Each step: reward = -1 (encourages finding shortest path) - Actions: {up, down, left, right} - Discount factor: \u03b3 = 0.9 - Learning rate: \u03b1 = 0.1</p> <p>Initial Q-table: All zeros</p> <pre><code>State | Up   | Down | Left | Right\n------|------|------|------|------\n(0,0) | 0.0  | 0.0  | 0.0  | 0.0\n(0,1) | 0.0  | 0.0  | 0.0  | 0.0\n...\n</code></pre>"},{"location":"reinforcement_learning/4_value_based/#episode-1-random-exploration","title":"Episode 1: Random Exploration","text":"<p>Step 1: State (0,0), choose random action: Right - Take action, observe: s' = (1,0), r = -1 - Update:   [   Q((0,0), \\text{right}) = 0 + 0.1[-1 + 0.9 \\times 0 - 0] = -0.1   ]</p> <p>Step 2: State (1,0), choose random action: Up - Take action, observe: s' = (1,1), r = -1 - Update:   [   Q((1,0), \\text{up}) = 0 + 0.1[-1 + 0.9 \\times 0 - 0] = -0.1   ]</p> <p>Continue until reaching goal...</p>"},{"location":"reinforcement_learning/4_value_based/#after-many-episodes","title":"After Many Episodes","text":"<p>The Q-table converges to show the value of each action:</p> <pre><code>State (1,1) - one step from goal:\n  Up    = 7.9   (one step \u2192 goal, r=-1, then +10)\n  Right = 7.9   (one step \u2192 goal, r=-1, then +10)\n  Down  = -1.0  (moves away)\n  Left  = -1.0  (moves away)\n\nOptimal policy at (1,1): Up or Right (both lead to goal in 1 step)\n</code></pre> <p>The agent has learned that: - Actions toward the goal have high Q-values - Actions away from the goal have low Q-values - The optimal path is visible from the Q-values</p>"},{"location":"reinforcement_learning/4_value_based/#tabular-vs-function-approximation","title":"Tabular vs Function Approximation","text":""},{"location":"reinforcement_learning/4_value_based/#tabular-q-learning","title":"Tabular Q-Learning","text":"<p>Store Q-values in a table: one entry for each (state, action) pair.</p> <p>Pros: - Simple to implement - Guaranteed convergence (under conditions) - Easy to inspect and debug</p> <p>Cons: - Only works for discrete, small state/action spaces - Doesn't generalize: must visit every (s, a) pair - Memory explodes with large spaces</p> <p>When to use: Simple grid worlds, small discrete problems</p>"},{"location":"reinforcement_learning/4_value_based/#function-approximation-deep-q-networks","title":"Function Approximation (Deep Q-Networks)","text":"<p>Approximate Q-function with a parameterized function (e.g., neural network):</p> \\[ Q(s, a) \\approx Q_\\theta(s, a) \\] <p>Where \\( \\theta \\) are the parameters (weights) of the neural network.</p> <p>Update rule:</p> \\[ \\theta \\leftarrow \\theta + \\alpha \\left[r + \\gamma \\max_{a'} Q_\\theta(s', a') - Q_\\theta(s, a)\\right] \\nabla_\\theta Q_\\theta(s, a) \\] <p>Pros: - Handles large/continuous state spaces - Generalizes to unseen states - Can learn from images, sensors, etc.</p> <p>Cons: - Can be unstable and diverge - Requires careful tuning - Loss of convergence guarantees</p> <p>When to use: Complex state spaces (images, continuous sensors)</p>"},{"location":"reinforcement_learning/4_value_based/#deep-q-networks-dqn-a-brief-overview","title":"Deep Q-Networks (DQN): A Brief Overview","text":"<p>DQN (DeepMind, 2015) made Q-Learning work with neural networks by introducing two key innovations:</p>"},{"location":"reinforcement_learning/4_value_based/#1-experience-replay","title":"1. Experience Replay","text":"<p>Store experiences \\( (s, a, r, s') \\) in a replay buffer.</p> <p>Sample random mini-batches from the buffer for training.</p> <p>Why it helps: - Breaks correlation between consecutive samples - Reuses data multiple times (sample efficient) - Stabilizes training</p> <pre><code># Experience replay\nreplay_buffer = []\n\n# During interaction\nexperience = (s, a, r, s')\nreplay_buffer.append(experience)\n\n# During training\nbatch = random_sample(replay_buffer, batch_size)\nfor (s, a, r, s') in batch:\n    target = r + \u03b3 * max_a' Q(s', a')\n    loss = (Q(s, a) - target)\u00b2\n    Update Q-network using this loss\n</code></pre>"},{"location":"reinforcement_learning/4_value_based/#2-target-network","title":"2. Target Network","text":"<p>Maintain two networks: - Online network \\( Q_\\theta \\): Updated every step - Target network \\( Q_{\\theta^-} \\): Updated periodically (e.g., every 1000 steps)</p> <p>Use target network for computing TD target:</p> \\[ \\text{TD target} = r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') \\] <p>Why it helps: - Prevents the target from moving too quickly - Reduces harmful correlations - Stabilizes training</p> <pre><code># Target network update (every C steps)\nif step % C == 0:\n    target_network.weights = online_network.weights\n</code></pre>"},{"location":"reinforcement_learning/4_value_based/#simple-python-implementation","title":"Simple Python Implementation","text":"<p>Here's a minimal Q-Learning implementation for a discrete environment:</p> <pre><code>import numpy as np\n\nclass QLearningAgent:\n    def __init__(self, n_states, n_actions, learning_rate=0.1, \n                 discount=0.99, epsilon=1.0, epsilon_decay=0.995, \n                 epsilon_min=0.01):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.lr = learning_rate\n        self.gamma = discount\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n\n        # Initialize Q-table\n        self.Q = np.zeros((n_states, n_actions))\n\n    def select_action(self, state):\n        \"\"\"\u03b5-greedy action selection\"\"\"\n        if np.random.random() &lt; self.epsilon:\n            return np.random.randint(self.n_actions)  # Explore\n        else:\n            return np.argmax(self.Q[state])  # Exploit\n\n    def update(self, state, action, reward, next_state, done):\n        \"\"\"Q-Learning update\"\"\"\n        # Current Q-value\n        current_q = self.Q[state, action]\n\n        # TD target\n        if done:\n            target_q = reward  # No future value at terminal state\n        else:\n            target_q = reward + self.gamma * np.max(self.Q[next_state])\n\n        # Q-Learning update\n        td_error = target_q - current_q\n        self.Q[state, action] += self.lr * td_error\n\n    def decay_epsilon(self):\n        \"\"\"Decay exploration rate\"\"\"\n        self.epsilon = max(self.epsilon_min, \n                          self.epsilon * self.epsilon_decay)\n\n# Training loop\nagent = QLearningAgent(n_states=100, n_actions=4)\n\nfor episode in range(1000):\n    state = env.reset()\n    done = False\n    total_reward = 0\n\n    while not done:\n        # Select and take action\n        action = agent.select_action(state)\n        next_state, reward, done, _ = env.step(action)\n\n        # Update Q-values\n        agent.update(state, action, reward, next_state, done)\n\n        state = next_state\n        total_reward += reward\n\n    # Decay exploration\n    agent.decay_epsilon()\n\n    print(f\"Episode {episode}: Total Reward = {total_reward}, \u03b5 = {agent.epsilon:.3f}\")\n</code></pre>"},{"location":"reinforcement_learning/4_value_based/#hyperparameters-and-their-effects","title":"Hyperparameters and Their Effects","text":""},{"location":"reinforcement_learning/4_value_based/#learning-rate","title":"Learning Rate (\u03b1)","text":"<p>Controls how much we update Q-values each step.</p> \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha [TD\\_target - Q(s,a)] \\] <ul> <li>Too high (e.g., \u03b1 = 1.0): Forgets old information, unstable</li> <li>Too low (e.g., \u03b1 = 0.001): Learns very slowly</li> <li>Typical values: 0.001 to 0.1</li> </ul> <p>Schedule: Often decayed over time (fast learning early, refinement later)</p>"},{"location":"reinforcement_learning/4_value_based/#discount-factor","title":"Discount Factor (\u03b3)","text":"<p>Controls how much we value future rewards.</p> \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)] \\] <ul> <li>\u03b3 = 0: Only care about immediate reward (myopic)</li> <li>\u03b3 = 1: Care equally about all future rewards (far-sighted)</li> <li>Typical values: 0.95 to 0.99</li> </ul> <p>Effect: Higher \u03b3 considers longer-term consequences.</p>"},{"location":"reinforcement_learning/4_value_based/#exploration-rate","title":"Exploration Rate (\u03b5)","text":"<p>Controls exploration vs exploitation trade-off.</p> <ul> <li>High \u03b5 (e.g., 1.0): Random actions, lots of exploration</li> <li>Low \u03b5 (e.g., 0.01): Mostly greedy, little exploration</li> <li>Typical schedule: Start at 1.0, decay to 0.01</li> </ul> <p>Decay schedule: <pre><code>epsilon = max(epsilon_min, epsilon * decay_rate)  # Exponential decay\nepsilon = max(epsilon_min, epsilon - decay_step)  # Linear decay\n</code></pre></p>"},{"location":"reinforcement_learning/4_value_based/#convergence-and-guarantees","title":"Convergence and Guarantees","text":"<p>Q-Learning is proven to converge to the optimal Q-function under these conditions:</p> <ol> <li>All state-action pairs visited infinitely often</li> <li> <p>Ensured by sufficient exploration (\u03b5-greedy with \u03b5 &gt; 0)</p> </li> <li> <p>Learning rate schedule</p> </li> <li>Must satisfy: \\( \\sum_{t=1}^{\\infty} \\alpha_t = \\infty \\) and \\( \\sum_{t=1}^{\\infty} \\alpha_t^2 &lt; \\infty \\)</li> <li> <p>Example: \\( \\alpha_t = \\frac{1}{t} \\)</p> </li> <li> <p>Tabular representation</p> </li> <li>Each (s, a) has independent Q-value</li> <li>With function approximation, no guarantees!</li> </ol> <p>Practical note: In practice, we use constant learning rates and function approximation. While theoretical guarantees don't hold, Q-Learning often works well!</p>"},{"location":"reinforcement_learning/4_value_based/#q-learning-variants","title":"Q-Learning Variants","text":""},{"location":"reinforcement_learning/4_value_based/#sarsa-on-policy-q-learning","title":"SARSA (On-Policy Q-Learning)","text":"<p>Instead of using \\( \\max_{a'} Q(s', a') \\), use the action actually taken:</p> \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma Q(s', a') - Q(s, a)] \\] <p>Where \\( a' \\) is the action selected by the current policy.</p> <p>Difference: - Q-Learning: Off-policy (learns optimal policy while acting exploratively) - SARSA: On-policy (learns policy being followed, including exploration)</p> <p>When to use SARSA: When you want the learned policy to account for exploration risk (e.g., near cliffs).</p>"},{"location":"reinforcement_learning/4_value_based/#double-q-learning","title":"Double Q-Learning","text":"<p>Q-Learning can overestimate values due to maximization bias.</p> <p>Solution: Use two Q-functions, select action with one, evaluate with the other:</p> \\[ Q_1(s, a) \\leftarrow Q_1(s, a) + \\alpha [r + \\gamma Q_2(s', \\arg\\max_{a'} Q_1(s', a')) - Q_1(s, a)] \\] <p>Why it helps: Reduces overestimation, more stable learning.</p>"},{"location":"reinforcement_learning/4_value_based/#expected-sarsa","title":"Expected SARSA","text":"<p>Average over all possible next actions weighted by policy:</p> \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\sum_{a'} \\pi(a'|s') Q(s', a') - Q(s, a)] \\] <p>Benefits: Lower variance than SARSA, still considers exploration policy.</p>"},{"location":"reinforcement_learning/4_value_based/#limitations-of-q-learning","title":"Limitations of Q-Learning","text":"<p>Limited to discrete actions - Need to compute \\( \\arg\\max_a Q(s, a) \\) - Difficult or impossible with continuous actions</p> <p>Overestimation bias - Always taking max can lead to overly optimistic estimates - Mitigated by Double Q-Learning</p> <p>Sample inefficiency - Each update only affects one (s, a) pair (in tabular case) - Needs many samples to converge</p> <p>Instability with function approximation - Neural network Q-functions can diverge - Requires experience replay, target networks, careful tuning</p> <p>No explicit exploration strategy - Must add \u03b5-greedy or other heuristics - Not principled like entropy regularization in policy methods</p>"},{"location":"reinforcement_learning/4_value_based/#when-to-use-q-learning","title":"When to Use Q-Learning","text":""},{"location":"reinforcement_learning/4_value_based/#perfect-for","title":"Perfect for:","text":"<p>Discrete action spaces (games, navigation with discrete moves)</p> <p>Offline learning from logged data</p> <p>Simple environments where tabular methods suffice</p> <p>When you want off-policy learning</p>"},{"location":"reinforcement_learning/4_value_based/#avoid-for","title":"Avoid for:","text":"<p>Continuous action spaces (use policy gradient methods or actor-critic)</p> <p>Extremely large state spaces without good function approximation</p> <p>Real-time learning with strict sample budgets (consider policy gradients)</p>"},{"location":"reinforcement_learning/4_value_based/#summary","title":"Summary","text":"<p>Q-Learning in a nutshell:</p> <ol> <li>Learn Q-values: Estimate the value of each action in each state</li> <li>Bellman updates: Use immediate reward + value of best next action</li> <li>Act greedily: Choose action with highest Q-value</li> <li>Explore: Use \u03b5-greedy to discover better strategies</li> </ol> <p>Key equation: [ Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)] ]</p> <p>Strengths: Simple, off-policy, proven convergence (tabular case)</p> <p>Weaknesses: Discrete actions only, can be sample inefficient, requires careful exploration</p> <p>Modern extensions: DQN, Double DQN, Dueling DQN, Rainbow DQN</p> <p>Now let's move to the other side: policy gradient methods that directly learn the policy!</p>"},{"location":"reinforcement_learning/4_value_based/#check-your-understanding","title":"Check your understanding","text":"<p>Quiz 4</p> <p>\u2190 Back to Policy vs Value-Based Continue to Policy Gradients (REINFORCE) \u2192</p>"},{"location":"reinforcement_learning/5_policy_based/","title":"Policy Gradients: REINFORCE Algorithm","text":""},{"location":"reinforcement_learning/5_policy_based/#from-values-to-policies","title":"From Values to Policies","text":"<p>In Q-Learning, we learned to estimate the value of actions and then derived a policy from those values. But what if we directly learn the policy instead?</p> <p>Policy gradient methods do exactly this: they parameterize the policy and optimize it using gradient ascent to maximize expected return.</p>"},{"location":"reinforcement_learning/5_policy_based/#the-policy-gradient-approach","title":"The Policy Gradient Approach","text":""},{"location":"reinforcement_learning/5_policy_based/#parameterized-policy","title":"Parameterized Policy","text":"<p>Instead of learning Q-values, we directly parameterize the policy with parameters \\( \\theta \\):</p> \\[ \\pi_\\theta(a | s) \\] <p>Examples: - Neural network: Input state, output action probabilities - Linear model: \\( \\pi_\\theta(a|s) = \\text{softmax}(\\theta^T \\phi(s)) \\) - Gaussian policy: \\( \\pi_\\theta(a|s) = \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s)) \\)</p>"},{"location":"reinforcement_learning/5_policy_based/#the-objective","title":"The Objective","text":"<p>We want to find parameters \\( \\theta \\) that maximize expected return:</p> \\[ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [G(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t \\right] \\] <p>Where \\( \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, ...) \\) is a trajectory sampled by following policy \\( \\pi_\\theta \\).</p> <p>Intuition: We're directly optimizing what we care about\u2014the total reward!</p>"},{"location":"reinforcement_learning/5_policy_based/#gradient-ascent","title":"Gradient Ascent","text":"<p>To maximize \\( J(\\theta) \\), we use gradient ascent:</p> \\[ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta) \\] <p>Move parameters in the direction that increases expected return.</p> <p>The challenge: How do we compute \\( \\nabla_\\theta J(\\theta) \\)?</p>"},{"location":"reinforcement_learning/5_policy_based/#the-policy-gradient-theorem","title":"The Policy Gradient Theorem","text":"<p>The policy gradient theorem gives us a practical way to compute the gradient:</p> \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) G_t \\right] \\] <p>Where \\( G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k \\) is the return from time \\( t \\).</p> <p>What this means: - Sample trajectories using current policy \\( \\pi_\\theta \\) - For each action taken, compute its log-probability gradient - Weight by the return obtained from that point onward - Average over many trajectories</p> <p>Intuitive interpretation: - If action \\( a_t \\) led to high return \\( G_t \\): increase \\( \\pi_\\theta(a_t | s_t) \\) - If action \\( a_t \\) led to low return \\( G_t \\): decrease \\( \\pi_\\theta(a_t | s_t) \\)</p>"},{"location":"reinforcement_learning/5_policy_based/#reinforce-the-monte-carlo-policy-gradient-algorithm","title":"REINFORCE: The Monte Carlo Policy Gradient Algorithm","text":"<p>REINFORCE (Williams, 1992) is the classic policy gradient algorithm. It's beautifully simple!</p>"},{"location":"reinforcement_learning/5_policy_based/#the-algorithm","title":"The Algorithm","text":"<pre><code>Initialize policy parameters \u03b8 randomly\n\nFor each episode:\n    1. Generate trajectory \u03c4 = (s\u2080, a\u2080, r\u2081, s\u2081, ..., sT)\n       by following \u03c0\u03b8\n\n    2. For each timestep t in the episode:\n        Compute return: Gt = \u03a3(k=t to T) \u03b3^(k-t) * r_k\n\n        Compute gradient: \u2207\u03b8 log \u03c0\u03b8(at | st)\n\n        Update: \u03b8 \u2190 \u03b8 + \u03b1 * Gt * \u2207\u03b8 log \u03c0\u03b8(at | st)\n</code></pre>"},{"location":"reinforcement_learning/5_policy_based/#why-this-works-intuition","title":"Why This Works: Intuition","text":"<p>Imagine training a robot to navigate:</p> <p>Episode 1: Robot wanders randomly, finds goal by luck - Total return: G = 50 (pretty good!) - Actions taken are reinforced (made more likely)</p> <p>Episode 2: Robot goes wrong direction, fails - Total return: G = -10 (bad!) - Actions taken are suppressed (made less likely)</p> <p>Over many episodes, good actions become more probable, bad actions less probable.</p>"},{"location":"reinforcement_learning/5_policy_based/#the-log-probability-trick","title":"The Log-Probability Trick","text":"<p>Why do we use \\( \\nabla_\\theta \\log \\pi_\\theta(a|s) \\) instead of \\( \\nabla_\\theta \\pi_\\theta(a|s) \\)?</p> <p>Mathematical reason: The policy gradient theorem gives us this form.</p> <p>Practical reason: It's easier to compute!</p> <p>For a softmax policy: [ \\pi_\\theta(a|s) = \\frac{e^{\\theta^T \\phi(s, a)}}{\\sum_{a'} e^{\\theta^T \\phi(s, a')}} ]</p> \\[ \\nabla_\\theta \\log \\pi_\\theta(a|s) = \\phi(s, a) - \\mathbb{E}_{a' \\sim \\pi_\\theta}[\\phi(s, a')] \\] <p>The gradient is just the feature vector minus its expectation!</p> <p>For neural networks with automatic differentiation (PyTorch, TensorFlow), computing log-probability gradients is straightforward.</p>"},{"location":"reinforcement_learning/5_policy_based/#reinforce-with-baseline","title":"REINFORCE with Baseline","text":""},{"location":"reinforcement_learning/5_policy_based/#the-high-variance-problem","title":"The High Variance Problem","text":"<p>Pure REINFORCE has a critical issue: very high variance in gradient estimates.</p> <p>Why? Returns \\( G_t \\) can vary wildly between episodes, even for similar actions.</p> <p>Example: <pre><code>Episode 1: Gt = 100  \u2192 Big update!\nEpisode 2: Gt = 95   \u2192 Big update in slightly different direction\nEpisode 3: Gt = 5    \u2192 Big negative update\nEpisode 4: Gt = 90   \u2192 Big positive update again\n</code></pre></p> <p>The agent gets conflicting signals, making learning slow and unstable.</p>"},{"location":"reinforcement_learning/5_policy_based/#introducing-a-baseline","title":"Introducing a Baseline","text":"<p>We can subtract a baseline \\( b(s_t) \\) from the return without changing the expected gradient:</p> \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) (G_t - b(s_t)) \\right] \\] <p>Why this doesn't introduce bias: The expectation of \\( \\nabla_\\theta \\log \\pi_\\theta(a|s) \\) is zero, so adding/subtracting baselines doesn't change the expected gradient.</p> <p>Why it reduces variance: By centering the returns around zero, we reduce the magnitude of gradient updates.</p>"},{"location":"reinforcement_learning/5_policy_based/#choosing-a-baseline","title":"Choosing a Baseline","text":"<p>Best baseline: The state-value function \\( V(s_t) \\)!</p> \\[ G_t - V(s_t) = A_t \\] <p>This is called the advantage \\( A_t \\): how much better was this action compared to the average?</p> <p>Interpretation: - \\( A_t &gt; 0 \\): Action was better than expected \u2192 increase probability - \\( A_t &lt; 0 \\): Action was worse than expected \u2192 decrease probability - \\( A_t = 0 \\): Action was as expected \u2192 no change</p> <p>Practical implementation: Learn \\( V(s) \\) using a separate value network (this leads to Actor-Critic methods!)</p>"},{"location":"reinforcement_learning/5_policy_based/#reinforce-with-baseline-algorithm","title":"REINFORCE with Baseline Algorithm","text":"<pre><code>Initialize policy parameters \u03b8 and value function parameters w\n\nFor each episode:\n    Generate trajectory \u03c4 = (s\u2080, a\u2080, r\u2081, s\u2081, ..., sT) using \u03c0\u03b8\n\n    For each timestep t:\n        Compute return: Gt = \u03a3(k=t to T) \u03b3^(k-t) * r_k\n\n        Compute advantage: At = Gt - V_w(st)\n\n        Update policy: \u03b8 \u2190 \u03b8 + \u03b1 * At * \u2207\u03b8 log \u03c0\u03b8(at | st)\n\n        Update value: w \u2190 w + \u03b2 * (Gt - V_w(st)) * \u2207w V_w(st)\n</code></pre>"},{"location":"reinforcement_learning/5_policy_based/#policy-representations","title":"Policy Representations","text":""},{"location":"reinforcement_learning/5_policy_based/#discrete-actions-softmax-policy","title":"Discrete Actions: Softmax Policy","text":"<p>For discrete action spaces, use a softmax (categorical) policy:</p> \\[ \\pi_\\theta(a | s) = \\frac{e^{f_\\theta(s, a)}}{\\sum_{a'} e^{f_\\theta(s, a')}} \\] <p>Where \\( f_\\theta(s, a) \\) is a neural network that outputs action logits.</p> <p>Network architecture: <pre><code>State s \u2192 [Neural Network] \u2192 [Logits for each action] \u2192 Softmax \u2192 Probabilities\n</code></pre></p> <p>Sampling: <pre><code>logits = policy_network(state)\naction_probs = softmax(logits)\naction = categorical_sample(action_probs)\n</code></pre></p> <p>Log-probability: <pre><code>log_prob = log_softmax(logits)[action]\n</code></pre></p>"},{"location":"reinforcement_learning/5_policy_based/#continuous-actions-gaussian-policy","title":"Continuous Actions: Gaussian Policy","text":"<p>For continuous action spaces, use a Gaussian policy:</p> \\[ \\pi_\\theta(a | s) = \\mathcal{N}(a | \\mu_\\theta(s), \\sigma_\\theta(s)) \\] <p>Where the neural network outputs mean \\( \\mu_\\theta(s) \\) and standard deviation \\( \\sigma_\\theta(s) \\).</p> <p>Network architecture: <pre><code>State s \u2192 [Neural Network] \u2192 [\u03bc\u2081, \u03bc\u2082, ..., \u03bcn, \u03c3\u2081, \u03c3\u2082, ..., \u03c3n]\n</code></pre></p> <p>Sampling: <pre><code>mu, sigma = policy_network(state)\naction = mu + sigma * random_normal()\n</code></pre></p> <p>Log-probability: <pre><code>log_prob = -0.5 * ((action - mu) / sigma)\u00b2 - log(sigma) - 0.5 * log(2\u03c0)\n</code></pre></p> <p>Practical tip: Often use a diagonal Gaussian (independent dimensions) for simplicity.</p>"},{"location":"reinforcement_learning/5_policy_based/#simple-python-implementation","title":"Simple Python Implementation","text":"<p>Here's a minimal REINFORCE implementation using PyTorch:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n\n    def forward(self, state):\n        logits = self.network(state)\n        return logits\n\nclass REINFORCE:\n    def __init__(self, state_dim, action_dim, learning_rate=1e-3, gamma=0.99):\n        self.policy = PolicyNetwork(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n        self.gamma = gamma\n\n    def select_action(self, state):\n        \"\"\"Select action and compute log probability\"\"\"\n        state = torch.FloatTensor(state)\n        logits = self.policy(state)\n\n        # Sample from categorical distribution\n        dist = Categorical(logits=logits)\n        action = dist.sample()\n        log_prob = dist.log_prob(action)\n\n        return action.item(), log_prob\n\n    def compute_returns(self, rewards):\n        \"\"\"Compute discounted returns for each timestep\"\"\"\n        returns = []\n        G = 0\n        for r in reversed(rewards):\n            G = r + self.gamma * G\n            returns.insert(0, G)\n\n        returns = torch.FloatTensor(returns)\n        # Normalize returns for stability\n        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n        return returns\n\n    def update(self, log_probs, rewards):\n        \"\"\"Update policy using REINFORCE\"\"\"\n        # Compute returns\n        returns = self.compute_returns(rewards)\n\n        # Compute policy gradient loss\n        policy_loss = []\n        for log_prob, G in zip(log_probs, returns):\n            policy_loss.append(-log_prob * G)\n\n        # Perform gradient ascent\n        self.optimizer.zero_grad()\n        policy_loss = torch.stack(policy_loss).sum()\n        policy_loss.backward()\n        self.optimizer.step()\n\n# Training loop\nagent = REINFORCE(state_dim=4, action_dim=2)\n\nfor episode in range(1000):\n    state = env.reset()\n    log_probs = []\n    rewards = []\n    done = False\n\n    # Collect trajectory\n    while not done:\n        action, log_prob = agent.select_action(state)\n        next_state, reward, done, _ = env.step(action)\n\n        log_probs.append(log_prob)\n        rewards.append(reward)\n        state = next_state\n\n    # Update policy\n    agent.update(log_probs, rewards)\n\n    print(f\"Episode {episode}: Total Reward = {sum(rewards)}\")\n</code></pre>"},{"location":"reinforcement_learning/5_policy_based/#reinforce-with-baseline-implementation","title":"REINFORCE with Baseline Implementation","text":"<p>Adding a value function baseline:</p> <pre><code>class ValueNetwork(nn.Module):\n    def __init__(self, state_dim, hidden_dim=128):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, state):\n        return self.network(state).squeeze()\n\nclass REINFORCEWithBaseline:\n    def __init__(self, state_dim, action_dim, lr_policy=1e-3, lr_value=1e-3, gamma=0.99):\n        self.policy = PolicyNetwork(state_dim, action_dim)\n        self.value = ValueNetwork(state_dim)\n\n        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr_policy)\n        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr_value)\n        self.gamma = gamma\n\n    def update(self, states, log_probs, rewards):\n        \"\"\"Update policy and value function\"\"\"\n        # Compute returns\n        returns = self.compute_returns(rewards)\n\n        states = torch.FloatTensor(states)\n        returns = torch.FloatTensor(returns)\n\n        # Compute value predictions\n        values = self.value(states)\n\n        # Compute advantages\n        advantages = returns - values.detach()  # Don't backprop through value for policy\n\n        # Update policy\n        policy_loss = []\n        for log_prob, advantage in zip(log_probs, advantages):\n            policy_loss.append(-log_prob * advantage)\n\n        self.policy_optimizer.zero_grad()\n        policy_loss = torch.stack(policy_loss).sum()\n        policy_loss.backward()\n        self.policy_optimizer.step()\n\n        # Update value function (minimize MSE)\n        value_loss = ((values - returns) ** 2).mean()\n\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        self.value_optimizer.step()\n</code></pre>"},{"location":"reinforcement_learning/5_policy_based/#advantages-of-policy-gradients","title":"Advantages of Policy Gradients","text":"<p>Continuous actions: Naturally handles continuous action spaces</p> <p>Stochastic policies: Can learn optimal stochastic policies</p> <p>Convergence: Better convergence properties than value-based methods</p> <p>High-dimensional actions: Doesn't need argmax over action space</p> <p>Stable: Generally more stable than Q-learning with function approximation</p>"},{"location":"reinforcement_learning/5_policy_based/#disadvantages-of-policy-gradients","title":"Disadvantages of Policy Gradients","text":"<p>Sample inefficient: Requires many episodes to estimate gradients accurately</p> <p>High variance: Gradient estimates can be very noisy</p> <p>On-policy: Must collect new data after each policy update (expensive!)</p> <p>Local optima: Gradient ascent can get stuck in local maxima</p> <p>Slow convergence: Can take many iterations to learn</p>"},{"location":"reinforcement_learning/5_policy_based/#variance-reduction-techniques","title":"Variance Reduction Techniques","text":"<p>Beyond baselines, several techniques reduce gradient variance:</p>"},{"location":"reinforcement_learning/5_policy_based/#1-reward-to-go","title":"1. Reward-to-Go","text":"<p>Instead of using full episode return \\( G_0 \\), use reward-to-go \\( G_t \\):</p> \\[ G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k \\] <p>Why it helps: Action at time \\( t \\) doesn't affect rewards before time \\( t \\), so we shouldn't credit it for those rewards.</p>"},{"location":"reinforcement_learning/5_policy_based/#2-generalized-advantage-estimation-gae","title":"2. Generalized Advantage Estimation (GAE)","text":"<p>Combine multiple \\( n \\)-step returns with exponential weighting:</p> \\[ A_t^{\\text{GAE}} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l} \\] <p>Where \\( \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) \\) is the TD error.</p> <p>Parameter \\( \\lambda \\): Controls bias-variance tradeoff - \\( \\lambda = 0 \\): Low variance, high bias (like TD) - \\( \\lambda = 1 \\): High variance, low bias (like Monte Carlo)</p>"},{"location":"reinforcement_learning/5_policy_based/#3-entropy-regularization","title":"3. Entropy Regularization","text":"<p>Add entropy bonus to encourage exploration:</p> \\[ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau)] + \\beta \\mathcal{H}(\\pi_\\theta) \\] <p>Where \\( \\mathcal{H}(\\pi_\\theta) = -\\sum_a \\pi_\\theta(a|s) \\log \\pi_\\theta(a|s) \\) is the policy entropy.</p> <p>Effect: Prevents premature convergence to deterministic policy, encourages exploration.</p>"},{"location":"reinforcement_learning/5_policy_based/#modern-policy-gradient-methods","title":"Modern Policy Gradient Methods","text":"<p>REINFORCE is the foundation, but modern methods add improvements:</p>"},{"location":"reinforcement_learning/5_policy_based/#ppo-proximal-policy-optimization","title":"PPO (Proximal Policy Optimization)","text":"<p>Key idea: Limit how much the policy can change in one update.</p> <p>Clipped objective: [ L(\\theta) = \\mathbb{E} \\left[ \\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t) \\right] ]</p> <p>Where \\( r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)} \\) is the probability ratio.</p> <p>Why it works: Prevents destructively large updates, more stable training.</p>"},{"location":"reinforcement_learning/5_policy_based/#trpo-trust-region-policy-optimization","title":"TRPO (Trust Region Policy Optimization)","text":"<p>Key idea: Constrain policy updates to stay within a \"trust region.\"</p> <p>Constraint: [ \\mathbb{E}[D_{KL}(\\pi_{\\theta_{\\text{old}}} || \\pi_\\theta)] \\leq \\delta ]</p> <p>Ensures new policy isn't too different from old policy.</p> <p>Why it works: Guarantees monotonic improvement, very stable.</p>"},{"location":"reinforcement_learning/5_policy_based/#a3ca2c-advantage-actor-critic","title":"A3C/A2C (Advantage Actor-Critic)","text":"<p>Key idea: Use value function as baseline, update after every step (not full episodes).</p> <p>Benefits: - Lower variance (value function baseline) - More sample efficient (use bootstrapping like TD) - Can train in parallel (A3C)</p> <p>We'll cover actor-critic methods in detail in the next section!</p>"},{"location":"reinforcement_learning/5_policy_based/#comparing-reinforce-to-q-learning","title":"Comparing REINFORCE to Q-Learning","text":"Aspect Q-Learning REINFORCE What we learn Q(s, a) values Policy \u03c0\u03b8 directly Update frequency Every step Every episode Sample efficiency More efficient Less efficient Action space Discrete only Discrete or continuous Variance Lower Higher Bias Biased (with function approx) Unbiased Convergence Can diverge More stable Off-policy Yes No (by default) <p>When to use REINFORCE over Q-Learning: - Continuous action spaces (robot control) - Stochastic policies needed - Stability more important than sample efficiency</p>"},{"location":"reinforcement_learning/5_policy_based/#practical-tips-for-training","title":"Practical Tips for Training","text":""},{"location":"reinforcement_learning/5_policy_based/#1-normalize-returns","title":"1. Normalize Returns","text":"<pre><code>returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n</code></pre> <p>Helps stabilize training by keeping gradients in reasonable range.</p>"},{"location":"reinforcement_learning/5_policy_based/#2-gradient-clipping","title":"2. Gradient Clipping","text":"<pre><code>torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n</code></pre> <p>Prevents exploding gradients.</p>"},{"location":"reinforcement_learning/5_policy_based/#3-learning-rate-scheduling","title":"3. Learning Rate Scheduling","text":"<p>Start with higher learning rate, decay over time: <pre><code>scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n</code></pre></p>"},{"location":"reinforcement_learning/5_policy_based/#4-early-stopping","title":"4. Early Stopping","text":"<p>Monitor validation performance and stop if not improving.</p>"},{"location":"reinforcement_learning/5_policy_based/#5-multiple-random-seeds","title":"5. Multiple Random Seeds","text":"<p>Train with different random seeds, report mean and std of performance.</p>"},{"location":"reinforcement_learning/5_policy_based/#summary","title":"Summary","text":"<p>REINFORCE in a nutshell:</p> <ol> <li>Sample trajectories: Follow current policy to collect episodes</li> <li>Compute returns: Calculate discounted reward-to-go for each timestep  </li> <li>Compute advantages: Subtract baseline (optional but recommended)</li> <li>Update policy: Gradient ascent weighted by advantages</li> </ol> <p>Key equation: [ \\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) (G_t - b(s_t)) ]</p> <p>Strengths: Simple, direct optimization, handles continuous actions, stable convergence</p> <p>Weaknesses: Sample inefficient, high variance, on-policy</p> <p>Modern extensions: PPO, TRPO, A3C (address sample efficiency and variance)</p> <p>Now let's see REINFORCE in action with a hands-on implementation!</p>"},{"location":"reinforcement_learning/5_policy_based/#check-your-understanding","title":"Check your understanding","text":"<p>Quiz 5</p> <p>\u2190 Back to Q-Learning Continue to Practical Tutorial \u2192</p>"},{"location":"reinforcement_learning/5_quiz/","title":"Quiz 5","text":"Submit Quiz"},{"location":"reinforcement_learning/5_quiz/#question-1","title":"Question 1","text":"<p> <p>What is the core idea behind REINFORCE and policy gradient methods?</p> <ul><li> <p>Learn Q-values for each state-action pair, then derive a policy</p></li><li> <p>Directly learn and optimize the policy parameters to maximize expected return</p></li><li> <p>Learn a value function to estimate state values</p></li><li> <p>Use dynamic programming to compute optimal policies</p></li></ul> </p>"},{"location":"reinforcement_learning/5_quiz/#question-2","title":"Question 2","text":"<p> <p>Why does REINFORCE use the log-probability gradient \u2207_\u03b8 log \u03c0_\u03b8(a|s) instead of the probability gradient \u2207_\u03b8 \u03c0_\u03b8(a|s)?</p> <ul><li> <p>Log-probability gradients are always larger, making learning faster</p></li><li> <p>The policy gradient theorem requires this form, and it's easier to compute for neural networks</p></li><li> <p>Log-probability prevents the policy from becoming deterministic</p></li><li> <p>It's only used for discrete action spaces, not continuous ones</p></li></ul> </p>"},{"location":"reinforcement_learning/5_quiz/#question-3","title":"Question 3","text":"<p> <p>What is the main purpose of using a baseline (like V(s_t)) in REINFORCE?</p> <ul><li> <p>To make the algorithm converge faster</p></li><li> <p>To reduce the variance of gradient estimates without introducing bias</p></li><li> <p>To ensure the policy always improves</p></li><li> <p>To make the algorithm work with continuous actions</p></li></ul> </p>"},{"location":"reinforcement_learning/5_quiz/#question-4","title":"Question 4","text":"<p> <p>What is a key advantage of policy gradient methods like REINFORCE over value-based methods like Q-Learning?</p> <ul><li> <p>They are always more sample efficient</p></li><li> <p>They naturally handle continuous action spaces without needing to maximize over infinite actions</p></li><li> <p>They always converge faster</p></li><li> <p>They don't require any exploration strategy</p></li></ul> </p>"},{"location":"reinforcement_learning/6_practical_tutorial/","title":"Putting It All Together: Practical RL Implementation","text":""},{"location":"reinforcement_learning/6_practical_tutorial/#overview","title":"Overview","text":"<p>In this tutorial, we'll implement REINFORCE from scratch and compare it with PPO from Stable-Baselines3 (SB3). You'll see:</p> <ol> <li>How to implement REINFORCE algorithm step-by-step</li> <li>How to use Gymnasium (OpenAI Gym) environments</li> <li>How to train and evaluate your agent</li> <li>How your implementation compares to state-of-the-art (PPO)</li> <li>Practical tips for debugging and improving performance</li> </ol>"},{"location":"reinforcement_learning/6_practical_tutorial/#environment-cartpole-v1","title":"Environment: CartPole-v1","text":"<p>We'll use CartPole-v1, a classic control problem:</p> <p>Goal: Balance a pole on a moving cart by applying left/right forces.</p> <p>Observations:  - Cart position: \\([-4.8, 4.8]\\) - Cart velocity: \\([-\\infty, \\infty]\\) - Pole angle: \\([-0.418, 0.418]\\) rad (\u224824\u00b0) - Pole angular velocity: \\([-\\infty, \\infty]\\)</p> <p>Actions: - 0: Push cart to the left - 1: Push cart to the right</p> <p>Rewards: - +1 for every timestep the pole remains upright - Episode ends when pole falls &gt; 12\u00b0 or cart moves &gt; 2.4 units from center</p> <p>Success criterion: Average reward of 475 over 100 episodes (max is 500)</p>"},{"location":"reinforcement_learning/6_practical_tutorial/#part-1-reinforce-from-scratch","title":"Part 1: REINFORCE from Scratch","text":""},{"location":"reinforcement_learning/6_practical_tutorial/#step-1-setup-and-imports","title":"Step 1: Setup and Imports","text":"<pre><code>import gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions import Categorical\nimport matplotlib.pyplot as plt\nfrom collections import deque\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#step-2-policy-network","title":"Step 2: Policy Network","text":"<pre><code>class PolicyNetwork(nn.Module):\n    \"\"\"\n    Neural network that outputs action probabilities\n    \"\"\"\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super(PolicyNetwork, self).__init__()\n\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n\n    def forward(self, state):\n        \"\"\"\n        Forward pass: state -&gt; action logits\n        \"\"\"\n        return self.network(state)\n\n    def get_action(self, state):\n        \"\"\"\n        Sample action from policy and return log probability\n        \"\"\"\n        state = torch.FloatTensor(state).unsqueeze(0)\n        logits = self.forward(state)\n\n        # Create categorical distribution\n        dist = Categorical(logits=logits)\n\n        # Sample action\n        action = dist.sample()\n\n        # Get log probability for policy gradient\n        log_prob = dist.log_prob(action)\n\n        return action.item(), log_prob\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#step-3-reinforce-agent","title":"Step 3: REINFORCE Agent","text":"<pre><code>class REINFORCEAgent:\n    \"\"\"\n    REINFORCE algorithm implementation\n    \"\"\"\n    def __init__(self, state_dim, action_dim, learning_rate=1e-3, gamma=0.99):\n        self.gamma = gamma\n\n        # Initialize policy network\n        self.policy = PolicyNetwork(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n\n        # Storage for episode data\n        self.reset_episode()\n\n    def reset_episode(self):\n        \"\"\"Clear episode storage\"\"\"\n        self.log_probs = []\n        self.rewards = []\n\n    def select_action(self, state):\n        \"\"\"Select action using current policy\"\"\"\n        action, log_prob = self.policy.get_action(state)\n        self.log_probs.append(log_prob)\n        return action\n\n    def store_reward(self, reward):\n        \"\"\"Store reward for current timestep\"\"\"\n        self.rewards.append(reward)\n\n    def compute_returns(self):\n        \"\"\"\n        Compute discounted returns (reward-to-go) for each timestep\n        G_t = r_t + \u03b3*r_{t+1} + \u03b3\u00b2*r_{t+2} + ...\n        \"\"\"\n        returns = []\n        G = 0\n\n        # Compute returns in reverse order\n        for r in reversed(self.rewards):\n            G = r + self.gamma * G\n            returns.insert(0, G)\n\n        # Convert to tensor\n        returns = torch.FloatTensor(returns)\n\n        # Normalize returns for stability (optional but recommended)\n        if len(returns) &gt; 1:\n            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n\n        return returns\n\n    def update_policy(self):\n        \"\"\"\n        Update policy using REINFORCE algorithm\n        \"\"\"\n        # Compute returns\n        returns = self.compute_returns()\n\n        # Compute policy gradient loss\n        policy_loss = []\n        for log_prob, G in zip(self.log_probs, returns):\n            # Negative because we're doing gradient ascent\n            policy_loss.append(-log_prob * G)\n\n        # Sum all losses\n        policy_loss = torch.stack(policy_loss).sum()\n\n        # Perform gradient ascent step\n        self.optimizer.zero_grad()\n        policy_loss.backward()\n\n        # Gradient clipping for stability\n        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=1.0)\n\n        self.optimizer.step()\n\n        # Clear episode data\n        self.reset_episode()\n\n        return policy_loss.item()\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#step-4-training-loop","title":"Step 4: Training Loop","text":"<pre><code>def train_reinforce(env_name='CartPole-v1', \n                   num_episodes=1000, \n                   learning_rate=1e-2,\n                   gamma=0.99,\n                   print_every=50):\n    \"\"\"\n    Train REINFORCE agent\n    \"\"\"\n    # Create environment\n    env = gym.make(env_name)\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n\n    # Create agent\n    agent = REINFORCEAgent(state_dim, action_dim, learning_rate, gamma)\n\n    # Training tracking\n    episode_rewards = []\n    episode_lengths = []\n    running_reward = deque(maxlen=100)\n\n    print(f\"Training REINFORCE on {env_name}\")\n    print(f\"State dim: {state_dim}, Action dim: {action_dim}\")\n    print(\"-\" * 50)\n\n    for episode in range(num_episodes):\n        state, _ = env.reset()\n        episode_reward = 0\n        done = False\n        steps = 0\n\n        # Collect one episode\n        while not done:\n            # Select action\n            action = agent.select_action(state)\n\n            # Take action in environment\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n\n            # Store reward\n            agent.store_reward(reward)\n\n            state = next_state\n            episode_reward += reward\n            steps += 1\n\n        # Update policy after episode\n        loss = agent.update_policy()\n\n        # Track statistics\n        episode_rewards.append(episode_reward)\n        episode_lengths.append(steps)\n        running_reward.append(episode_reward)\n\n        # Print progress\n        if (episode + 1) % print_every == 0:\n            avg_reward = np.mean(running_reward)\n            print(f\"Episode {episode + 1}/{num_episodes} | \"\n                  f\"Avg Reward: {avg_reward:.2f} | \"\n                  f\"Last Reward: {episode_reward:.2f} | \"\n                  f\"Loss: {loss:.4f}\")\n\n            # Check if solved\n            if avg_reward &gt;= 475.0:\n                print(f\"\\n\ud83c\udf89 Solved in {episode + 1} episodes! \"\n                      f\"Average reward: {avg_reward:.2f}\")\n                break\n\n    env.close()\n\n    return agent, episode_rewards, episode_lengths\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#step-5-evaluation-function","title":"Step 5: Evaluation Function","text":"<pre><code>def evaluate_agent(agent, env_name='CartPole-v1', num_episodes=100, render=False):\n    \"\"\"\n    Evaluate trained agent\n    \"\"\"\n    if render:\n        env = gym.make(env_name, render_mode='human')\n    else:\n        env = gym.make(env_name)\n\n    episode_rewards = []\n\n    for episode in range(num_episodes):\n        state, _ = env.reset()\n        episode_reward = 0\n        done = False\n\n        while not done:\n            # Select action (greedy, no exploration)\n            with torch.no_grad():\n                action, _ = agent.policy.get_action(state)\n\n            state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            episode_reward += reward\n\n        episode_rewards.append(episode_reward)\n\n    env.close()\n\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    print(f\"\\nEvaluation over {num_episodes} episodes:\")\n    print(f\"Mean reward: {mean_reward:.2f} \u00b1 {std_reward:.2f}\")\n\n    return episode_rewards\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#step-6-visualization","title":"Step 6: Visualization","text":"<pre><code>def plot_training_progress(rewards, window=100):\n    \"\"\"\n    Plot training progress\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Plot raw rewards\n    ax1.plot(rewards, alpha=0.3, label='Raw')\n\n    # Plot moving average\n    if len(rewards) &gt;= window:\n        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n        ax1.plot(range(window-1, len(rewards)), moving_avg, \n                label=f'{window}-episode moving average')\n\n    ax1.axhline(y=475, color='r', linestyle='--', label='Solved threshold')\n    ax1.set_xlabel('Episode')\n    ax1.set_ylabel('Reward')\n    ax1.set_title('Training Progress')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # Plot distribution of recent rewards\n    recent_rewards = rewards[-100:] if len(rewards) &gt; 100 else rewards\n    ax2.hist(recent_rewards, bins=20, edgecolor='black')\n    ax2.axvline(x=np.mean(recent_rewards), color='r', linestyle='--', \n               label=f'Mean: {np.mean(recent_rewards):.2f}')\n    ax2.set_xlabel('Reward')\n    ax2.set_ylabel('Frequency')\n    ax2.set_title('Recent Reward Distribution')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#step-7-run-training","title":"Step 7: Run Training","text":"<pre><code># Train the agent\nagent, rewards, lengths = train_reinforce(\n    env_name='CartPole-v1',\n    num_episodes=1000,\n    learning_rate=1e-2,\n    gamma=0.99\n)\n\n# Plot results\nplot_training_progress(rewards)\n\n# Evaluate the trained agent\neval_rewards = evaluate_agent(agent, num_episodes=100)\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#part-2-ppo-from-stable-baselines3","title":"Part 2: PPO from Stable-Baselines3","text":"<p>Now let's compare with PPO, a state-of-the-art algorithm:</p>"},{"location":"reinforcement_learning/6_practical_tutorial/#setup","title":"Setup","text":"<pre><code>from stable_baselines3 import PPO\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.callbacks import EvalCallback\nimport time\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#train-ppo","title":"Train PPO","text":"<pre><code>def train_ppo(env_name='CartPole-v1', \n              total_timesteps=100000,\n              learning_rate=3e-4):\n    \"\"\"\n    Train PPO agent using Stable-Baselines3\n    \"\"\"\n    print(f\"\\nTraining PPO on {env_name}\")\n    print(\"-\" * 50)\n\n    # Create environment\n    env = gym.make(env_name)\n\n    # Create PPO agent\n    model = PPO(\n        'MlpPolicy',  # Multi-layer perceptron policy\n        env,\n        learning_rate=learning_rate,\n        n_steps=2048,  # Collect 2048 steps before each update\n        batch_size=64,\n        n_epochs=10,  # Number of epochs per update\n        gamma=0.99,\n        gae_lambda=0.95,  # For advantage estimation\n        clip_range=0.2,  # PPO clipping parameter\n        verbose=1\n    )\n\n    # Train\n    start_time = time.time()\n    model.learn(total_timesteps=total_timesteps)\n    training_time = time.time() - start_time\n\n    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n\n    return model\n\n# Train PPO\nppo_model = train_ppo(total_timesteps=100000)\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#evaluate-ppo","title":"Evaluate PPO","text":"<pre><code># Evaluate PPO\nenv = gym.make('CartPole-v1')\nmean_reward, std_reward = evaluate_policy(\n    ppo_model, \n    env, \n    n_eval_episodes=100\n)\n\nprint(f\"\\nPPO Evaluation over 100 episodes:\")\nprint(f\"Mean reward: {mean_reward:.2f} \u00b1 {std_reward:.2f}\")\n\nenv.close()\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#part-3-detailed-comparison","title":"Part 3: Detailed Comparison","text":""},{"location":"reinforcement_learning/6_practical_tutorial/#comparison-function","title":"Comparison Function","text":"<pre><code>def compare_algorithms():\n    \"\"\"\n    Train both algorithms and compare\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"REINFORCE vs PPO Comparison on CartPole-v1\")\n    print(\"=\" * 60)\n\n    # Train REINFORCE\n    print(\"\\n1. Training REINFORCE...\")\n    reinforce_start = time.time()\n    reinforce_agent, reinforce_rewards, _ = train_reinforce(\n        num_episodes=1000,\n        learning_rate=1e-2,\n        print_every=100\n    )\n    reinforce_time = time.time() - reinforce_start\n\n    # Evaluate REINFORCE\n    reinforce_eval = evaluate_agent(reinforce_agent, num_episodes=100)\n    reinforce_mean = np.mean(reinforce_eval)\n    reinforce_std = np.std(reinforce_eval)\n\n    # Train PPO\n    print(\"\\n2. Training PPO...\")\n    ppo_start = time.time()\n    ppo_model = train_ppo(total_timesteps=100000)\n    ppo_time = time.time() - ppo_start\n\n    # Evaluate PPO\n    env = gym.make('CartPole-v1')\n    ppo_mean, ppo_std = evaluate_policy(ppo_model, env, n_eval_episodes=100)\n    env.close()\n\n    # Print comparison\n    print(\"\\n\" + \"=\" * 60)\n    print(\"RESULTS SUMMARY\")\n    print(\"=\" * 60)\n\n    print(\"\\nPerformance:\")\n    print(f\"  REINFORCE: {reinforce_mean:.2f} \u00b1 {reinforce_std:.2f}\")\n    print(f\"  PPO:       {ppo_mean:.2f} \u00b1 {ppo_std:.2f}\")\n\n    print(\"\\nTraining Time:\")\n    print(f\"  REINFORCE: {reinforce_time:.2f} seconds\")\n    print(f\"  PPO:       {ppo_time:.2f} seconds\")\n\n    print(\"\\nSample Efficiency:\")\n    reinforce_episodes = len(reinforce_rewards)\n    reinforce_steps = sum([len(r) for r in reinforce_rewards]) if isinstance(reinforce_rewards[0], list) else reinforce_episodes * 200  # Approximate\n    print(f\"  REINFORCE: ~{reinforce_episodes} episodes\")\n    print(f\"  PPO:       ~100,000 timesteps\")\n\n    # Plot comparison\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Training curves\n    axes[0].plot(reinforce_rewards, label='REINFORCE', alpha=0.7)\n    axes[0].axhline(y=475, color='r', linestyle='--', label='Solved')\n    axes[0].set_xlabel('Episode')\n    axes[0].set_ylabel('Reward')\n    axes[0].set_title('Training Progress')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    # Evaluation comparison\n    algorithms = ['REINFORCE', 'PPO']\n    means = [reinforce_mean, ppo_mean]\n    stds = [reinforce_std, ppo_std]\n\n    axes[1].bar(algorithms, means, yerr=stds, capsize=10, \n               color=['blue', 'orange'], alpha=0.7)\n    axes[1].axhline(y=475, color='r', linestyle='--', label='Solved')\n    axes[1].set_ylabel('Mean Reward')\n    axes[1].set_title('Evaluation Performance (100 episodes)')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3, axis='y')\n\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        'reinforce': {\n            'mean': reinforce_mean,\n            'std': reinforce_std,\n            'time': reinforce_time,\n            'rewards': reinforce_rewards\n        },\n        'ppo': {\n            'mean': ppo_mean,\n            'std': ppo_std,\n            'time': ppo_time\n        }\n    }\n\n# Run comparison\nresults = compare_algorithms()\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#expected-results-and-analysis","title":"Expected Results and Analysis","text":""},{"location":"reinforcement_learning/6_practical_tutorial/#reinforce-performance","title":"REINFORCE Performance","text":"<p>Typical outcomes: - Convergence: 300-600 episodes to solve (avg reward \u2265 475) - Stability: Some variance, may occasionally drop performance - Final performance: 450-500 mean reward - Training time: 1-3 minutes on CPU</p> <p>Characteristics: - High variance in early training - Gradual, sometimes unsteady improvement - Sensitive to hyperparameters (learning rate, gamma) - Simple and interpretable</p>"},{"location":"reinforcement_learning/6_practical_tutorial/#ppo-performance","title":"PPO Performance","text":"<p>Typical outcomes: - Convergence: Solves within 50,000-100,000 timesteps - Stability: Very stable training curve - Final performance: 495-500 mean reward (near optimal) - Training time: 30-60 seconds on CPU</p> <p>Characteristics: - Smooth, consistent improvement - Very stable due to clipped updates - More sample efficient than REINFORCE - More complex implementation</p>"},{"location":"reinforcement_learning/6_practical_tutorial/#key-differences","title":"Key Differences","text":"Aspect REINFORCE PPO Sample Efficiency Lower Higher Stability Moderate High Variance High Low Complexity Simple Complex Convergence Speed Slower Faster Final Performance Good Excellent Hyperparameter Sensitivity High Moderate"},{"location":"reinforcement_learning/6_practical_tutorial/#part-4-debugging-and-improvements","title":"Part 4: Debugging and Improvements","text":""},{"location":"reinforcement_learning/6_practical_tutorial/#common-issues-with-reinforce","title":"Common Issues with REINFORCE","text":""},{"location":"reinforcement_learning/6_practical_tutorial/#issue-1-not-learning-random-performance","title":"Issue 1: Not Learning / Random Performance","text":"<p>Symptoms: Rewards stay around 20-30, no improvement</p> <p>Possible causes: - Learning rate too high or too low - No return normalization - Incorrect gradient computation</p> <p>Solutions: <pre><code># Try different learning rates\nlearning_rates = [1e-4, 1e-3, 1e-2]\n\n# Ensure return normalization\nreturns = (returns - returns.mean()) / (returns.std() + 1e-8)\n\n# Add gradient clipping\ntorch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)\n</code></pre></p>"},{"location":"reinforcement_learning/6_practical_tutorial/#issue-2-unstable-learning","title":"Issue 2: Unstable Learning","text":"<p>Symptoms: Performance improves then suddenly drops</p> <p>Solutions: <pre><code># Use smaller learning rate\nlearning_rate = 1e-3\n\n# Add entropy bonus for exploration\nentropy = dist.entropy().mean()\nloss = policy_loss - 0.01 * entropy\n\n# Increase gamma for longer-term thinking\ngamma = 0.99 or 0.995\n</code></pre></p>"},{"location":"reinforcement_learning/6_practical_tutorial/#issue-3-slow-convergence","title":"Issue 3: Slow Convergence","text":"<p>Solutions: <pre><code># Use baseline (value function)\nadvantage = returns - baseline\n\n# Better network architecture\nhidden_dim = 256  # Larger network\n\n# Use adaptive learning rate\noptimizer = optim.Adam(policy.parameters(), lr=1e-2, eps=1e-5)\n</code></pre></p>"},{"location":"reinforcement_learning/6_practical_tutorial/#hyperparameter-tuning-guide","title":"Hyperparameter Tuning Guide","text":"<pre><code>def hyperparameter_search():\n    \"\"\"\n    Grid search over hyperparameters\n    \"\"\"\n    learning_rates = [1e-3, 3e-3, 1e-2]\n    gammas = [0.95, 0.99, 0.995]\n    hidden_dims = [64, 128, 256]\n\n    best_score = 0\n    best_params = {}\n\n    for lr in learning_rates:\n        for gamma in gammas:\n            for hidden in hidden_dims:\n                print(f\"\\nTrying lr={lr}, gamma={gamma}, hidden={hidden}\")\n\n                # Train with these params\n                # ... (training code)\n\n                # Evaluate\n                score = evaluate_agent(agent)\n\n                if score &gt; best_score:\n                    best_score = score\n                    best_params = {\n                        'lr': lr,\n                        'gamma': gamma,\n                        'hidden': hidden\n                    }\n\n    print(f\"\\nBest parameters: {best_params}\")\n    print(f\"Best score: {best_score}\")\n\n    return best_params\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#part-5-extensions-and-next-steps","title":"Part 5: Extensions and Next Steps","text":""},{"location":"reinforcement_learning/6_practical_tutorial/#extension-1-reinforce-with-baseline","title":"Extension 1: REINFORCE with Baseline","text":"<pre><code>class REINFORCEWithBaseline(REINFORCEAgent):\n    \"\"\"\n    REINFORCE with value function baseline\n    \"\"\"\n    def __init__(self, state_dim, action_dim, lr_policy=1e-3, lr_value=1e-3, gamma=0.99):\n        super().__init__(state_dim, action_dim, lr_policy, gamma)\n\n        # Add value network\n        self.value_network = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr_value)\n\n        self.states = []\n\n    def select_action(self, state):\n        self.states.append(state)\n        return super().select_action(state)\n\n    def update_policy(self):\n        returns = self.compute_returns()\n        states = torch.FloatTensor(np.array(self.states))\n\n        # Compute values\n        values = self.value_network(states).squeeze()\n\n        # Compute advantages\n        advantages = returns - values.detach()\n\n        # Update policy\n        policy_loss = []\n        for log_prob, advantage in zip(self.log_probs, advantages):\n            policy_loss.append(-log_prob * advantage)\n\n        policy_loss = torch.stack(policy_loss).sum()\n\n        self.optimizer.zero_grad()\n        policy_loss.backward()\n        self.optimizer.step()\n\n        # Update value function\n        value_loss = ((values - returns) ** 2).mean()\n\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        self.value_optimizer.step()\n\n        # Reset\n        self.reset_episode()\n        self.states = []\n\n        return policy_loss.item(), value_loss.item()\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#extension-2-try-other-environments","title":"Extension 2: Try Other Environments","text":"<pre><code># More challenging environments\nenvironments = [\n    'CartPole-v1',      # Solved: 475\n    'Acrobot-v1',       # Solved: -100\n    'LunarLander-v2',   # Solved: 200\n    'MountainCar-v0'    # Solved: -110\n]\n\nfor env_name in environments:\n    print(f\"\\n{'='*60}\")\n    print(f\"Training on {env_name}\")\n    print(f\"{'='*60}\")\n\n    agent, rewards, _ = train_reinforce(\n        env_name=env_name,\n        num_episodes=2000\n    )\n\n    plot_training_progress(rewards)\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#extension-3-continuous-actions","title":"Extension 3: Continuous Actions","text":"<pre><code>from torch.distributions import Normal\n\nclass ContinuousPolicyNetwork(nn.Module):\n    \"\"\"\n    Policy network for continuous actions (Gaussian policy)\n    \"\"\"\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super().__init__()\n\n        self.shared = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        self.mean = nn.Linear(hidden_dim, action_dim)\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n\n    def forward(self, state):\n        x = self.shared(state)\n        mean = self.mean(x)\n        std = torch.exp(self.log_std)\n        return mean, std\n\n    def get_action(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0)\n        mean, std = self.forward(state)\n\n        # Create normal distribution\n        dist = Normal(mean, std)\n\n        # Sample action\n        action = dist.sample()\n        log_prob = dist.log_prob(action).sum(dim=-1)\n\n        return action.squeeze().numpy(), log_prob\n\n# Use this for continuous control tasks like Pendulum-v1, HalfCheetah, etc.\n</code></pre>"},{"location":"reinforcement_learning/6_practical_tutorial/#summary","title":"Summary","text":"<p>In this tutorial, you've:</p> <ul> <li>Implemented REINFORCE from scratch with detailed explanations</li> <li>Learned how to train and evaluate RL agents</li> <li>Compared your implementation with state-of-the-art PPO</li> <li>Understood the tradeoffs between different algorithms</li> <li>Learned debugging techniques and hyperparameter tuning</li> <li>Explored extensions like baselines and continuous actions</li> </ul>"},{"location":"reinforcement_learning/6_practical_tutorial/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>REINFORCE is simple but effective for basic problems</li> <li>Modern algorithms (PPO) are more sample efficient and stable</li> <li>Implementation details matter: normalization, clipping, learning rates</li> <li>Start simple: Get basic version working before adding complexity</li> <li>Evaluation is critical: Don't just look at training curves</li> </ol> <p>The complete, runnable code is available in the accompanying Jupyter notebooks:</p>"},{"location":"reinforcement_learning/6_practical_tutorial/#unit-1-train-your-first-deep-reinforcement-learning-agent","title":"Unit 1: Train Your First Deep Reinforcement Learning Agent","text":"<p>What you'll learn: This notebook introduces you to training your first deep RL agent using Stable-Baselines3. You'll learn how to: - Set up Gymnasium environments (LunarLander-v2) - Use PPO (Proximal Policy Optimization) from a high-level library - Train and evaluate a policy gradient agent - Understand the practical workflow of RL training</p> <p>This is perfect for getting started with RL without implementing algorithms from scratch.</p>"},{"location":"reinforcement_learning/6_practical_tutorial/#unit-2-q-learning-from-scratch","title":"Unit 2: Q-Learning from Scratch","text":"<p>What you'll learn: This notebook guides you through implementing Q-Learning from scratch. You'll: - Build a Q-table and implement the Q-Learning update rule - Implement epsilon-greedy exploration strategies - Train Q-Learning agents on discrete environments (FrozenLake-v1 and Taxi-v3) - Understand how tabular Q-Learning works in practice - Learn to evaluate and debug RL agents</p> <p>This hands-on experience will solidify your understanding of value-based methods.</p>"},{"location":"reinforcement_learning/6_practical_tutorial/#unit-3-policy-gradient-reinforce-from-scratch","title":"Unit 3: Policy Gradient (REINFORCE) from Scratch","text":"<p>What you'll learn: This notebook teaches you to implement REINFORCE from scratch using PyTorch. You'll: - Build a policy network using neural networks - Implement the REINFORCE algorithm step-by-step - Compute discounted returns and policy gradients - Train policy gradient agents on CartPole-v1 and PixelCopter - Understand the practical challenges of policy gradient methods (variance, sample efficiency)</p> <p>This implementation will give you deep insight into how policy-based methods work under the hood.</p> <p>\u2190 Back to Policy Gradients Back to RL Module Home</p>"}]}