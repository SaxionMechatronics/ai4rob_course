# Challenges within AI for robotics
Data limitations

If it is so promising and helpful, why is AI not the standard within robotics? If you look at large language model (LLM) like Chatgpt and Copilot they can already do amazing things. So it should be to difficult to map this knowledge towards the robotics world right? That is an understable thought, but there is a big difference between the LLM and the AI that is used within robotics. The availablity of the existing data. Chatgpt could use everything from the internet to learn from.  They scraped the whole internet for learning data. This is similar towards object detection based upon images, there exists already a large and a lot of data labelled data sets. As already suspected from this the data is key in ai. The data type also determines the quality of what is learned, if you for example only learn to catch a red ball and the ball is suddenly blue it is like that the  algorithm does not recognize the ball. 

The usage of AI within industrial robotics is limited, but it starts to emerge.  Most used AI algorithms are part of the machine learning, meaning it recognizes patterns in data. In order to learn these pattern data is required, but within robotics there is just a limited data available. As not every movement a robot is does is logged and store on location accessible to everyone (in contrary to a lot of things on the internet).  It is currently one of the biggest challenges faced within ai for robotics, how can we learn/train with the minimal amount of data? A solution is to just increase the data on which AI learns. However, this is tedious as it requires to run a lot of tasks on robots.  Therefore, synthetic data is generated. It means that the ai algorithms are trained op data which has been created in simulations. 

In addition to that,  preferably one algorithm could be used for one robot. This is less feasible, because there is not just one type of robot, a robot dog executes a certain task differently as compared to a humanoid. There is thus also the issue of cross embodiment. The data gathered is typically specifically for just one type of robot.   Another option is just by hand, at the time of writing the company 1x announced a humanoid which can be bought. This humanoid has the potential to do task autonomously. If turns out during the deployment of this humanoid that it can not deploy the task you can schedule a meeting with someone who can teleoperate the robot. In doing so more data is generated by hand. Again the disadvantage of this is that it only creates data for this specific robot. 
Sim 2 real

Another issue is now that robotics are getting trained with artificial data and in simulations (which typically the case in reinforcement learning). In the case for reinforcement learning it is also not possible/smart decision to first train on a physical system, because in the beginning of training the algorithm is doing random things. If you apply such a policy on a robot this leads to unexpected and sometimes even dangerous situations, because it is not know what this robot is doing. Therefore, the initial phase of such a policy is always trained in simulations. When the results in simulations are satisfactory the algorithm is translated from simulations towards the real robot. This typically lead to a non-working algorithm on the real robot, because in simulation simplification were performed. In addition, the not all dynamics and events are captured in simulation. The AI typically has to do some retaining when it is deployed on the real robot. This issue is also called sim2real gap, the translation from simulations to the real robot. This gap can stem from:

    Physics inaccuracies: Simulators often simplify or approximate physical interactions (e.g., friction, contact dynamics), which may not match real-world behavior.

    Sensor noise and latency: Real sensors introduce noise, delays, and calibration issues that are hard to model accurately in simulation.

    Actuator differences: Motors and joints in real robots may behave differently due to wear, backlash, or manufacturing variability.

    Environmental variability: Lighting, textures, and unexpected disturbances in the real world are difficult to replicate in simulation.

One common method to encounter sim2real is to apply domain randomization, meaning you are adding random components in the simulation to make the AI more robust to the uncertainties when deployed on the real robot. 
Generalization
Lastly, generalization is a key challenge in robotics. As now know machine learning extracts patterns from pattern and uses this pattern to perform action. However, current algorithms are only limited to data represented.  A simple example is that a robot knows how to tie shoelace for brand a, but it does not directly know how to tie shoelaces for brand b. Another example is picking up cubes, if it only knows how to learn and pick up red cubes it does not directly also know how to pickup blue cubes. It is something we as humans can relatively easy do, but these way of thinking/seeing connection is something difficult to convey to robotics systems.  Possible solutions to this is to use already within the data sets randomization and expanding the data set as fast as possible. Another solution to be found is the usage of one shot learning, which lead to the fact that humans can show case the robot what should be done if they do not know. A more new and upcoming method is considered are the foundation models, these are trained with a lot and different data's and can be used as base algorithm if these are implemented only fine tuning would be required for tasks it allows for better understanding.  Hence also the name foundation model, because they are seen as foundation for more specific AI practices.  (found Foundation Models Explained: What They Are, How They Work, and Why They Matter | by Munikanth | Medium) 