# Challenges within AI for robotics

If it is so promising and helpful, why is AI not the standard within robotics? If you look at large language models (LLM) like ChatGPT and Copilot they can already do amazing things. So it should not be too difficult to map this knowledge towards the robotics world right? That is an understandable thought, but there is a big difference between the LLM and the AI that is used within robotics. The availablity of the existing data. ChatGPT could use everything from the internet to learn from. They scraped the whole internet for learning data. This is similar towards object detection based upon images, there exists already large labelled data sets. As already suspected from this the data is key in AI. The type of data present in the dataset also determines the quality of what is learned, if you for example only learn to catch a red ball and the ball is suddenly blue it is like that the  algorithm does not recognize the ball. 

The usage of AI within industrial robotics is limited, but it starts to emerge. Most used AI algorithms are part of machine learning, meaning it recognizes patterns in data. In order to learn these pattern data is required, but within robotics there is limited data available. As not every movement a robot makes is logged and stored on a location accessible to everyone (in contrary to a lot of things on the internet). It is currently one of the biggest challenges faced within AI for robotics, how can we learn/train with the minimal amount of data? A solution is to just increase the data on which AI learns. However, this is tedious as it requires to run a lot of tasks on robots. Therefore, synthetic data is generated. It means that the AI algorithms are trained on data which has been created in simulations. 

In addition to that preferably one type of data, for example movement of a manipulator, could be used to train all different kind of robots. This would help to gain more data about specific tasks. This is less realistic, because there is not just one type of robot, a robot dog executes a certain task differently as compared to a humanoid. This variation in physical form and capabilities is referred to as cross-embodiment. The data gathered is typically specifically for just one type of robot. Another option to encounter the data shortage is by manually control the robot when it does not know what to do within a specific task. In this case, the operator shows the robot how to do this task and the robot can also learn from this. 

## Sim to Real gap

Another issue is now that robotics are getting trained with artificial data and in simulations (which typically is the case in reinforcement learning). In the case for reinforcement learning it is also not a smart decision to first train on a physical system, because in the beginning of training the algorithm is doing random things. If you apply such a policy on a robot this leads to unexpected and sometimes even dangerous situations, because it is not known what this robot is doing. Therefore, the initial phase of such a policy is always trained in simulations. When the results in simulations are satisfactory the algorithm is moved from simulations towards the real robot. This typically leads to a non-working algorithm on the real robot, because in simulation simplifications were made. In addition, not all dynamics and events are captured in simulation. The AI typically has to do some retaining when it is deployed on the real robot. This issue is also called sim2real gap, the translation from simulations to the real robot. This gap can stem from:

- Physics inaccuracies: Simulators often simplify or approximate physical interactions (e.g., friction, contact dynamics), which may not match real-world behavior.
- Sensor noise and latency: Real sensors introduce noise, delays, and calibration issues that are hard to model accurately in simulation.
- Actuator differences: Motors and joints in real robots may behave differently due to wear, backlash, or manufacturing variability.
- Environmental variability: Lighting, textures, and unexpected disturbances in the real world are difficult to replicate in simulation.

One common method to minimize the simulations to real gap is to apply domain randomization, meaning you are adding random components in the simulation to make the AI more robust to the uncertainties when deployed on the real robot. 

## Generalization

Lastly, generalization is a key challenge in robotics. As we know now machine learning extracts patterns from data and uses this extract pattern to perform actions. However, current algorithms are only limited to data represented. A simple example is that a robot knows how to tie a shoelace for brand A, but it does not directly know how to tie shoelaces for brand B. Another example is picking up cubes, if it only knows how to learn and pick up red cubes it does not directly also know how to pickup blue cubes. It is something we as humans can relatively easy do, but this way of thinking/seeing connection is something difficult to convey to robotics systems. A possible solution is to randomize within data sets and expanding the data set as fast as possible. Another solution to be found is the usage of one shot learning, which lead to the fact that humans can show case the robot what should be done if they do not know. Another method to improve generalization is the usage of foundation models, these are trained with a lot and different data's and can be used as base algorithm if these are implemented only fine tuning would be required for tasks it allows for better understanding. Hence also the name foundation model, because they are seen as foundation for more specific AI practices.  

[← Back to AI helping robotics](Combine_ai_and_robotics.md){ .md-button .md-button--primary }
[Continue to Why start with deep learning? →](why_start_with_deep_learning.md){ .md-button .md-button--primary }