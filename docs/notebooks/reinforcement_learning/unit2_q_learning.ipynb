{"cells":[{"cell_type":"markdown","metadata":{"id":"njb_ProuHiOe"},"source":["# Unit 2: Q-Learning with FrozenLake-v1 and Taxi-v3"]},{"cell_type":"markdown","source":["## Install dependencies and create a virtual display"],"metadata":{"id":"4gpxC1_kqUYe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XaULfDZDvrC"},"outputs":[],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install numpy\n","!pip install pickle5\n","!pip install pyyaml==6.0\n","!pip install imageio\n","!pip install imageio_ffmpeg\n","!pip install pyglet==1.5.1\n","!pip install tqdm"]},{"cell_type":"code","source":["!sudo apt-get update\n","!sudo apt-get install -y python3-opengl\n","!apt install ffmpeg xvfb\n","!pip3 install pyvirtualdisplay"],"metadata":{"id":"n71uTX7qqzz2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**"],"metadata":{"id":"K6XC13pTfFiD"}},{"cell_type":"code","source":["import os\n","os.kill(os.getpid(), 9)"],"metadata":{"id":"3kuZbWAkfHdg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W-7f-Swax_9x"},"source":["## Import the packages"]},{"cell_type":"code","source":["# Virtual display\n","from pyvirtualdisplay import Display\n","import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"],"metadata":{"id":"DaY1N4dBrabi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcNvOAQlysBJ"},"outputs":[],"source":["import numpy as np\n","import gymnasium as gym\n","import random\n","import imageio\n","import os\n","import tqdm\n","\n","import pickle5 as pickle\n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","metadata":{"id":"xya49aNJWVvv"},"source":["# Part 1: Frozen Lake (non slippery version)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IzJnb8O3y8up"},"outputs":[],"source":["# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\n","env = gym.make() # TODO use the correct parameters"]},{"cell_type":"markdown","metadata":{"id":"Ji_UrI5l2zzn"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNxUbPMP0akP"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{"id":"KASNViqL4tZn"},"source":["You can create your own custom grid like this:\n","\n","```python\n","desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n","gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n","```\n","\n","but we'll use the default environment for now."]},{"cell_type":"markdown","metadata":{"id":"SXbTfdeJ1Xi9"},"source":["### Let's see what the Environment looks like:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNPG0g_UGCfh"},"outputs":[],"source":["# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n","print(\"_____OBSERVATION SPACE_____ \\n\")\n","print(\"Observation Space\", env.observation_space)\n","print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"]},{"cell_type":"markdown","metadata":{"id":"2MXc15qFE0M9"},"source":["We see with `Observation Space Shape Discrete(16)` that the observation is an integer representing the **agent‚Äôs current position as current_row * ncols + current_col (where both the row and col start at 0)**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"We5WqOBGLoSm"},"outputs":[],"source":["print(\"\\n _____ACTION SPACE_____ \\n\")\n","print(\"Action Space Shape\", env.action_space.n)\n","print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"]},{"cell_type":"markdown","metadata":{"id":"MyxXwkI2Magx"},"source":["The action space (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\n","- 0: GO LEFT\n","- 1: GO DOWN\n","- 2: GO RIGHT\n","- 3: GO UP\n","\n","Reward function üí∞:\n","- Reach goal: +1\n","- Reach hole: 0\n","- Reach frozen: 0"]},{"cell_type":"markdown","metadata":{"id":"1pFhWblk3Awr"},"source":["## Create and Initialize the Q-table\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3ZCdluj3k0l"},"outputs":[],"source":["state_space =\n","print(\"There are \", state_space, \" possible states\")\n","\n","action_space =\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCddoOXM3UQH"},"outputs":[],"source":["# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n","def initialize_q_table(state_space, action_space):\n","  Qtable =\n","  return Qtable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YfvrqRt3jdR"},"outputs":[],"source":["Qtable_frozenlake = initialize_q_table(state_space, action_space)"]},{"cell_type":"markdown","metadata":{"id":"67OdoKL63eDD"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuTKv3th3ohG"},"outputs":[],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")\n","\n","action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnrb_nX33fJo"},"outputs":[],"source":["# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n","def initialize_q_table(state_space, action_space):\n","  Qtable = np.zeros((state_space, action_space))\n","  return Qtable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0WlgkVO3Jf9"},"outputs":[],"source":["Qtable_frozenlake = initialize_q_table(state_space, action_space)"]},{"cell_type":"markdown","metadata":{"id":"Atll4Z774gri"},"source":["## Define the greedy policy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3SCLmLX5bWG"},"outputs":[],"source":["def greedy_policy(Qtable, state):\n","  # Exploitation: take the action with the highest state, action value\n","  action =\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"B2_-8b8z5k54"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"se2OzWGW5kYJ"},"outputs":[],"source":["def greedy_policy(Qtable, state):\n","  # Exploitation: take the action with the highest state, action value\n","  action = np.argmax(Qtable[state][:])\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"flILKhBU3yZ7"},"source":["##Define the epsilon-greedy policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Bj7x3in3_Pq"},"outputs":[],"source":["def epsilon_greedy_policy(Qtable, state, epsilon):\n","  # Randomly generate a number between 0 and 1\n","  random_num =\n","  # if random_num > greater than epsilon --> exploitation\n","  if random_num > epsilon:\n","    # Take the action with the highest value given a state\n","    # np.argmax can be useful here\n","    action =\n","  # else --> exploration\n","  else:\n","    action = # Take a random action\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"8R5ej1fS4P2V"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYxHuckr4LiG"},"outputs":[],"source":["def epsilon_greedy_policy(Qtable, state, epsilon):\n","  # Randomly generate a number between 0 and 1\n","  random_num = random.uniform(0,1)\n","  # if random_num > greater than epsilon --> exploitation\n","  if random_num > epsilon:\n","    # Take the action with the highest value given a state\n","    # np.argmax can be useful here\n","    action = greedy_policy(Qtable, state)\n","  # else --> exploration\n","  else:\n","    action = env.action_space.sample()\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"hW80DealcRtu"},"source":["## Define the hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1tWn0tycWZ1"},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 10000  # Total training episodes\n","learning_rate = 0.7          # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# Environment parameters\n","env_id = \"FrozenLake-v1\"     # Name of the environment\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","eval_seed = []               # The evaluation seed of the environment\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05            # Minimum exploration probability\n","decay_rate = 0.0005            # Exponential decay rate for exploration prob"]},{"cell_type":"markdown","metadata":{"id":"cDb7Tdx8atfL"},"source":["## Create the training loop method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paOynXy3aoJW"},"outputs":[],"source":["def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n","  for episode in tqdm(range(n_training_episodes)):\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    # Reset the environment\n","    state, info = env.reset()\n","    step = 0\n","    terminated = False\n","    truncated = False\n","\n","    # repeat\n","    for step in range(max_steps):\n","      # Choose the action At using epsilon greedy policy\n","      action =\n","\n","      # Take action At and observe Rt+1 and St+1\n","      # Take the action (a) and observe the outcome state(s') and reward (r)\n","      new_state, reward, terminated, truncated, info =\n","\n","      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","      Qtable[state][action] =\n","\n","      # If terminated or truncated finish the episode\n","      if terminated or truncated:\n","        break\n","\n","      # Our next state is the new state\n","      state = new_state\n","  return Qtable"]},{"cell_type":"markdown","metadata":{"id":"Pnpk2ePoem3r"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyZaYbUAeolw"},"outputs":[],"source":["def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n","  for episode in tqdm(range(n_training_episodes)):\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    # Reset the environment\n","    state, info = env.reset()\n","    step = 0\n","    terminated = False\n","    truncated = False\n","\n","    # repeat\n","    for step in range(max_steps):\n","      # Choose the action At using epsilon greedy policy\n","      action = epsilon_greedy_policy(Qtable, state, epsilon)\n","\n","      # Take action At and observe Rt+1 and St+1\n","      # Take the action (a) and observe the outcome state(s') and reward (r)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","\n","      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n","\n","      # If terminated or truncated finish the episode\n","      if terminated or truncated:\n","        break\n","\n","      # Our next state is the new state\n","      state = new_state\n","  return Qtable"]},{"cell_type":"markdown","metadata":{"id":"WLwKQ4tUdhGI"},"source":["## Train the Q-Learning agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPBxfjJdTCOH"},"outputs":[],"source":["Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"]},{"cell_type":"markdown","metadata":{"id":"yVeEhUCrc30L"},"source":["## Let's see what our Q-Learning table looks like now"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmfchsTITw4q"},"outputs":[],"source":["Qtable_frozenlake"]},{"cell_type":"markdown","metadata":{"id":"pUrWkxsHccXD"},"source":["## Evaluation function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNl0_JO2cbkm"},"outputs":[],"source":["def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n","  \"\"\"\n","  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n","  :param env: The evaluation environment\n","  :param max_steps: Maximum number of steps per episode\n","  :param n_eval_episodes: Number of episode to evaluate the agent\n","  :param Q: The Q-table\n","  :param seed: The evaluation seed array (for taxi-v3)\n","  \"\"\"\n","  episode_rewards = []\n","  for episode in tqdm(range(n_eval_episodes)):\n","    if seed:\n","      state, info = env.reset(seed=seed[episode])\n","    else:\n","      state, info = env.reset()\n","    step = 0\n","    truncated = False\n","    terminated = False\n","    total_rewards_ep = 0\n","\n","    for step in range(max_steps):\n","      # Take the action (index) that have the maximum expected future reward given that state\n","      action = greedy_policy(Q, state)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","      total_rewards_ep += reward\n","\n","      if terminated or truncated:\n","        break\n","      state = new_state\n","    episode_rewards.append(total_rewards_ep)\n","  mean_reward = np.mean(episode_rewards)\n","  std_reward = np.std(episode_rewards)\n","\n","  return mean_reward, std_reward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAgB7s0HEFMm"},"outputs":[],"source":["# Evaluate our Agent\n","mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n","print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"18lN8Bz7yvLt"},"source":["# Part 2: Taxi-v3\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gL0wpeO8gpej"},"outputs":[],"source":["env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{"id":"gBOaXgtsrmtT"},"source":["There are **500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger** (including the case when the passenger is in the taxi), and **4 destination locations.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TPNaGSZrgqA"},"outputs":[],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CdeeZuokrhit"},"outputs":[],"source":["action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"markdown","metadata":{"id":"R1r50Advrh5Q"},"source":["The action space (the set of possible actions the agent can take) is discrete with **6 actions available üéÆ**:\n","\n","- 0: move south\n","- 1: move north\n","- 2: move east\n","- 3: move west\n","- 4: pickup passenger\n","- 5: drop off passenger\n","\n","Reward function üí∞:\n","\n","- -1 per step unless other reward is triggered.\n","- +20 delivering passenger.\n","- -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"US3yDXnEtY9I"},"outputs":[],"source":["# Create our Q table with state_size rows and action_size columns (500x6)\n","Qtable_taxi = initialize_q_table(state_space, action_space)\n","print(Qtable_taxi)\n","print(\"Q-table shape: \", Qtable_taxi .shape)"]},{"cell_type":"markdown","metadata":{"id":"gUMKPH0_LJyH"},"source":["## Define the hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AB6n__hhg7YS"},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 25000   # Total training episodes\n","learning_rate = 0.7           # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# DO NOT MODIFY EVAL_SEED\n","eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n"," 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n"," 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n","                                                          # Each seed has a specific starting state\n","\n","# Environment parameters\n","env_id = \"Taxi-v3\"           # Name of the environment\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05           # Minimum exploration probability\n","decay_rate = 0.005            # Exponential decay rate for exploration prob\n"]},{"cell_type":"markdown","metadata":{"id":"1TMORo1VLTsX"},"source":["## Train our Q-Learning agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WwP3Y2z2eS-K"},"outputs":[],"source":["Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n","Qtable_taxi"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["67OdoKL63eDD","B2_-8b8z5k54","8R5ej1fS4P2V","Pnpk2ePoem3r"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}