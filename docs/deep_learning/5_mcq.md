## Question 1
```mcq
---
type: single
question: What regularization technique is introduced in Code 5?
---
- [ ] Weight decay  
- [x] Dropout regularization  
- [ ] Batch normalization  
- [ ] Gradient clipping  
 
```

---

## Question 2
```mcq
---
type: single
question: What is the primary goal of adding Dropout in Code 5?
---
- [ ] To improve training speed  
- [x] To reduce overfitting  
- [ ] To increase model depth  
- [ ] To normalize activations  

```

---

## Question 3
```mcq
---
type: single
question: How does Dropout function during training?
---
- [ ] It freezes specific neurons permanently  
- [x] It randomly deactivates neurons during training to prevent co-adaptation  
- [ ] It adds extra convolutional layers  
- [ ] It changes the learning rate dynamically  

```

---

## Question 4
```mcq
---
type: single
question: What is the observed result of adding Dropout?
---
- [x] Slightly lower training accuracy but improved test accuracy  
- [ ] Higher overfitting  
- [ ] Model collapse  
- [ ] Unstable training  

```

---

## Question 5
```mcq
---
type: single
question: Why was it difficult to push performance higher at this stage?
---
- [ ] Model capacity was too large  
- [x] Model capacity was too small  
- [ ] Dropout rate was too high  
- [ ] The optimizer was not adaptive  

```