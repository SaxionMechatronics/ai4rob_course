## Question 1
```mcq
---
type: single
question: Why was the model capacity increased in Code 7?
---
- [x] Because Global Average Pooling (GAP) reduced too many parameters  
- [ ] To make the model lighter  
- [ ] To fix an overfitting issue  
- [ ] To reduce training time  

  
 
```

---

## Question 2
```mcq
---
type: single
question: What is the new parameter count after increasing capacity?
---
- [ ] 6 k  
- [x] 11.9 k  
- [ ] 13.8 k  
- [ ] 10.7 k  


```

---

## Question 3
```mcq
---
type: single
question: What was the observed effect of increasing the modelâ€™s capacity?
---
- [x] Slight return of overfitting but improved accuracy  
- [ ] Decreased training accuracy and underfitting  
- [ ] Training divergence  
- [ ] Dropout became ineffective  


```

---

## Question 4
```mcq
---
type: single
question: What design insight was gained from this experiment?
---
- [ ] BatchNorm is unnecessary for convergence  
- [x] End-layer capacity strongly influences model performance  
- [ ] Dropout rate must always be high  
- [ ] More convolutional filters always reduce accuracy  


```

---

## Question 5
```mcq
---
type: single
question: Adding layers after GAP helps the model by:
---
- [x] Improving feature abstraction and final decision quality  
- [ ] Reducing the receptive field size  
- [ ] Eliminating redundant neurons  
- [ ] Flattening the feature maps for dense input  


```