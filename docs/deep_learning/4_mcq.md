## Question 1
```mcq
---
type: single
question: Why is Batch Normalization added in Code 4?
---
- [ ] To randomize inputs  
- [x] To stabilize learning and normalize activations  
- [ ] To increase dropout rate  
- [ ] To reduce dataset size  


```

---

## Question 2
```mcq
---
type: single
question: What effect does Batch Normalization have on convergence?
---
- [ ] Slows it down  
- [x] Speeds it up  
- [ ] Prevents training  
- [ ] Randomizes gradients  


```

---

## Question 3
```mcq
---
type: single
question: After applying Batch Normalization, what new challenge appears?
---
- [x] Overfitting  
- [ ] Underfitting  
- [ ] Gradient explosion  
- [ ] Loss plateau  


```

---

## Question 4
```mcq
---
type: single
question: What is the updated parameter count after applying Batch Normalization?
---
- [ ] 6 k  
- [x] 10.9 k  
- [ ] 194 k  
- [ ] 11.9 k  



```

---

## Question 5
```mcq
---
type: single
question: Batch Normalization is typically inserted at which position in a CNN block?
---
- [ ] After activation  
- [x] After each convolutional layer (before activation)  
- [ ] At the output layer only  
- [ ] Between pooling layers  



```