## Question 1
```mcq
---
type: single
question: What new feature was introduced in Code 10?
---
- [ ] Weight decay regularization  
- [x] Learning Rate (LR) Scheduler  
- [ ] Additional convolutional layer  
- [ ] New activation function    
 
```

---

## Question 2
```mcq
---
type: single
question: What was the main purpose of adding a Learning Rate Scheduler?
---
- [x] To improve convergence by reducing the learning rate over time  
- [ ] To randomize gradients during training  
- [ ] To keep the learning rate constant  
- [ ] To increase the model depth  

```

---

## Question 3
```mcq
---
type: single
question: How was the scheduler configured in this implementation?
---
- [ ] Increase LR every 5 epochs  
- [x] Decrease LR by a factor of 10 after the 6th epoch  
- [ ] Keep LR constant for all epochs  
- [ ] Randomly change LR between 0.001 and 0.1  

```

---

## Question 4
```mcq
---
type: single
question: What was the final test accuracy achieved after adding the LR scheduler?
---
- [ ] 98 %  
- [x] 99.5 %  
- [ ] 97 %  
- [ ] 100 %  

```

---

## Question 5
```mcq
---
type: single
question: What is the main challenge when using a Learning Rate Scheduler?
---
- [x] Choosing the right schedule for stable and efficient convergence  
- [ ] Deciding the correct batch size  
- [ ] Removing normalization layers  
- [ ] Increasing the dropout rate at each step  

```